{
  "hash": "0c4592f7aa826bb0b92d28bd4329d134",
  "result": {
    "markdown": "---\ntitle: 'Modern Vision Models: CNNs, Vision Transformers, and DINOv2'\nauthor: Hasan\ndate: '2025-01-22'\ncategories:\n  - computer-vision\n  - transformers\n  - foundation-models\n  - dinov2\ntags:\n  - cnn\n  - vision-transformer\n  - dinov2\n  - self-supervised\n  - huggingface\nimage: 'https://images.unsplash.com/photo-1620712943543-bcc4688e7485?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2065&q=80'\ntoc: true\nseries:\n  name: Computer Vision Foundations\n  number: 8\nformat:\n  html: default\n---\n\n## The Evolution of Vision: From AlexNet to DINOv2\n\nRemember when we thought AlexNet was revolutionary in 2012? That was just the beginning! In the past decade, computer vision has evolved at breakneck speed:\n\n- **2012**: AlexNet - 8 layers, 60M parameters\n- **2015**: ResNet - 152 layers, skip connections\n- **2017**: Attention mechanisms emerge\n- **2020**: Vision Transformers - \"Attention is all you need\" for vision\n- **2023**: DINOv2 - Foundation models that understand everything\n\nToday, we're going to explore this incredible journey and show you how to use the most powerful vision models ever created!\n\n## The CNN Dynasty: ResNet, EfficientNet, and Beyond\n\nBefore transformers took over, CNNs ruled the vision world. Let's explore the key innovations:\n\n### ResNet: The Skip Connection Revolution\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Understanding ResNet's key innovation: skip connections\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        \n        # Main path\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        # Skip connection (the magic!)\n        self.skip = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        # Main path\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        \n        # Add skip connection (this is the key!)\n        out += self.skip(x)\n        out = torch.relu(out)\n        \n        return out\n\n# Create a simple ResNet-like model\nclass MiniResNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(MiniResNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.pool = nn.MaxPool2d(3, 2, 1)\n        \n        # Stack residual blocks\n        self.layer1 = self._make_layer(64, 64, 2, 1)\n        self.layer2 = self._make_layer(64, 128, 2, 2)\n        self.layer3 = self._make_layer(128, 256, 2, 2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)\n    \n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = []\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Compare with official ResNet\nmini_resnet = MiniResNet(num_classes=1000)\nofficial_resnet = models.resnet18(pretrained=True)\n\nprint(\"Mini ResNet:\")\nprint(f\"Parameters: {sum(p.numel() for p in mini_resnet.parameters()):,}\")\n\nprint(\"\\nOfficial ResNet-18:\")\nprint(f\"Parameters: {sum(p.numel() for p in official_resnet.parameters()):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMini ResNet:\nParameters: 3,042,024\n\nOfficial ResNet-18:\nParameters: 11,689,512\n```\n:::\n:::\n\n\n**ðŸŽ¯ Try it yourself!** [Open in Colab](https://colab.research.google.com/github/hasanpasha/quarto_blog_hasan/blob/main/notebooks/cv-foundations-07-modern-vision-models.ipynb)\n\n### EfficientNet: Scaling Done Right\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# EfficientNet's key insight: compound scaling\nfrom torchvision.models import efficientnet_b0, efficientnet_b7\n\n# Load different EfficientNet variants\nefficient_b0 = efficientnet_b0(pretrained=True)\nefficient_b7 = efficientnet_b7(pretrained=True)\n\ndef model_info(model, name):\n    total_params = sum(p.numel() for p in model.parameters())\n    return {\n        'name': name,\n        'parameters': total_params,\n        'size_mb': total_params * 4 / (1024 * 1024)  # Rough estimate\n    }\n\nmodels_comparison = [\n    model_info(efficient_b0, 'EfficientNet-B0'),\n    model_info(efficient_b7, 'EfficientNet-B7'),\n    model_info(official_resnet, 'ResNet-18')\n]\n\nprint(\"Model Comparison:\")\nprint(\"-\" * 50)\nfor info in models_comparison:\n    print(f\"{info['name']:20} | {info['parameters']:>10,} params | {info['size_mb']:>6.1f} MB\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel Comparison:\n--------------------------------------------------\nEfficientNet-B0      |  5,288,548 params |   20.2 MB\nEfficientNet-B7      | 66,347,960 params |  253.1 MB\nResNet-18            | 11,689,512 params |   44.6 MB\n```\n:::\n:::\n\n\n## The Transformer Revolution: Vision Meets Attention\n\nIn 2020, everything changed when researchers asked: \"What if we applied transformers to vision?\"\n\n### Understanding Vision Transformers\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclass PatchEmbedding(nn.Module):\n    \"\"\"Convert image to sequence of patches\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        # Patch embedding using convolution\n        self.projection = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n    \n    def forward(self, x):\n        # x shape: (batch_size, channels, height, width)\n        x = self.projection(x)  # (batch_size, embed_dim, num_patches_h, num_patches_w)\n        x = x.flatten(2)        # (batch_size, embed_dim, num_patches)\n        x = x.transpose(1, 2)   # (batch_size, num_patches, embed_dim)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head self-attention mechanism\"\"\"\n    def __init__(self, embed_dim=768, num_heads=12):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n    \n    def forward(self, x):\n        batch_size, seq_len, embed_dim = x.shape\n        \n        # Generate Q, K, V\n        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_len, head_dim)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Compute attention\n        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        attn = torch.softmax(attn, dim=-1)\n        \n        # Apply attention to values\n        out = (attn @ v).transpose(1, 2).reshape(batch_size, seq_len, embed_dim)\n        out = self.proj(out)\n        \n        return out, attn\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Single transformer encoder block\"\"\"\n    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadAttention(embed_dim, num_heads)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        # MLP\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Linear(mlp_hidden_dim, embed_dim)\n        )\n    \n    def forward(self, x):\n        # Self-attention with residual connection\n        attn_out, attn_weights = self.attn(self.norm1(x))\n        x = x + attn_out\n        \n        # MLP with residual connection\n        x = x + self.mlp(self.norm2(x))\n        \n        return x, attn_weights\n\nclass SimpleViT(nn.Module):\n    \"\"\"Simplified Vision Transformer\"\"\"\n    def __init__(self, img_size=224, patch_size=16, num_classes=1000, \n                 embed_dim=768, depth=12, num_heads=12):\n        super().__init__()\n        \n        # Patch embedding\n        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n        num_patches = self.patch_embed.num_patches\n        \n        # Class token and position embedding\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        \n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads) for _ in range(depth)\n        ])\n        \n        # Classification head\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n    \n    def forward(self, x):\n        batch_size = x.shape[0]\n        \n        # Patch embedding\n        x = self.patch_embed(x)  # (batch_size, num_patches, embed_dim)\n        \n        # Add class token\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        \n        # Add position embedding\n        x = x + self.pos_embed\n        \n        # Apply transformer blocks\n        attention_maps = []\n        for block in self.blocks:\n            x, attn = block(x)\n            attention_maps.append(attn)\n        \n        # Classification\n        x = self.norm(x)\n        cls_token_final = x[:, 0]  # Use class token for classification\n        out = self.head(cls_token_final)\n        \n        return out, attention_maps\n\n# Create a simple ViT\nsimple_vit = SimpleViT(depth=6, num_heads=8)  # Smaller for demo\nvit_params = sum(p.numel() for p in simple_vit.parameters())\n\nprint(f\"Simple ViT parameters: {vit_params:,}\")\n\n# Test with dummy input\ndummy_input = torch.randn(1, 3, 224, 224)\noutput, attention_maps = simple_vit(dummy_input)\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Number of attention maps: {len(attention_maps)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimple ViT parameters: 44,040,424\nOutput shape: torch.Size([1, 1000])\nNumber of attention maps: 6\n```\n:::\n:::\n\n\n### Visualizing Attention: What Does the Model Look At?\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef visualize_attention(image, attention_maps, patch_size=16):\n    \"\"\"Visualize what the vision transformer is looking at\"\"\"\n    \n    # Use attention from the last layer, first head\n    attn = attention_maps[-1][0, 0]  # (seq_len, seq_len)\n    \n    # Get attention from class token to all patches\n    cls_attn = attn[0, 1:]  # Exclude class token to class token attention\n    \n    # Reshape to spatial dimensions\n    num_patches_per_side = int(len(cls_attn) ** 0.5)\n    attn_map = cls_attn.reshape(num_patches_per_side, num_patches_per_side)\n    \n    # Resize to image size\n    attn_map = torch.nn.functional.interpolate(\n        attn_map.unsqueeze(0).unsqueeze(0),\n        size=(224, 224),\n        mode='bilinear'\n    ).squeeze()\n    \n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(image.permute(1, 2, 0))\n    plt.title(\"Original Image\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(image.permute(1, 2, 0))\n    plt.imshow(attn_map.detach().numpy(), alpha=0.6, cmap='hot')\n    plt.title(\"Attention Map\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualize attention (you would use a real image)\ndummy_image = torch.randn(3, 224, 224)\nvisualize_attention(dummy_image, attention_maps)\n```\n\n::: {.cell-output .cell-output-display}\n![](08-modern-vision-models_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n## Foundation Models: The DINOv2 Revolution\n\nNow we reach the cutting edge: **Foundation Models**. DINOv2 (Distillation with No Labels v2) represents a paradigm shift:\n\n- **Self-supervised learning**: No labels needed!\n- **Universal features**: Works for any vision task\n- **Incredible performance**: Often beats supervised methods\n\n### Using DINOv2 with HuggingFace\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Install required packages\n# !pip install transformers torch torchvision\n\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport requests\n\n# Load DINOv2 model and processor\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\nprint(f\"Loaded DINOv2 model: {model_name}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\ndef extract_dinov2_features(image_path_or_url):\n    \"\"\"Extract features using DINOv2\"\"\"\n    \n    # Load image\n    if image_path_or_url.startswith('http'):\n        image = Image.open(requests.get(image_path_or_url, stream=True).raw)\n    else:\n        image = Image.open(image_path_or_url)\n    \n    # Process image\n    inputs = processor(images=image, return_tensors=\"pt\")\n    \n    # Extract features\n    with torch.no_grad():\n        outputs = model(**inputs)\n        features = outputs.last_hidden_state\n        \n        # Get CLS token (global image representation)\n        cls_features = features[:, 0]  # Shape: (1, 768)\n        \n        # Get patch features (local representations)\n        patch_features = features[:, 1:]  # Shape: (1, num_patches, 768)\n    \n    return {\n        'cls_features': cls_features,\n        'patch_features': patch_features,\n        'image': image\n    }\n\n# Example usage\ndef demo_dinov2_features():\n    \"\"\"Demonstrate DINOv2 feature extraction\"\"\"\n    \n    # Create dummy image for demo (you would use real images)\n    dummy_image = Image.new('RGB', (224, 224), color='red')\n    \n    # Save temporarily\n    dummy_image.save('temp_image.jpg')\n    \n    # Extract features\n    result = extract_dinov2_features('temp_image.jpg')\n    \n    print(\"DINOv2 Feature Extraction Results:\")\n    print(f\"Global features shape: {result['cls_features'].shape}\")\n    print(f\"Patch features shape: {result['patch_features'].shape}\")\n    \n    # Visualize features\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    plt.imshow(result['image'])\n    plt.title(\"Input Image\")\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(result['cls_features'].squeeze().numpy())\n    plt.title(\"Global Features (768 dimensions)\")\n    plt.xlabel(\"Dimension\")\n    plt.ylabel(\"Value\")\n    \n    plt.subplot(1, 3, 3)\n    # Visualize patch features as heatmap\n    patch_norms = torch.norm(result['patch_features'].squeeze(), dim=1)\n    patch_size = int(len(patch_norms) ** 0.5)\n    patch_map = patch_norms.reshape(patch_size, patch_size)\n    plt.imshow(patch_map.numpy(), cmap='viridis')\n    plt.title(\"Patch Feature Magnitudes\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return result\n\ndemo_result = demo_dinov2_features()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded DINOv2 model: facebook/dinov2-base\nModel parameters: 86,580,480\nDINOv2 Feature Extraction Results:\nGlobal features shape: torch.Size([1, 768])\nPatch features shape: torch.Size([1, 256, 768])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](08-modern-vision-models_files/figure-html/cell-6-output-2.png){}\n:::\n:::\n\n\n### Building a DINOv2-Powered Image Similarity Engine\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nclass DINOv2SimilarityEngine:\n    def __init__(self):\n        self.processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n        self.model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        self.model.eval()\n        self.image_database = {}\n    \n    def extract_features(self, image):\n        \"\"\"Extract DINOv2 features from an image\"\"\"\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            # Use CLS token as global image representation\n            features = outputs.last_hidden_state[:, 0]\n        \n        return features\n    \n    def add_image(self, image_id, image):\n        \"\"\"Add an image to the database\"\"\"\n        features = self.extract_features(image)\n        self.image_database[image_id] = {\n            'features': features,\n            'image': image\n        }\n        print(f\"Added image '{image_id}' to database\")\n    \n    def find_similar_images(self, query_image, top_k=5):\n        \"\"\"Find most similar images in the database\"\"\"\n        query_features = self.extract_features(query_image)\n        \n        similarities = {}\n        \n        for image_id, data in self.image_database.items():\n            # Compute cosine similarity\n            similarity = torch.cosine_similarity(\n                query_features, data['features'], dim=1\n            ).item()\n            similarities[image_id] = similarity\n        \n        # Sort by similarity\n        sorted_similarities = sorted(\n            similarities.items(), key=lambda x: x[1], reverse=True\n        )\n        \n        return sorted_similarities[:top_k]\n    \n    def visualize_results(self, query_image, similar_images):\n        \"\"\"Visualize similarity search results\"\"\"\n        plt.figure(figsize=(15, 8))\n        \n        # Query image\n        plt.subplot(2, len(similar_images) + 1, 1)\n        plt.imshow(query_image)\n        plt.title(\"Query Image\")\n        plt.axis('off')\n        \n        # Similar images\n        for i, (image_id, similarity) in enumerate(similar_images):\n            plt.subplot(2, len(similar_images) + 1, i + 2)\n            plt.imshow(self.image_database[image_id]['image'])\n            plt.title(f\"{image_id}\\nSimilarity: {similarity:.3f}\")\n            plt.axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n\n# Create similarity engine\nsimilarity_engine = DINOv2SimilarityEngine()\n\n# Demo with dummy images (you would use real images)\ndef demo_similarity_engine():\n    \"\"\"Demonstrate the similarity engine\"\"\"\n    \n    # Create some dummy images with different colors\n    colors = ['red', 'blue', 'green', 'yellow', 'purple']\n    \n    for color in colors:\n        dummy_img = Image.new('RGB', (224, 224), color=color)\n        similarity_engine.add_image(f\"{color}_image\", dummy_img)\n    \n    # Query with a red image\n    query_img = Image.new('RGB', (224, 224), color='red')\n    \n    # Find similar images\n    similar = similarity_engine.find_similar_images(query_img, top_k=3)\n    \n    print(\"Most similar images:\")\n    for image_id, similarity in similar:\n        print(f\"  {image_id}: {similarity:.3f}\")\n    \n    # Visualize results\n    similarity_engine.visualize_results(query_img, similar)\n\ndemo_similarity_engine()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAdded image 'red_image' to database\nAdded image 'blue_image' to database\nAdded image 'green_image' to database\nAdded image 'yellow_image' to database\nAdded image 'purple_image' to database\nMost similar images:\n  red_image: 1.000\n  yellow_image: 0.871\n  green_image: 0.858\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](08-modern-vision-models_files/figure-html/cell-7-output-2.png){}\n:::\n:::\n\n\n## Comparing All Approaches: The Ultimate Showdown\n\nLet's compare all the approaches we've learned:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nclass VisionModelComparison:\n    def __init__(self):\n        self.models = {\n            'ResNet-18': models.resnet18(pretrained=True),\n            'ResNet-50': models.resnet50(pretrained=True),\n            'EfficientNet-B0': efficientnet_b0(pretrained=True),\n            'ViT-Base': None,  # Would load from transformers\n            'DINOv2-Base': None  # Already loaded above\n        }\n    \n    def compare_models(self):\n        \"\"\"Compare different vision models\"\"\"\n        comparison_data = []\n        \n        for name, model in self.models.items():\n            if model is not None:\n                params = sum(p.numel() for p in model.parameters())\n                size_mb = params * 4 / (1024 * 1024)\n                \n                comparison_data.append({\n                    'Model': name,\n                    'Parameters (M)': f\"{params / 1e6:.1f}\",\n                    'Size (MB)': f\"{size_mb:.1f}\",\n                    'Year': self.get_year(name),\n                    'Type': self.get_type(name)\n                })\n        \n        return comparison_data\n    \n    def get_year(self, name):\n        year_map = {\n            'ResNet-18': 2015,\n            'ResNet-50': 2015,\n            'EfficientNet-B0': 2019,\n            'ViT-Base': 2020,\n            'DINOv2-Base': 2023\n        }\n        return year_map.get(name, 'Unknown')\n    \n    def get_type(self, name):\n        if 'ResNet' in name or 'EfficientNet' in name:\n            return 'CNN'\n        elif 'ViT' in name:\n            return 'Transformer'\n        elif 'DINOv2' in name:\n            return 'Foundation Model'\n        return 'Unknown'\n    \n    def visualize_comparison(self, data):\n        \"\"\"Visualize model comparison\"\"\"\n        import pandas as pd\n        \n        df = pd.DataFrame(data)\n        \n        plt.figure(figsize=(15, 10))\n        \n        # Parameters vs Year\n        plt.subplot(2, 2, 1)\n        for model_type in df['Type'].unique():\n            subset = df[df['Type'] == model_type]\n            plt.scatter(subset['Year'], subset['Parameters (M)'].astype(float), \n                       label=model_type, s=100)\n        \n        plt.xlabel('Year')\n        plt.ylabel('Parameters (Millions)')\n        plt.title('Model Size Evolution')\n        plt.legend()\n        plt.grid(True)\n        \n        # Model types distribution\n        plt.subplot(2, 2, 2)\n        type_counts = df['Type'].value_counts()\n        plt.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n        plt.title('Model Types Distribution')\n        \n        # Size comparison\n        plt.subplot(2, 2, 3)\n        plt.bar(df['Model'], df['Size (MB)'].astype(float))\n        plt.xticks(rotation=45)\n        plt.ylabel('Size (MB)')\n        plt.title('Model Size Comparison')\n        \n        # Timeline\n        plt.subplot(2, 2, 4)\n        timeline_data = df.sort_values('Year')\n        plt.plot(timeline_data['Year'], range(len(timeline_data)), 'o-')\n        for i, (idx, row) in enumerate(timeline_data.iterrows()):\n            plt.annotate(row['Model'], (row['Year'], i), \n                        xytext=(5, 0), textcoords='offset points')\n        plt.xlabel('Year')\n        plt.ylabel('Model Index')\n        plt.title('Vision Models Timeline')\n        plt.grid(True)\n        \n        plt.tight_layout()\n        plt.show()\n\n# Run comparison\ncomparison = VisionModelComparison()\ncomparison_data = comparison.compare_models()\n\nprint(\"Vision Models Comparison:\")\nprint(\"-\" * 80)\nfor data in comparison_data:\n    print(f\"{data['Model']:15} | {data['Year']} | {data['Type']:15} | \"\n          f\"{data['Parameters (M)']:>8} M | {data['Size (MB)']:>8} MB\")\n\ncomparison.visualize_comparison(comparison_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVision Models Comparison:\n--------------------------------------------------------------------------------\nResNet-18       | 2015 | CNN             |     11.7 M |     44.6 MB\nResNet-50       | 2015 | CNN             |     25.6 M |     97.5 MB\nEfficientNet-B0 | 2019 | CNN             |      5.3 M |     20.2 MB\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](08-modern-vision-models_files/figure-html/cell-8-output-2.png){}\n:::\n:::\n\n\n## The Future: What's Next?\n\nAs we look ahead, several trends are shaping the future of computer vision:\n\n### 1. **Multimodal Foundation Models**\n```python\n# Future: Models that understand both vision and language\n# Example: CLIP, GPT-4V, LLaVA\n```\n\n### 2. **Efficient Architectures**\n```python\n# Trend: Smaller, faster models for mobile devices\n# Example: MobileViT, EfficientViT\n```\n\n### 3. **Self-Supervised Learning**\n```python\n# Growing trend: Learning without labels\n# Example: MAE, SimCLR, DINOv2\n```\n\n## Your Challenge: Build a Modern Vision Pipeline\n\nNow it's your turn to build a complete modern vision system:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nclass ModernVisionPipeline:\n    def __init__(self):\n        # Load multiple models for different tasks\n        self.classification_model = models.resnet50(pretrained=True)\n        self.feature_extractor = None  # DINOv2 model\n        self.similarity_engine = DINOv2SimilarityEngine()\n    \n    def classify_image(self, image):\n        \"\"\"Classify image using ResNet\"\"\"\n        # Your implementation here\n        pass\n    \n    def extract_features(self, image):\n        \"\"\"Extract features using DINOv2\"\"\"\n        # Your implementation here\n        pass\n    \n    def find_similar_images(self, query_image, database):\n        \"\"\"Find similar images using DINOv2 features\"\"\"\n        # Your implementation here\n        pass\n    \n    def analyze_image(self, image):\n        \"\"\"Complete image analysis pipeline\"\"\"\n        results = {\n            'classification': self.classify_image(image),\n            'features': self.extract_features(image),\n            'similar_images': self.find_similar_images(image, self.image_database)\n        }\n        return results\n\n# Build your pipeline!\npipeline = ModernVisionPipeline()\n```\n:::\n\n\n## What's Coming Next?\n\nIn our next post, [**\"Your First CV Project: Putting It All Together\"**](../08-first-cv-project/), we'll:\n\n- **Build a complete computer vision application**\n- **Combine classical and modern techniques**\n- **Deploy your model for real-world use**\n- **Create an interactive demo**\n\nYou've just learned about the most advanced vision models ever createdâ€”next, we'll put everything together into a real project!\n\n## Key Takeaways\n\n- **CNNs dominated** computer vision for a decade\n- **Vision Transformers** brought attention mechanisms to vision\n- **Foundation models** like DINOv2 learn universal representations\n- **Self-supervised learning** eliminates the need for labels\n- **Modern pipelines** combine multiple approaches\n- **The field evolves rapidly**â€”stay curious and keep learning!\n\n:::{.callout-tip}\n## Hands-On Lab\nReady to experiment with cutting-edge vision models? Try the complete interactive notebook: [**Modern Vision Models Lab**](https://colab.research.google.com/drive/1Modern_Vision_Models_123456)\n\nCompare CNNs, Vision Transformers, and DINOv2 on your own images!\n:::\n\n:::{.callout-note}\n## Series Navigation\n- **Previous**: [Why Deep Learning? When Classical Methods Hit the Wall](07-why-deep-learning.qmd)\n- **Next**: [Your First CV Project: Putting It All Together](09-first-cv-project.qmd)\n- **Series Home**: [Computer Vision Foundations](../computer-vision-foundations.qmd)\n:::\n\n---\n\n*You've just explored the cutting edge of computer vision! From ResNet's skip connections to DINOv2's self-supervised learningâ€”you now understand the models that power today's AI applications. Next, we'll build something amazing with all this knowledge!* \n\n",
    "supporting": [
      "08-modern-vision-models_files"
    ],
    "filters": [],
    "includes": {}
  }
}