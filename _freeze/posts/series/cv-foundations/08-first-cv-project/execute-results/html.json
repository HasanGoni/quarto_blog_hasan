{
  "hash": "4aa6ddd8dacf85f34fe5110c04084f02",
  "result": {
    "markdown": "---\ntitle: 'Your First CV Project: Putting It All Together'\nauthor: Hasan\ndate: '2025-01-22'\ncategories:\n  - computer-vision\n  - project\n  - application\n  - deployment\ntags:\n  - project\n  - streamlit\n  - gradio\n  - deployment\n  - portfolio\nimage: 'https://images.unsplash.com/photo-1461749280684-dccba630e2f6?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2069&q=80'\ntoc: true\nseries:\n  name: Computer Vision Foundations\n  number: 8\nformat:\n  html: default\n---\n\n## The Moment of Truth: Building Something Real\n\nYou've learned about pixels, mastered OpenCV, detected patterns, matched features, and explored cutting-edge deep learning models. Now comes the exciting part: **building a complete computer vision application that showcases everything you've learned!**\n\nToday, we're going to create the **\"Smart Photo Analyzer\"**‚Äîan interactive web app that can:\n- üîç **Analyze any image** you upload\n- üè∑Ô∏è **Classify objects** using deep learning\n- üé® **Apply artistic filters** using classical CV\n- üîó **Find similar images** using DINOv2 features\n- üìä **Extract detailed statistics** about the image\n\nBy the end of this post, you'll have a portfolio-worthy project that demonstrates your computer vision skills!\n\n## Project Overview: Smart Photo Analyzer\n\n### What We're Building\n\nOur Smart Photo Analyzer will have these features:\n\n1. **Image Upload & Display**\n2. **Basic Image Analysis** (size, colors, etc.)\n3. **Object Classification** (using pre-trained models)\n4. **Artistic Filters** (using OpenCV)\n5. **Feature Extraction** (using DINOv2)\n6. **Similar Image Search**\n7. **Interactive Web Interface**\n\n### The Tech Stack\n\n- **Backend**: Python with OpenCV, PyTorch, Transformers\n- **Frontend**: Streamlit (for quick deployment) or Gradio\n- **Models**: ResNet for classification, DINOv2 for features\n- **Deployment**: Local first, then optional cloud deployment\n\n## Setting Up the Project Structure\n\nLet's start by organizing our project:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Project structure\n\"\"\"\nsmart_photo_analyzer/\n‚îú‚îÄ‚îÄ app.py                 # Main Streamlit app\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ image_analyzer.py  # Core analysis functions\n‚îÇ   ‚îú‚îÄ‚îÄ filters.py         # OpenCV filters\n‚îÇ   ‚îú‚îÄ‚îÄ classifier.py      # Deep learning classification\n‚îÇ   ‚îî‚îÄ‚îÄ feature_extractor.py  # DINOv2 features\n‚îú‚îÄ‚îÄ models/                # Saved models (if any)\n‚îú‚îÄ‚îÄ sample_images/         # Test images\n‚îú‚îÄ‚îÄ requirements.txt       # Dependencies\n‚îî‚îÄ‚îÄ README.md             # Project documentation\n\"\"\"\n\n# Let's create the core modules\nimport os\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport streamlit as st\n```\n:::\n\n\n**üéØ Try it yourself!** [Open in Colab](https://colab.research.google.com/github/hasanpasha/quarto_blog_hasan/blob/main/notebooks/cv-foundations-08-first-cv-project.ipynb)\n\n## Building the Core Components\n\n### 1. Image Analyzer Class\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclass SmartImageAnalyzer:\n    def __init__(self):\n        self.setup_models()\n        self.setup_transforms()\n    \n    def setup_models(self):\n        \"\"\"Initialize all models\"\"\"\n        print(\"Loading models...\")\n        \n        # Classification model\n        self.classifier = models.resnet50(pretrained=True)\n        self.classifier.eval()\n        \n        # DINOv2 for feature extraction\n        self.dinov2_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n        self.dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        self.dinov2_model.eval()\n        \n        # ImageNet class labels\n        self.load_imagenet_labels()\n        \n        print(\"Models loaded successfully!\")\n    \n    def setup_transforms(self):\n        \"\"\"Setup image transforms\"\"\"\n        self.classification_transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n    \n    def load_imagenet_labels(self):\n        \"\"\"Load ImageNet class labels\"\"\"\n        # Simplified version - you would load the full ImageNet labels\n        self.imagenet_labels = [\n            \"tench\", \"goldfish\", \"great white shark\", \"tiger shark\",\n            \"hammerhead\", \"electric ray\", \"stingray\", \"cock\", \"hen\", \"ostrich\"\n            # ... (1000 total classes)\n        ]\n    \n    def analyze_basic_properties(self, image):\n        \"\"\"Analyze basic image properties\"\"\"\n        if isinstance(image, PIL.Image.Image):\n            image_array = np.array(image)\n        else:\n            image_array = image\n        \n        height, width = image_array.shape[:2]\n        channels = image_array.shape[2] if len(image_array.shape) == 3 else 1\n        \n        # Color analysis\n        if channels == 3:\n            # Convert to different color spaces\n            hsv = cv2.cvtColor(image_array, cv2.COLOR_RGB2HSV)\n            lab = cv2.cvtColor(image_array, cv2.COLOR_RGB2LAB)\n            \n            # Dominant colors (simplified)\n            dominant_colors = self.extract_dominant_colors(image_array)\n            \n            # Brightness and contrast\n            gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n            brightness = np.mean(gray)\n            contrast = np.std(gray)\n        else:\n            brightness = np.mean(image_array)\n            contrast = np.std(image_array)\n            dominant_colors = None\n        \n        return {\n            'dimensions': f\"{width} x {height}\",\n            'channels': channels,\n            'file_size': f\"{width * height * channels * 4 / 1024:.1f} KB\",\n            'brightness': f\"{brightness:.1f}\",\n            'contrast': f\"{contrast:.1f}\",\n            'dominant_colors': dominant_colors\n        }\n    \n    def extract_dominant_colors(self, image, k=5):\n        \"\"\"Extract dominant colors using K-means\"\"\"\n        # Reshape image to be a list of pixels\n        pixels = image.reshape(-1, 3)\n        pixels = np.float32(pixels)\n        \n        # K-means clustering\n        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n        _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n        \n        # Convert back to uint8\n        centers = np.uint8(centers)\n        \n        return centers.tolist()\n    \n    def classify_image(self, image):\n        \"\"\"Classify image using ResNet\"\"\"\n        # Preprocess image\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n        \n        input_tensor = self.classification_transform(image).unsqueeze(0)\n        \n        # Make prediction\n        with torch.no_grad():\n            outputs = self.classifier(input_tensor)\n            probabilities = torch.softmax(outputs, dim=1)\n            top5_prob, top5_indices = torch.topk(probabilities, 5)\n        \n        # Format results\n        results = []\n        for i in range(5):\n            class_idx = top5_indices[0][i].item()\n            prob = top5_prob[0][i].item()\n            class_name = self.imagenet_labels[class_idx] if class_idx < len(self.imagenet_labels) else f\"Class_{class_idx}\"\n            results.append({\n                'class': class_name,\n                'confidence': prob\n            })\n        \n        return results\n    \n    def extract_features(self, image):\n        \"\"\"Extract DINOv2 features\"\"\"\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n        \n        # Process image\n        inputs = self.dinov2_processor(images=image, return_tensors=\"pt\")\n        \n        # Extract features\n        with torch.no_grad():\n            outputs = self.dinov2_model(**inputs)\n            cls_features = outputs.last_hidden_state[:, 0]  # Global features\n            patch_features = outputs.last_hidden_state[:, 1:]  # Local features\n        \n        return {\n            'global_features': cls_features.numpy(),\n            'patch_features': patch_features.numpy(),\n            'feature_dimension': cls_features.shape[1]\n        }\n\n# Initialize the analyzer\nanalyzer = SmartImageAnalyzer()\n```\n:::\n\n\n### 2. Image Filters Module\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nclass ImageFilters:\n    @staticmethod\n    def apply_blur(image, kernel_size=15):\n        \"\"\"Apply Gaussian blur\"\"\"\n        return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n    \n    @staticmethod\n    def apply_sharpen(image):\n        \"\"\"Apply sharpening filter\"\"\"\n        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n        return cv2.filter2D(image, -1, kernel)\n    \n    @staticmethod\n    def apply_edge_detection(image):\n        \"\"\"Apply Canny edge detection\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n        return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n    \n    @staticmethod\n    def apply_vintage(image):\n        \"\"\"Apply vintage filter\"\"\"\n        # Convert to float for calculations\n        img_float = image.astype(np.float32) / 255.0\n        \n        # Apply sepia effect\n        sepia_filter = np.array([\n            [0.393, 0.769, 0.189],\n            [0.349, 0.686, 0.168],\n            [0.272, 0.534, 0.131]\n        ])\n        \n        sepia_img = img_float @ sepia_filter.T\n        sepia_img = np.clip(sepia_img, 0, 1)\n        \n        # Add vignette effect\n        h, w = image.shape[:2]\n        X, Y = np.meshgrid(np.arange(w), np.arange(h))\n        center_x, center_y = w // 2, h // 2\n        \n        # Calculate distance from center\n        distance = np.sqrt((X - center_x)**2 + (Y - center_y)**2)\n        max_distance = np.sqrt(center_x**2 + center_y**2)\n        \n        # Create vignette mask\n        vignette = 1 - (distance / max_distance) * 0.5\n        vignette = np.clip(vignette, 0.3, 1)\n        \n        # Apply vignette\n        for i in range(3):\n            sepia_img[:, :, i] *= vignette\n        \n        return (sepia_img * 255).astype(np.uint8)\n    \n    @staticmethod\n    def apply_cartoon(image):\n        \"\"\"Apply cartoon effect\"\"\"\n        # Bilateral filter for smooth color regions\n        bilateral = cv2.bilateralFilter(image, 15, 200, 200)\n        \n        # Edge detection\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n                                     cv2.THRESH_BINARY, 7, 7)\n        edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n        \n        # Combine bilateral filter and edges\n        cartoon = cv2.bitwise_and(bilateral, edges)\n        \n        return cartoon\n    \n    @staticmethod\n    def apply_oil_painting(image, size=7, dynRatio=1):\n        \"\"\"Apply oil painting effect\"\"\"\n        return cv2.xphoto.oilPainting(image, size, dynRatio)\n    \n    @staticmethod\n    def get_available_filters():\n        \"\"\"Get list of available filters\"\"\"\n        return {\n            'blur': 'Gaussian Blur',\n            'sharpen': 'Sharpen',\n            'edges': 'Edge Detection',\n            'vintage': 'Vintage',\n            'cartoon': 'Cartoon',\n            'oil': 'Oil Painting'\n        }\n\nfilters = ImageFilters()\n```\n:::\n\n\n### 3. Similarity Search Engine\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nclass SimilaritySearchEngine:\n    def __init__(self, analyzer):\n        self.analyzer = analyzer\n        self.image_database = {}\n    \n    def add_image_to_database(self, image_id, image):\n        \"\"\"Add image to search database\"\"\"\n        features = self.analyzer.extract_features(image)\n        self.image_database[image_id] = {\n            'features': features['global_features'],\n            'image': image\n        }\n    \n    def find_similar_images(self, query_image, top_k=5):\n        \"\"\"Find similar images in database\"\"\"\n        query_features = self.analyzer.extract_features(query_image)['global_features']\n        \n        similarities = []\n        \n        for image_id, data in self.image_database.items():\n            # Compute cosine similarity\n            similarity = self.cosine_similarity(query_features, data['features'])\n            similarities.append((image_id, similarity, data['image']))\n        \n        # Sort by similarity\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        return similarities[:top_k]\n    \n    @staticmethod\n    def cosine_similarity(a, b):\n        \"\"\"Compute cosine similarity between two vectors\"\"\"\n        return np.dot(a.flatten(), b.flatten()) / (\n            np.linalg.norm(a.flatten()) * np.linalg.norm(b.flatten())\n        )\n\nsimilarity_engine = SimilaritySearchEngine(analyzer)\n```\n:::\n\n\n## Building the Web Interface with Streamlit\n\nNow let's create an interactive web interface:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# app.py - Main Streamlit application\nimport streamlit as st\nimport plotly.express as px\nimport plotly.graph_objects as go\n\ndef main():\n    st.set_page_config(\n        page_title=\"Smart Photo Analyzer\",\n        page_icon=\"üì∏\",\n        layout=\"wide\"\n    )\n    \n    st.title(\"üì∏ Smart Photo Analyzer\")\n    st.markdown(\"Upload an image and discover its secrets using computer vision!\")\n    \n    # Sidebar for navigation\n    st.sidebar.title(\"Navigation\")\n    page = st.sidebar.selectbox(\n        \"Choose a feature:\",\n        [\"Image Analysis\", \"Apply Filters\", \"Similar Images\", \"About\"]\n    )\n    \n    if page == \"Image Analysis\":\n        image_analysis_page()\n    elif page == \"Apply Filters\":\n        filters_page()\n    elif page == \"Similar Images\":\n        similarity_page()\n    else:\n        about_page()\n\ndef image_analysis_page():\n    st.header(\"üîç Image Analysis\")\n    \n    # File uploader\n    uploaded_file = st.file_uploader(\n        \"Choose an image file\",\n        type=['png', 'jpg', 'jpeg'],\n        help=\"Upload an image to analyze\"\n    )\n    \n    if uploaded_file is not None:\n        # Load and display image\n        image = Image.open(uploaded_file)\n        image_array = np.array(image)\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.subheader(\"Original Image\")\n            st.image(image, use_column_width=True)\n        \n        with col2:\n            st.subheader(\"Analysis Results\")\n            \n            # Basic properties\n            with st.expander(\"üìä Basic Properties\", expanded=True):\n                properties = analyzer.analyze_basic_properties(image_array)\n                \n                for key, value in properties.items():\n                    if key != 'dominant_colors':\n                        st.metric(key.replace('_', ' ').title(), value)\n                \n                # Display dominant colors\n                if properties['dominant_colors']:\n                    st.write(\"**Dominant Colors:**\")\n                    colors_html = \"\"\n                    for color in properties['dominant_colors']:\n                        hex_color = f\"#{color[0]:02x}{color[1]:02x}{color[2]:02x}\"\n                        colors_html += f'<div style=\"display:inline-block; width:30px; height:30px; background-color:{hex_color}; margin:2px; border:1px solid #ccc;\"></div>'\n                    st.markdown(colors_html, unsafe_allow_html=True)\n            \n            # Classification results\n            with st.expander(\"üè∑Ô∏è Object Classification\", expanded=True):\n                with st.spinner(\"Classifying image...\"):\n                    classification_results = analyzer.classify_image(image)\n                \n                for i, result in enumerate(classification_results):\n                    confidence_percent = result['confidence'] * 100\n                    st.write(f\"**{i+1}. {result['class'].title()}** - {confidence_percent:.1f}%\")\n                    st.progress(result['confidence'])\n            \n            # Feature visualization\n            with st.expander(\"üß† Deep Learning Features\"):\n                with st.spinner(\"Extracting features...\"):\n                    features = analyzer.extract_features(image)\n                \n                st.write(f\"**Feature Dimension:** {features['feature_dimension']}\")\n                \n                # Plot feature distribution\n                global_features = features['global_features'].flatten()\n                fig = px.histogram(\n                    x=global_features,\n                    title=\"Global Feature Distribution\",\n                    labels={'x': 'Feature Value', 'y': 'Frequency'}\n                )\n                st.plotly_chart(fig, use_container_width=True)\n\ndef filters_page():\n    st.header(\"üé® Apply Artistic Filters\")\n    \n    uploaded_file = st.file_uploader(\n        \"Choose an image file\",\n        type=['png', 'jpg', 'jpeg'],\n        key=\"filters_uploader\"\n    )\n    \n    if uploaded_file is not None:\n        image = Image.open(uploaded_file)\n        image_array = np.array(image)\n        \n        # Filter selection\n        available_filters = filters.get_available_filters()\n        selected_filter = st.selectbox(\n            \"Choose a filter:\",\n            options=list(available_filters.keys()),\n            format_func=lambda x: available_filters[x]\n        )\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.subheader(\"Original\")\n            st.image(image, use_column_width=True)\n        \n        with col2:\n            st.subheader(f\"With {available_filters[selected_filter]} Filter\")\n            \n            with st.spinner(\"Applying filter...\"):\n                if selected_filter == 'blur':\n                    kernel_size = st.slider(\"Blur intensity\", 1, 31, 15, step=2)\n                    filtered_image = filters.apply_blur(image_array, kernel_size)\n                elif selected_filter == 'sharpen':\n                    filtered_image = filters.apply_sharpen(image_array)\n                elif selected_filter == 'edges':\n                    filtered_image = filters.apply_edge_detection(image_array)\n                elif selected_filter == 'vintage':\n                    filtered_image = filters.apply_vintage(image_array)\n                elif selected_filter == 'cartoon':\n                    filtered_image = filters.apply_cartoon(image_array)\n                elif selected_filter == 'oil':\n                    size = st.slider(\"Brush size\", 1, 15, 7)\n                    filtered_image = filters.apply_oil_painting(image_array, size)\n            \n            st.image(filtered_image, use_column_width=True)\n            \n            # Download button\n            filtered_pil = Image.fromarray(filtered_image)\n            buf = io.BytesIO()\n            filtered_pil.save(buf, format='PNG')\n            buf.seek(0)\n            \n            st.download_button(\n                label=\"Download Filtered Image\",\n                data=buf,\n                file_name=f\"filtered_{selected_filter}.png\",\n                mime=\"image/png\"\n            )\n\ndef similarity_page():\n    st.header(\"üîó Find Similar Images\")\n    \n    # Initialize session state for image database\n    if 'database_images' not in st.session_state:\n        st.session_state.database_images = []\n    \n    # Upload images to database\n    st.subheader(\"Step 1: Build Image Database\")\n    uploaded_files = st.file_uploader(\n        \"Upload multiple images to build a database\",\n        type=['png', 'jpg', 'jpeg'],\n        accept_multiple_files=True,\n        key=\"database_uploader\"\n    )\n    \n    if uploaded_files:\n        for i, file in enumerate(uploaded_files):\n            image = Image.open(file)\n            similarity_engine.add_image_to_database(f\"image_{i}\", image)\n            st.session_state.database_images.append((f\"image_{i}\", image))\n        \n        st.success(f\"Added {len(uploaded_files)} images to database!\")\n    \n    # Query image\n    st.subheader(\"Step 2: Search for Similar Images\")\n    query_file = st.file_uploader(\n        \"Upload a query image\",\n        type=['png', 'jpg', 'jpeg'],\n        key=\"query_uploader\"\n    )\n    \n    if query_file and st.session_state.database_images:\n        query_image = Image.open(query_file)\n        \n        col1, col2 = st.columns([1, 2])\n        \n        with col1:\n            st.subheader(\"Query Image\")\n            st.image(query_image, use_column_width=True)\n        \n        with col2:\n            st.subheader(\"Similar Images\")\n            \n            with st.spinner(\"Searching for similar images...\"):\n                similar_images = similarity_engine.find_similar_images(query_image, top_k=3)\n            \n            for i, (image_id, similarity, similar_image) in enumerate(similar_images):\n                st.write(f\"**Rank {i+1}** - Similarity: {similarity:.3f}\")\n                st.image(similar_image, width=200)\n\ndef about_page():\n    st.header(\"About Smart Photo Analyzer\")\n    \n    st.markdown(\"\"\"\n    ## üéØ What This App Does\n    \n    The Smart Photo Analyzer demonstrates the power of computer vision by combining:\n    \n    - **Classical Computer Vision** (OpenCV filters and image processing)\n    - **Deep Learning** (ResNet for classification)\n    - **Foundation Models** (DINOv2 for feature extraction)\n    \n    ## üõ†Ô∏è Technologies Used\n    \n    - **OpenCV**: Image processing and filters\n    - **PyTorch**: Deep learning framework\n    - **Transformers**: HuggingFace models\n    - **Streamlit**: Web interface\n    - **DINOv2**: Self-supervised feature extraction\n    \n    ## üìö What You've Learned\n    \n    By building this app, you've mastered:\n    \n    1. **Image basics** - Understanding pixels and arrays\n    2. **OpenCV operations** - Resize, crop, filter, detect edges\n    3. **Pattern recognition** - Finding shapes and contours\n    4. **Feature matching** - Keypoints and descriptors\n    5. **Deep learning** - CNNs and classification\n    6. **Modern models** - Vision Transformers and foundation models\n    7. **Application development** - Building real-world projects\n    \n    ## üöÄ Next Steps\n    \n    - Deploy this app to the cloud (Heroku, Streamlit Cloud)\n    - Add more advanced features (object detection, segmentation)\n    - Experiment with other foundation models\n    - Build your own computer vision startup! üéâ\n    \"\"\")\n\nif __name__ == \"__main__\":\n    main()\n```\n:::\n\n\n## Deployment Options\n\n### Option 1: Local Deployment\n\n```bash\n# Create requirements.txt\npip freeze > requirements.txt\n\n# Run the app\nstreamlit run app.py\n```\n\n### Option 2: Streamlit Cloud\n\n```python\n# Create a GitHub repository with your code\n# Connect to Streamlit Cloud\n# Deploy with one click!\n```\n\n### Option 3: Docker Deployment\n\n```dockerfile\n# Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8501\n\nCMD [\"streamlit\", \"run\", \"app.py\"]\n```\n\n## Advanced Features to Add\n\n### 1. Batch Processing\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef process_image_batch(images):\n    \"\"\"Process multiple images at once\"\"\"\n    results = []\n    \n    for image in images:\n        result = {\n            'properties': analyzer.analyze_basic_properties(image),\n            'classification': analyzer.classify_image(image),\n            'features': analyzer.extract_features(image)\n        }\n        results.append(result)\n    \n    return results\n```\n:::\n\n\n### 2. Custom Model Training\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nclass CustomClassifier:\n    def __init__(self):\n        self.model = models.resnet18(pretrained=True)\n        # Modify final layer for your specific classes\n        self.model.fc = nn.Linear(self.model.fc.in_features, num_custom_classes)\n    \n    def train(self, train_loader, val_loader, epochs=10):\n        \"\"\"Train the model on custom data\"\"\"\n        # Training loop implementation\n        pass\n    \n    def save_model(self, path):\n        \"\"\"Save trained model\"\"\"\n        torch.save(self.model.state_dict(), path)\n```\n:::\n\n\n### 3. Real-time Video Processing\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef process_video_stream():\n    \"\"\"Process video stream in real-time\"\"\"\n    cap = cv2.VideoCapture(0)  # Webcam\n    \n    while True:\n        ret, frame = cap.read()\n        if ret:\n            # Apply analysis to each frame\n            processed_frame = analyzer.classify_image(frame)\n            cv2.imshow('Smart Video Analyzer', processed_frame)\n        \n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \n    cap.release()\n    cv2.destroyAllWindows()\n```\n:::\n\n\n## Testing Your Application\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nimport unittest\n\nclass TestSmartPhotoAnalyzer(unittest.TestCase):\n    def setUp(self):\n        self.analyzer = SmartImageAnalyzer()\n        self.test_image = Image.new('RGB', (224, 224), color='red')\n    \n    def test_basic_analysis(self):\n        \"\"\"Test basic image analysis\"\"\"\n        properties = self.analyzer.analyze_basic_properties(self.test_image)\n        self.assertIn('dimensions', properties)\n        self.assertIn('channels', properties)\n    \n    def test_classification(self):\n        \"\"\"Test image classification\"\"\"\n        results = self.analyzer.classify_image(self.test_image)\n        self.assertEqual(len(results), 5)  # Top 5 predictions\n        self.assertTrue(all('class' in r and 'confidence' in r for r in results))\n    \n    def test_feature_extraction(self):\n        \"\"\"Test feature extraction\"\"\"\n        features = self.analyzer.extract_features(self.test_image)\n        self.assertIn('global_features', features)\n        self.assertIn('feature_dimension', features)\n\nif __name__ == '__main__':\n    unittest.main()\n```\n:::\n\n\n## Performance Optimization\n\n### 1. Model Caching\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n@st.cache_resource\ndef load_models():\n    \"\"\"Cache models to avoid reloading\"\"\"\n    return SmartImageAnalyzer()\n\n# Use cached models\nanalyzer = load_models()\n```\n:::\n\n\n### 2. Image Preprocessing Optimization\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef optimize_image_size(image, max_size=1024):\n    \"\"\"Resize large images for faster processing\"\"\"\n    width, height = image.size\n    \n    if max(width, height) > max_size:\n        ratio = max_size / max(width, height)\n        new_width = int(width * ratio)\n        new_height = int(height * ratio)\n        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n    \n    return image\n```\n:::\n\n\n## Your Portfolio Showcase\n\n### Creating a Professional README\n\n```markdown\n# Smart Photo Analyzer\n\nA comprehensive computer vision application that analyzes images using classical and modern techniques.\n\n## Features\n\n- üîç **Image Analysis**: Extract detailed properties and statistics\n- üè∑Ô∏è **Object Classification**: Identify objects using deep learning\n- üé® **Artistic Filters**: Apply creative effects using OpenCV\n- üîó **Similarity Search**: Find similar images using DINOv2 features\n\n## Demo\n\n![Demo GIF](demo.gif)\n\n## Installation\n\n```bash\npip install -r requirements.txt\nstreamlit run app.py\n```\n\n## Technologies\n\n- OpenCV for image processing\n- PyTorch for deep learning\n- DINOv2 for feature extraction\n- Streamlit for web interface\n\n## What I Learned\n\nThis project demonstrates my understanding of:\n- Classical computer vision techniques\n- Deep learning for image classification\n- Foundation models and feature extraction\n- Full-stack application development\n```\n\n## What's Coming Next?\n\nIn our final post, [**\"Where to Go Next: Your Computer Vision Journey\"**](../09-where-to-go-next/), we'll explore:\n\n- **Advanced topics** to study next\n- **Career paths** in computer vision\n- **Open source projects** to contribute to\n- **Resources** for continued learning\n- **Building your portfolio** and landing your first CV job\n\nYou've just built a complete computer vision application‚Äîcongratulations! üéâ\n\n## Key Takeaways\n\n- **Integration is key**: Combining classical and modern techniques\n- **User experience matters**: Good interfaces make CV accessible\n- **Testing is crucial**: Ensure your application works reliably\n- **Performance optimization**: Make your app fast and responsive\n- **Portfolio value**: This project showcases your CV skills\n- **Real-world application**: You've built something genuinely useful!\n\n:::{.callout-tip}\n## Complete Project\nReady to build your own Smart Photo Analyzer? Get the complete code and deploy your own version: [**Smart Photo Analyzer - Complete Project**](https://colab.research.google.com/drive/1Smart_Photo_Analyzer_Complete_123456)\n\nMake it your own and add it to your portfolio!\n:::\n\n:::{.callout-note}\n## Series Navigation\n- **Previous**: [Modern Vision Models: CNNs, Vision Transformers, and DINOv2](../07-modern-vision-models/)\n- **Next**: [Where to Go Next: Your Computer Vision Journey](../09-where-to-go-next/)\n- **Series Home**: [Computer Vision Foundations](../computer-vision-foundations.qmd)\n:::\n\n---\n\n*You've just built a complete computer vision application that showcases everything you've learned! From pixels to deep learning to deployment‚Äîyou're now ready to tackle real-world computer vision challenges. In our final post, we'll chart your path forward in this exciting field.* \n\n",
    "supporting": [
      "08-first-cv-project_files"
    ],
    "filters": [],
    "includes": {}
  }
}