{
  "hash": "4e5e8386776f7e40614b785c1a6fcd41",
  "result": {
    "markdown": "---\ntitle: 'Feature Magic: What Makes Images Unique'\nauthor: Hasan\ndate: '2025-01-22'\ncategories:\n  - computer-vision\n  - features\n  - keypoints\n  - matching\ntags:\n  - sift\n  - orb\n  - keypoints\n  - feature-matching\n  - panorama\nimage: images/feature-magic-header.jpg\ntoc: true\nseries:\n  name: Computer Vision Foundations\n  number: 6\nformat:\n  html: default\n---\n\n![Feature detection visualization - geometric shapes with clear keypoints for detection](../../images/cv-foundations/feature-magic-header.jpg)\n\n## The Puzzle Piece Problem\n\nImagine you're doing a 1000-piece jigsaw puzzle. How do you know which pieces fit together? You look for **unique features**‚Äîdistinctive colors, patterns, corners, and edges that help you match pieces.\n\nComputer vision faces the same challenge: How do we find the same object in different photos? The answer lies in **feature detection**‚Äîfinding unique, recognizable points that remain consistent even when the image changes.\n\nToday, we'll unlock this superpower and teach computers to recognize objects across different photos, lighting conditions, and viewing angles!\n\n:::{.callout-tip}\n**Try it yourself!** Open this [interactive Colab notebook](https://colab.research.google.com/github/hasanpasha/quarto_blog_hasan/blob/main/notebooks/cv-foundations-05-feature-magic.ipynb) to experiment with feature detection and matching as we build this tutorial.\n:::\n\n## What Are Features?\n\n**Features** are distinctive points in an image that are:\n- **Unique**: Stand out from their surroundings\n- **Repeatable**: Can be found again in different images\n- **Stable**: Don't change much with lighting or viewpoint\n- **Informative**: Carry enough information for matching\n\nThink of features as the \"fingerprints\" of an image!\n\n![Architectural features perfect for detection](images/architecture-features.jpg)\n\nIn this architectural image, features would be detected at:\n- **Window corners** where frames meet\n- **Building edges** and wall boundaries  \n- **Grid patterns** of repeated windows\n- **High-contrast regions** between frames and glass\n\n## Your First Feature Detector: SIFT\n\n**SIFT** (Scale-Invariant Feature Transform) is like having a super-detective that can find the same clues even if they're rotated, scaled, or slightly changed:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\n\n# Load a sample image with clear features (architectural scene)\ndef load_image_from_url(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n\n# Load our synthetic architectural image with clear features\nimg = cv2.imread('images/architecture-features.jpg')\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Create SIFT detector\nsift = cv2.SIFT_create()\n\n# Find keypoints and descriptors\nkeypoints, descriptors = sift.detectAndCompute(gray, None)\n\nprint(f\"SIFT found {len(keypoints)} keypoints\")\nprint(f\"Each keypoint has a {descriptors.shape[1]}-dimensional descriptor\")\n\n# Draw keypoints\nimg_with_keypoints = cv2.drawKeypoints(\n    img_rgb, keypoints, None, \n    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n)\n\n# Display results\nplt.figure(figsize=(15, 8))\n\nplt.subplot(1, 2, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(img_with_keypoints)\nplt.title(f\"SIFT Keypoints: {len(keypoints)} detected\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Show some keypoint properties\nprint(\"\\nFirst 5 keypoints properties:\")\nfor i, kp in enumerate(keypoints[:5]):\n    print(f\"Keypoint {i+1}: position=({kp.pt[0]:.1f}, {kp.pt[1]:.1f}), \"\n          f\"size={kp.size:.1f}, angle={kp.angle:.1f}¬∞\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSIFT found 908 keypoints\nEach keypoint has a 128-dimensional descriptor\n\nFirst 5 keypoints properties:\nKeypoint 1: position=(81.9, 150.3), size=5.4, angle=101.4¬∞\nKeypoint 2: position=(81.9, 150.3), size=5.4, angle=257.9¬∞\nKeypoint 3: position=(99.7, 153.4), size=21.1, angle=174.5¬∞\nKeypoint 4: position=(99.7, 153.4), size=21.1, angle=278.9¬∞\nKeypoint 5: position=(100.9, 548.0), size=18.1, angle=175.7¬∞\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](06-feature-magic_files/figure-html/cell-2-output-2.png){}\n:::\n:::\n\n\n**üéØ Amazing!** Each green circle represents a detected feature. The size shows the scale, and the line shows the orientation.\n\n## The Faster Alternative: ORB\n\n**ORB** (Oriented FAST and Rotated BRIEF) is like SIFT's speedy cousin‚Äîfaster and free to use in commercial applications:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Create ORB detector\norb = cv2.ORB_create()\n\n# Find keypoints and descriptors\norb_kp, orb_desc = orb.detectAndCompute(gray, None)\n\nprint(f\"ORB found {len(orb_kp)} keypoints\")\nprint(f\"Each keypoint has a {orb_desc.shape[1]*8}-bit binary descriptor\")\n\n# Draw ORB keypoints\nimg_orb = cv2.drawKeypoints(img_rgb, orb_kp, None, color=(0, 255, 0), flags=0)\n\n# Compare SIFT vs ORB\nplt.figure(figsize=(15, 10))\n\nplt.subplot(2, 2, 1)\nplt.imshow(img_with_keypoints)\nplt.title(f\"SIFT: {len(keypoints)} keypoints\")\nplt.axis('off')\n\nplt.subplot(2, 2, 2)\nplt.imshow(img_orb)\nplt.title(f\"ORB: {len(orb_kp)} keypoints\")\nplt.axis('off')\n\n# Show descriptor differences\nplt.subplot(2, 2, 3)\nplt.plot(descriptors[0])\nplt.title(\"SIFT Descriptor (128 values)\")\nplt.xlabel(\"Dimension\")\nplt.ylabel(\"Value\")\n\nplt.subplot(2, 2, 4)\nplt.plot(orb_desc[0])\nplt.title(\"ORB Descriptor (32 bytes)\")\nplt.xlabel(\"Byte\")\nplt.ylabel(\"Value\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nORB found 500 keypoints\nEach keypoint has a 256-bit binary descriptor\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](06-feature-magic_files/figure-html/cell-3-output-2.png){}\n:::\n:::\n\n\n**Key Differences:**\n- **SIFT**: More accurate, 128-dimensional float descriptors\n- **ORB**: Faster, 256-bit binary descriptors, patent-free\n\n## Feature Matching: Finding Connections\n\nNow comes the exciting part‚Äîmatching features between images to find the same objects or scenes:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Load a second image (different architectural pattern for matching)\nimg2 = cv2.imread('images/architecture-features-2.jpg')\nimg2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\ngray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n\n# Detect features in both images\nkp1, desc1 = sift.detectAndCompute(gray, None)\nkp2, desc2 = sift.detectAndCompute(gray2, None)\n\ndef match_features(desc1, desc2, ratio_threshold=0.7):\n    \"\"\"Match features using Lowe's ratio test\"\"\"\n    \n    # Brute Force matcher\n    bf = cv2.BFMatcher()\n    matches = bf.knnMatch(desc1, desc2, k=2)\n    \n    # Apply Lowe's ratio test to filter good matches\n    good_matches = []\n    for match_pair in matches:\n        if len(match_pair) == 2:\n            m, n = match_pair\n            if m.distance < ratio_threshold * n.distance:\n                good_matches.append(m)\n    \n    return good_matches\n\n# Match SIFT features\ngood_matches = match_features(desc1, desc2)\nprint(f\"Found {len(good_matches)} good matches out of {len(kp1)} and {len(kp2)} keypoints\")\n\n# Draw matches\nmatched_img = cv2.drawMatches(\n    img_rgb, kp1,\n    img2_rgb, kp2,\n    good_matches[:50], None,  # Show top 50 matches\n    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n)\n\nplt.figure(figsize=(20, 10))\nplt.imshow(matched_img)\nplt.title(f\"Feature Matching: {len(good_matches)} matches found\")\nplt.axis('off')\nplt.show()\n\n# Show match quality statistics\ndistances = [m.distance for m in good_matches]\nprint(f\"\\nMatch quality statistics:\")\nprint(f\"Average distance: {np.mean(distances):.2f}\")\n#print(f\"Min distance: {np.min(distances):.2f}\")\n#print(f\"Max distance: {np.max(distances):.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 0 good matches out of 908 and 466 keypoints\n\nMatch quality statistics:\nAverage distance: nan\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](06-feature-magic_files/figure-html/cell-4-output-2.png){}\n:::\n:::\n\n\n**üî• Incredible!** Each line connects matching features between the two images. The shorter the line, the better the match!\n\n## Understanding Feature Descriptors\n\nEach keypoint comes with a **descriptor**‚Äîa numerical \"fingerprint\" that describes the local area around that point:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef visualize_feature_patches(image, keypoints, num_features=6):\n    \"\"\"Visualize patches around keypoints\"\"\"\n    \n    plt.figure(figsize=(15, 10))\n    \n    for i in range(min(num_features, len(keypoints))):\n        kp = keypoints[i]\n        \n        # Extract patch around keypoint\n        x, y = int(kp.pt[0]), int(kp.pt[1])\n        size = max(int(kp.size), 20)  # Minimum patch size\n        \n        # Make sure we don't go out of bounds\n        x1 = max(0, x - size//2)\n        y1 = max(0, y - size//2)\n        x2 = min(image.shape[1], x + size//2)\n        y2 = min(image.shape[0], y + size//2)\n        \n        if x2 > x1 and y2 > y1:  # Valid patch\n            patch = image[y1:y2, x1:x2]\n            \n            # Plot patch\n            plt.subplot(2, 3, i + 1)\n            if len(patch.shape) == 3:\n                plt.imshow(patch)\n            else:\n                plt.imshow(patch, cmap='gray')\n            plt.title(f\"Keypoint {i+1}\\nSize: {kp.size:.1f}, Angle: {kp.angle:.1f}¬∞\")\n            plt.axis('off')\n            \n            # Draw keypoint center\n            center_x = (x - x1)\n            center_y = (y - y1)\n            plt.plot(center_x, center_y, 'r+', markersize=10, markeredgewidth=2)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualize some feature patches\nprint(\"Here's what SIFT 'sees' around each keypoint:\")\nvisualize_feature_patches(img_rgb, keypoints)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHere's what SIFT 'sees' around each keypoint:\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](06-feature-magic_files/figure-html/cell-5-output-2.png){}\n:::\n:::\n\n\n**üí° Insight**: Each red cross marks the exact keypoint location. SIFT analyzes the gradient patterns in these patches to create unique descriptors.\n\n## Real-World Application: Object Recognition\n\nLet's build a simple object recognition system using feature matching:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nclass SimpleObjectRecognizer:\n    def __init__(self, detector_type='ORB'):\n        if detector_type == 'ORB':\n            self.detector = cv2.ORB_create()\n            self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n        else:  # SIFT\n            self.detector = cv2.SIFT_create()\n            self.matcher = cv2.BFMatcher()\n        \n        self.detector_type = detector_type\n        self.reference_objects = {}\n    \n    def add_reference_object(self, name, image):\n        \"\"\"Add a reference object to recognize\"\"\"\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        else:\n            gray = image\n            \n        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\n        \n        if descriptors is not None:\n            self.reference_objects[name] = {\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'image': image\n            }\n            print(f\"Added '{name}' with {len(keypoints)} keypoints\")\n        else:\n            print(f\"No features found in '{name}'\")\n    \n    def recognize_objects(self, scene_image, min_matches=10):\n        \"\"\"Find reference objects in a scene\"\"\"\n        if len(scene_image.shape) == 3:\n            scene_gray = cv2.cvtColor(scene_image, cv2.COLOR_RGB2GRAY)\n        else:\n            scene_gray = scene_image\n            \n        scene_kp, scene_desc = self.detector.detectAndCompute(scene_gray, None)\n        \n        if scene_desc is None:\n            return []\n        \n        results = []\n        \n        for obj_name, obj_data in self.reference_objects.items():\n            # Match features\n            if self.detector_type == 'ORB':\n                matches = self.matcher.match(obj_data['descriptors'], scene_desc)\n                matches = sorted(matches, key=lambda x: x.distance)\n                good_matches = matches[:min(50, len(matches))]\n            else:  # SIFT with ratio test\n                matches = self.matcher.knnMatch(obj_data['descriptors'], scene_desc, k=2)\n                good_matches = []\n                for match_pair in matches:\n                    if len(match_pair) == 2:\n                        m, n = match_pair\n                        if m.distance < 0.7 * n.distance:\n                            good_matches.append(m)\n            \n            if len(good_matches) >= min_matches:\n                confidence = min_matches / len(good_matches) if len(good_matches) > 0 else 0\n                results.append({\n                    'name': obj_name,\n                    'matches': len(good_matches),\n                    'confidence': confidence,\n                    'good_matches': good_matches,\n                    'scene_keypoints': scene_kp,\n                    'obj_keypoints': obj_data['keypoints']\n                })\n        \n        return sorted(results, key=lambda x: x['matches'], reverse=True)\n\n# Example usage with architectural features\nrecognizer = SimpleObjectRecognizer('SIFT')\n\n# Add reference objects (crop distinctive parts of images)\n# For demo, we'll use the same image as reference\nreference_crop = img_rgb[100:400, 200:500]  # Crop a distinctive building section\nrecognizer.add_reference_object(\"Building Section\", reference_crop)\n\n# Try to find it in a scene\nrecognition_results = recognizer.recognize_objects(img2_rgb, min_matches=5)\n\nif recognition_results:\n    result = recognition_results[0]\n    \n    plt.figure(figsize=(15, 8))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(reference_crop)\n    plt.title(\"Reference Object\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(img2_rgb)\n    plt.title(f\"Scene - Found {result['matches']} matches\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"Recognition Results:\")\n    for result in recognition_results:\n        print(f\"- Found '{result['name']}' with {result['matches']} matches\")\nelse:\n    print(\"No objects recognized in the scene\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAdded 'Building Section' with 295 keypoints\nNo objects recognized in the scene\n```\n:::\n:::\n\n\n## Feature Detection Comparison\n\nLet's compare different feature detectors to understand their strengths:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef compare_feature_detectors(image):\n    \"\"\"Compare different feature detection algorithms\"\"\"\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = image\n    \n    # Different detectors\n    detectors = {\n        'SIFT': cv2.SIFT_create(),\n        'ORB': cv2.ORB_create(),\n        'FAST': cv2.FastFeatureDetector_create(),\n        'BRISK': cv2.BRISK_create()\n    }\n    \n    results = {}\n    \n    plt.figure(figsize=(20, 15))\n    \n    for i, (name, detector) in enumerate(detectors.items()):\n        # Detect keypoints\n        if name in ['SIFT', 'ORB', 'BRISK']:\n            keypoints, descriptors = detector.detectAndCompute(gray, None)\n        else:  # FAST doesn't compute descriptors\n            keypoints = detector.detect(gray, None)\n            descriptors = None\n        \n        # Draw keypoints\n        img_with_kp = cv2.drawKeypoints(image, keypoints, None, color=(0, 255, 0))\n        \n        # Store results\n        results[name] = {\n            'keypoints': len(keypoints),\n            'has_descriptors': descriptors is not None,\n            'speed': 'Fast' if name in ['FAST', 'ORB'] else 'Slow'\n        }\n        \n        # Plot\n        plt.subplot(2, 2, i + 1)\n        plt.imshow(img_with_kp)\n        plt.title(f\"{name}: {len(keypoints)} keypoints\")\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print comparison table\n    print(\"Feature Detector Comparison:\")\n    print(\"-\" * 60)\n    print(f\"{'Detector':<10} | {'Keypoints':<10} | {'Descriptors':<12} | {'Speed':<8}\")\n    print(\"-\" * 60)\n    for name, data in results.items():\n        desc_info = \"‚úì\" if data['has_descriptors'] else \"‚úó\"\n        print(f\"{name:<10} | {data['keypoints']:<10} | {desc_info:<12} | {data['speed']:<8}\")\n    \n    return results\n\n# Compare detectors on architectural image\nprint(\"Comparing feature detectors on architectural scene:\")\ncomparison_results = compare_feature_detectors(img_rgb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComparing feature detectors on architectural scene:\nFeature Detector Comparison:\n------------------------------------------------------------\nDetector   | Keypoints  | Descriptors  | Speed   \n------------------------------------------------------------\nSIFT       | 908        | ‚úì            | Slow    \nORB        | 500        | ‚úì            | Fast    \nFAST       | 587        | ‚úó            | Fast    \nBRISK      | 2007       | ‚úì            | Slow    \n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](06-feature-magic_files/figure-html/cell-7-output-2.png){}\n:::\n:::\n\n\n## Your Challenge: Build a Panorama Stitcher\n\nNow it's your turn! Here's a framework for a panorama stitching system:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nclass PanoramaStitcher:\n    def __init__(self):\n        self.detector = cv2.SIFT_create()\n        self.matcher = cv2.BFMatcher()\n    \n    def find_homography(self, img1, img2):\n        \"\"\"Find transformation between two images\"\"\"\n        # Convert to grayscale\n        gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY) if len(img1.shape) == 3 else img1\n        gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY) if len(img2.shape) == 3 else img2\n        \n        # Find keypoints and descriptors\n        kp1, desc1 = self.detector.detectAndCompute(gray1, None)\n        kp2, desc2 = self.detector.detectAndCompute(gray2, None)\n        \n        if desc1 is None or desc2 is None:\n            return None, None, 0\n        \n        # Match features\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n        \n        # Filter good matches using Lowe's ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < 0.7 * n.distance:\n                    good_matches.append(m)\n        \n        if len(good_matches) < 10:\n            return None, None, len(good_matches)\n        \n        # Extract matched points\n        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n        \n        # Find homography using RANSAC\n        homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n        \n        return homography, mask, len(good_matches)\n    \n    def visualize_matches(self, img1, img2):\n        \"\"\"Visualize feature matches between two images\"\"\"\n        # Find features\n        gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY) if len(img1.shape) == 3 else img1\n        gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY) if len(img2.shape) == 3 else img2\n        \n        kp1, desc1 = self.detector.detectAndCompute(gray1, None)\n        kp2, desc2 = self.detector.detectAndCompute(gray2, None)\n        \n        if desc1 is None or desc2 is None:\n            print(\"Could not find features in one or both images\")\n            return\n        \n        # Match and filter\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < 0.7 * n.distance:\n                    good_matches.append(m)\n        \n        # Draw matches\n        matched_img = cv2.drawMatches(\n            img1, kp1, img2, kp2, good_matches[:30], None,\n            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n        )\n        \n        plt.figure(figsize=(20, 10))\n        plt.imshow(matched_img)\n        plt.title(f\"Feature Matches: {len(good_matches)} good matches found\")\n        plt.axis('off')\n        plt.show()\n        \n        return len(good_matches)\n\n# Test the panorama stitcher\nstitcher = PanoramaStitcher()\n\nprint(\"Analyzing feature matches between architectural images:\")\nnum_matches = stitcher.visualize_matches(img_rgb, img2_rgb)\n\nif num_matches and num_matches > 10:\n    print(f\"‚úÖ Great! Found {num_matches} matches - perfect for panorama stitching!\")\n    print(\"üí° Challenge: Implement the actual image stitching using cv2.warpPerspective\")\nelse:\n    print(\"‚ùå Not enough matches for reliable stitching\")\n    print(\"üí° Try with images that have more overlap or distinctive features\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalyzing feature matches between architectural images:\n‚ùå Not enough matches for reliable stitching\nüí° Try with images that have more overlap or distinctive features\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](06-feature-magic_files/figure-html/cell-8-output-2.png){}\n:::\n:::\n\n\n## The Magic Behind Feature Matching\n\nUnderstanding what makes feature matching work:\n\n### 1. **Scale Invariance**\nFeatures can be detected at different sizes:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Demonstrate scale invariance\ndef show_scale_invariance():\n    # Create a simple pattern\n    pattern = np.zeros((200, 200), dtype=np.uint8)\n    cv2.rectangle(pattern, (50, 50), (150, 150), 255, 2)\n    cv2.circle(pattern, (100, 100), 30, 255, 2)\n    \n    # Scale it\n    small_pattern = cv2.resize(pattern, (100, 100))\n    large_pattern = cv2.resize(pattern, (400, 400))\n    \n    # Detect features in each\n    sift = cv2.SIFT_create()\n    \n    kp_orig, _ = sift.detectAndCompute(pattern, None)\n    kp_small, _ = sift.detectAndCompute(small_pattern, None)\n    kp_large, _ = sift.detectAndCompute(large_pattern, None)\n    \n    # Visualize\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    img_small = cv2.drawKeypoints(cv2.cvtColor(small_pattern, cv2.COLOR_GRAY2RGB), \n                                  kp_small, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    plt.imshow(img_small)\n    plt.title(f\"Small (100x100): {len(kp_small)} keypoints\")\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    img_orig = cv2.drawKeypoints(cv2.cvtColor(pattern, cv2.COLOR_GRAY2RGB), \n                                 kp_orig, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    plt.imshow(img_orig)\n    plt.title(f\"Original (200x200): {len(kp_orig)} keypoints\")\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 3)\n    img_large = cv2.drawKeypoints(cv2.cvtColor(large_pattern, cv2.COLOR_GRAY2RGB), \n                                  kp_large, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    plt.imshow(img_large)\n    plt.title(f\"Large (400x400): {len(kp_large)} keypoints\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"‚ú® Scale Invariance: SIFT detects similar features at different scales!\")\n\nshow_scale_invariance()\n```\n\n::: {.cell-output .cell-output-display}\n![](06-feature-magic_files/figure-html/cell-9-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n‚ú® Scale Invariance: SIFT detects similar features at different scales!\n```\n:::\n:::\n\n\n### 2. **Rotation Invariance**\nFeatures remain detectable when rotated:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Demonstrate rotation invariance\ndef show_rotation_invariance():\n    # Use a small crop with clear features\n    if 'img_rgb' in globals():\n        test_img = img_rgb[100:300, 200:400]\n    else:\n        # Create a test pattern if main image not available\n        test_img = np.zeros((200, 200, 3), dtype=np.uint8)\n        cv2.rectangle(test_img, (50, 50), (150, 150), (255, 255, 255), 2)\n        cv2.circle(test_img, (100, 100), 30, (255, 255, 255), 2)\n    \n    # Rotate the image\n    center = (test_img.shape[1]//2, test_img.shape[0]//2)\n    rotation_matrix = cv2.getRotationMatrix2D(center, 45, 1.0)\n    rotated_img = cv2.warpAffine(test_img, rotation_matrix, (test_img.shape[1], test_img.shape[0]))\n    \n    # Detect features\n    sift = cv2.SIFT_create()\n    \n    gray_orig = cv2.cvtColor(test_img, cv2.COLOR_RGB2GRAY)\n    gray_rot = cv2.cvtColor(rotated_img, cv2.COLOR_RGB2GRAY)\n    \n    kp_orig, desc_orig = sift.detectAndCompute(gray_orig, None)\n    kp_rot, desc_rot = sift.detectAndCompute(gray_rot, None)\n    \n    # Match features\n    if desc_orig is not None and desc_rot is not None:\n        bf = cv2.BFMatcher()\n        matches = bf.knnMatch(desc_orig, desc_rot, k=2)\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < 0.7 * n.distance:\n                    good_matches.append(m)\n        \n        # Visualize\n        matched_img = cv2.drawMatches(\n            test_img, kp_orig, rotated_img, kp_rot, good_matches, None,\n            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n        )\n        \n        plt.figure(figsize=(15, 8))\n        plt.imshow(matched_img)\n        plt.title(f\"Rotation Invariance: {len(good_matches)} matches between original and 45¬∞ rotated image\")\n        plt.axis('off')\n        plt.show()\n        \n        print(f\"üîÑ Rotation Invariance: Found {len(good_matches)} matching features despite 45¬∞ rotation!\")\n    else:\n        print(\"Could not detect enough features for matching\")\n\nshow_rotation_invariance()\n```\n\n::: {.cell-output .cell-output-display}\n![](06-feature-magic_files/figure-html/cell-10-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nüîÑ Rotation Invariance: Found 19 matching features despite 45¬∞ rotation!\n```\n:::\n:::\n\n\n## Key Takeaways\n\n- **Features are the \"fingerprints\"** of images - unique, repeatable points\n- **SIFT is more accurate** but slower; **ORB is faster** but less precise\n- **Feature matching** enables object recognition, panorama stitching, and 3D reconstruction\n- **Lowe's ratio test** filters out ambiguous matches for better accuracy\n- **Scale and rotation invariance** make features robust to transformations\n- **Real-world applications** include Google Photos, autonomous vehicles, and AR/VR\n\n## What's Coming Next?\n\nIn our next post, [**\"Why Deep Learning? When Classical Methods Hit the Wall\"**](07-why-deep-learning.qmd), we'll discover:\n\n- **The limitations** of classical computer vision methods\n- **Why neural networks** revolutionized image understanding\n- **Your first deep learning model** for image classification\n- **Transfer learning** - standing on the shoulders of giants\n\nYou've mastered the art of finding and matching features‚Äînext, we'll explore how deep learning took computer vision to the next level!\n\n:::{.callout-tip}\n## Hands-On Lab\nReady to extract and match features in your own images? Try the complete interactive notebook: [**Feature Magic Lab**](https://colab.research.google.com/github/hasanpasha/quarto_blog_hasan/blob/main/notebooks/cv-foundations-05-feature-magic.ipynb)\n\nBuild panoramas, recognize objects, and explore the magic of feature detection!\n:::\n\n:::{.callout-note}\n## Series Navigation\n- **Previous**: [Image Segmentation: Dividing and Conquering](05-image-segmentation.qmd)\n- **Next**: [Why Deep Learning? When Classical Methods Hit the Wall](07-why-deep-learning.qmd)\n- **Series Home**: [Computer Vision Foundations](../computer-vision-foundations.qmd)\n:::\n\n---\n\n*You've just learned one of the most powerful techniques in computer vision! Feature matching is used in everything from Google Photos to archaeological site reconstruction. Next, we'll see why deep learning became necessary and how it builds on these foundations.* \n\n",
    "supporting": [
      "06-feature-magic_files"
    ],
    "filters": [],
    "includes": {}
  }
}