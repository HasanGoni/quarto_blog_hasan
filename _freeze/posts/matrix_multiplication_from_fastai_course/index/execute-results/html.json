{
  "hash": "56bdd52546aa49b9ea8982642bdcd22d",
  "result": {
    "markdown": "---\ntitle: Understanding Matrix Multiplication from FastAI\nsubtitle: A Deep Dive into Neural Network Fundamentals\nauthor: Hasan Goni\ndate: '2023-11-20'\ncategories:\n  - deep-learning\n  - mathematics\n  - fastai\ntags:\n  - matrix-multiplication\n  - neural-networks\n  - pytorch\n  - numpy\n  - mathematics\nimage: matrix_mult.png\nformat:\n  html:\n    code-fold: false\n---\n\n::: {.callout-note}\n## Related Content\nThis post is part of our deep learning foundations series. You might also be interested in:\n- [Data Science Steps Series](/posts/series/data-science-steps)\n- [Feature Preprocessing](/posts/data-science-steps-to-follow-part02)\n:::\n\n# Matrix Multiplication: The Building Block of Deep Learning\n\n![Matrix Multiplication Visualization](matrix_mult.png)\n\nIn this post, we'll explore matrix multiplication from first principles, following Jeremy Howard's excellent teaching approach from the FastAI course. We'll understand why it's crucial for deep learning and implement it from scratch in Python.\n\n## Why Matrix Multiplication Matters\n\nMatrix multiplication is fundamental to deep learning because:\n\n1. It's the core operation in neural network layers\n2. It enables efficient parallel computation\n3. It allows us to represent complex transformations compactly\n\n## Implementation from Scratch\n\nLet's implement matrix multiplication using Python and NumPy:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport torch\nfrom typing import List, Tuple\nimport matplotlib.pyplot as plt\n\ndef matmul(a: List[List[float]], b: List[List[float]]) -> List[List[float]]:\n    \"\"\"Matrix multiplication from scratch\"\"\"\n    # Check dimensions\n    assert len(a[0]) == len(b), \"Incompatible dimensions\"\n    \n    # Initialize result matrix\n    result = [[0.0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    \n    # Perform multiplication\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example matrices\nA = [[1, 2], [3, 4]]\nB = [[5, 6], [7, 8]]\n\n# Calculate result\nresult = matmul(A, B)\nprint(\"Result of matrix multiplication:\")\nprint(np.array(result))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResult of matrix multiplication:\n[[19. 22.]\n [43. 50.]]\n```\n:::\n:::\n\n\n## Visualizing Matrix Multiplication\n\nLet's create a visual representation of how matrix multiplication works:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef plot_matrix_mult(A: np.ndarray, B: np.ndarray) -> None:\n    \"\"\"Visualize matrix multiplication process\"\"\"\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Plot first matrix\n    ax1.imshow(A, cmap='viridis')\n    ax1.set_title('Matrix A')\n    \n    # Plot second matrix\n    ax2.imshow(B, cmap='viridis')\n    ax2.set_title('Matrix B')\n    \n    # Plot result\n    result = np.dot(A, B)\n    ax3.imshow(result, cmap='viridis')\n    ax3.set_title('A Ã— B')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Create example matrices\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nplot_matrix_mult(A, B)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=1430 height=457}\n:::\n:::\n\n\n## PyTorch Implementation\n\nIn practice, we use optimized libraries like PyTorch:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Convert to PyTorch tensors\nA_torch = torch.tensor(A, dtype=torch.float32)\nB_torch = torch.tensor(B, dtype=torch.float32)\n\n# PyTorch matrix multiplication\nresult_torch = torch.matmul(A_torch, B_torch)\nprint(\"PyTorch result:\")\nprint(result_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPyTorch result:\ntensor([[19., 22.],\n        [43., 50.]])\n```\n:::\n:::\n\n\n## Performance Comparison\n\nLet's compare our implementation with NumPy and PyTorch:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport time\n\ndef benchmark_matmul(size: int = 100) -> None:\n    \"\"\"Compare performance of different implementations\"\"\"\n    # Generate random matrices\n    A = np.random.randn(size, size)\n    B = np.random.randn(size, size)\n    \n    # Custom implementation\n    start = time.time()\n    _ = matmul(A.tolist(), B.tolist())\n    custom_time = time.time() - start\n    \n    # NumPy\n    start = time.time()\n    _ = np.dot(A, B)\n    numpy_time = time.time() - start\n    \n    # PyTorch\n    A_torch = torch.tensor(A)\n    B_torch = torch.tensor(B)\n    start = time.time()\n    _ = torch.matmul(A_torch, B_torch)\n    torch_time = time.time() - start\n    \n    print(f\"Custom implementation: {custom_time:.4f}s\")\n    print(f\"NumPy: {numpy_time:.4f}s\")\n    print(f\"PyTorch: {torch_time:.4f}s\")\n\nbenchmark_matmul()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCustom implementation: 0.0618s\nNumPy: 0.0094s\nPyTorch: 0.0001s\n```\n:::\n:::\n\n\n## Key Takeaways\n\n1. Matrix multiplication is a fundamental operation in deep learning\n2. Understanding it from first principles helps debug neural networks\n3. Libraries like PyTorch provide highly optimized implementations\n4. The operation is inherently parallelizable\n\n::: {.callout-tip}\n## FastAI Insight\nJeremy Howard emphasizes understanding matrix multiplication from scratch because it's the foundation of neural network operations. This understanding helps in debugging and optimizing deep learning models.\n:::\n\n## Next Steps\n\nIn future posts, we'll explore:\n- How matrix multiplication enables neural network layers\n- Efficient implementations using CUDA\n- Common optimization techniques\n\n## Related Posts\n\n- [Data Science Steps Series](/posts/series/data-science-steps)\n- [Feature Preprocessing](/posts/data-science-steps-to-follow-part02)\n- [Using Nougat for Research Papers](/posts/nougat-to-read-scientific-pdf-files)\n\n## References\n\n1. [FastAI Course](https://course.fast.ai/)\n2. [Deep Learning Book - Linear Algebra Chapter](https://www.deeplearningbook.org/contents/linear_algebra.html)\n3. [PyTorch Documentation](https://pytorch.org/docs/stable/torch.html#torch.matmul) \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}