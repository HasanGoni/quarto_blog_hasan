<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hasan">
<meta name="dcterms.date" content="2025-01-22">

<title>Hasan’s Data Science &amp; AI Blog - Why Deep Learning? When Classical Methods Hit the Wall</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Hasan’s Data Science &amp; AI Blog - Why Deep Learning? When Classical Methods Hit the Wall">
<meta property="og:description" content="">
<meta property="og:image" content="https://images.unsplash.com/photo-1677442136019-21780ecad995?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2032&amp;q=80">
<meta property="og:site-name" content="Hasan's Data Science &amp; AI Blog">
<meta name="twitter:title" content="Hasan’s Data Science &amp; AI Blog - Why Deep Learning? When Classical Methods Hit the Wall">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://images.unsplash.com/photo-1677442136019-21780ecad995?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2032&amp;q=80">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Hasan’s Data Science &amp; AI Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-series" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Series</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-series">    
        <li>
    <a class="dropdown-item" href="../../../posts/series/data-science-steps.html" rel="" target="">
 <span class="dropdown-text">Data Science Steps</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/computer-vision-foundations.html" rel="" target="">
 <span class="dropdown-text">Computer Vision Foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/vlm.html" rel="" target="">
 <span class="dropdown-text">VLM Series</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/anomaly-detection/index.html" rel="" target="">
 <span class="dropdown-text">Anomaly Detection</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../categories.html" rel="" target="">
 <span class="menu-text">Categories</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../tags.html" rel="" target="">
 <span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/HasanGoni" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hasangoni" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/series/computer-vision-foundations.html">Computer Vision Foundations</a></li><li class="breadcrumb-item"><a href="../../../posts/series/cv-foundations/07-why-deep-learning.html">7. Why Deep Learning?</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Why Deep Learning? When Classical Methods Hit the Wall</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">computer-vision</div>
                <div class="quarto-category">deep-learning</div>
                <div class="quarto-category">neural-networks</div>
                <div class="quarto-category">pytorch</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Hasan </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 22, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">June 28, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Data Science</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/data-science-steps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part01/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 01</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part02/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 02</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part03/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 03</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part04/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 04</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part05/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 05</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part06/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 06</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Computer Vision Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/computer-vision-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computer Vision Foundations Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/01-why-computer-vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. Why Computer Vision?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/02-images-as-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Images as Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/03-opencv-essentials.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. OpenCV Essentials</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/04-finding-patterns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Finding Patterns</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/05-image-segmentation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Image Segmentation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/06-feature-magic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Feature Magic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/07-why-deep-learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">7. Why Deep Learning?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/08-modern-vision-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. Modern Vision Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/09-first-cv-project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. First CV Project</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/10-where-to-go-next.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. Where to Go Next</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Anomaly Detection</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/anomaly-detection/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/anomaly-detection/finding-the-oddballs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Finding the Oddballs</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">VLM Series</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/vlm-qwen3-14b/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Qwen3-14B</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-great-computer-vision-crisis-of-2010" id="toc-the-great-computer-vision-crisis-of-2010" class="nav-link active" data-scroll-target="#the-great-computer-vision-crisis-of-2010"><span class="header-section-number">0.1</span> The Great Computer Vision Crisis of 2010</a></li>
  <li><a href="#the-imagenet-moment-2012" id="toc-the-imagenet-moment-2012" class="nav-link" data-scroll-target="#the-imagenet-moment-2012"><span class="header-section-number">0.2</span> The ImageNet Moment: 2012</a></li>
  <li><a href="#what-makes-deep-learning-different" id="toc-what-makes-deep-learning-different" class="nav-link" data-scroll-target="#what-makes-deep-learning-different"><span class="header-section-number">0.3</span> What Makes Deep Learning Different?</a>
  <ul class="collapse">
  <li><a href="#classical-approach-hand-crafted-features" id="toc-classical-approach-hand-crafted-features" class="nav-link" data-scroll-target="#classical-approach-hand-crafted-features"><span class="header-section-number">0.3.1</span> Classical Approach: Hand-Crafted Features</a></li>
  <li><a href="#deep-learning-approach-learned-features" id="toc-deep-learning-approach-learned-features" class="nav-link" data-scroll-target="#deep-learning-approach-learned-features"><span class="header-section-number">0.3.2</span> Deep Learning Approach: Learned Features</a></li>
  </ul></li>
  <li><a href="#your-first-neural-network" id="toc-your-first-neural-network" class="nav-link" data-scroll-target="#your-first-neural-network"><span class="header-section-number">0.4</span> Your First Neural Network</a></li>
  <li><a href="#the-convolutional-revolution" id="toc-the-convolutional-revolution" class="nav-link" data-scroll-target="#the-convolutional-revolution"><span class="header-section-number">0.5</span> The Convolutional Revolution</a></li>
  <li><a href="#understanding-convolutions-the-sliding-window" id="toc-understanding-convolutions-the-sliding-window" class="nav-link" data-scroll-target="#understanding-convolutions-the-sliding-window"><span class="header-section-number">0.6</span> Understanding Convolutions: The Sliding Window</a></li>
  <li><a href="#transfer-learning-standing-on-giants-shoulders" id="toc-transfer-learning-standing-on-giants-shoulders" class="nav-link" data-scroll-target="#transfer-learning-standing-on-giants-shoulders"><span class="header-section-number">0.7</span> Transfer Learning: Standing on Giants’ Shoulders</a></li>
  <li><a href="#your-first-image-classifier" id="toc-your-first-image-classifier" class="nav-link" data-scroll-target="#your-first-image-classifier"><span class="header-section-number">0.8</span> Your First Image Classifier</a></li>
  <li><a href="#visualizing-what-neural-networks-learn" id="toc-visualizing-what-neural-networks-learn" class="nav-link" data-scroll-target="#visualizing-what-neural-networks-learn"><span class="header-section-number">0.9</span> Visualizing What Neural Networks Learn</a></li>
  <li><a href="#feature-maps-seeing-through-the-networks-eyes" id="toc-feature-maps-seeing-through-the-networks-eyes" class="nav-link" data-scroll-target="#feature-maps-seeing-through-the-networks-eyes"><span class="header-section-number">0.10</span> Feature Maps: Seeing Through the Network’s Eyes</a></li>
  <li><a href="#the-deep-learning-advantage-why-it-works" id="toc-the-deep-learning-advantage-why-it-works" class="nav-link" data-scroll-target="#the-deep-learning-advantage-why-it-works"><span class="header-section-number">0.11</span> The Deep Learning Advantage: Why It Works</a></li>
  <li><a href="#why-deep-learning-won" id="toc-why-deep-learning-won" class="nav-link" data-scroll-target="#why-deep-learning-won"><span class="header-section-number">0.12</span> Why Deep Learning Won</a>
  <ul class="collapse">
  <li><a href="#automatic-feature-learning" id="toc-automatic-feature-learning" class="nav-link" data-scroll-target="#automatic-feature-learning"><span class="header-section-number">0.12.1</span> 1. <strong>Automatic Feature Learning</strong></a></li>
  <li><a href="#hierarchical-representations" id="toc-hierarchical-representations" class="nav-link" data-scroll-target="#hierarchical-representations"><span class="header-section-number">0.12.2</span> 2. <strong>Hierarchical Representations</strong></a></li>
  <li><a href="#end-to-end-learning" id="toc-end-to-end-learning" class="nav-link" data-scroll-target="#end-to-end-learning"><span class="header-section-number">0.12.3</span> 3. <strong>End-to-End Learning</strong></a></li>
  <li><a href="#scalability" id="toc-scalability" class="nav-link" data-scroll-target="#scalability"><span class="header-section-number">0.12.4</span> 4. <strong>Scalability</strong></a></li>
  </ul></li>
  <li><a href="#the-modern-deep-learning-pipeline" id="toc-the-modern-deep-learning-pipeline" class="nav-link" data-scroll-target="#the-modern-deep-learning-pipeline"><span class="header-section-number">0.13</span> The Modern Deep Learning Pipeline</a></li>
  <li><a href="#whats-coming-next" id="toc-whats-coming-next" class="nav-link" data-scroll-target="#whats-coming-next"><span class="header-section-number">0.14</span> What’s Coming Next?</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">0.15</span> Key Takeaways</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="the-great-computer-vision-crisis-of-2010" class="level2" data-number="0.1">
<h2 data-number="0.1" class="anchored" data-anchor-id="the-great-computer-vision-crisis-of-2010"><span class="header-section-number">0.1</span> The Great Computer Vision Crisis of 2010</h2>
<p>Picture this: It’s 2010, and computer vision researchers are frustrated. They’ve spent decades perfecting edge detection, feature matching, and object recognition. Their algorithms can find corners, match keypoints, and even stitch panoramas.</p>
<p>But there’s one problem they can’t solve: <strong>telling cats from dogs.</strong></p>
<p>Seriously! While a 3-year-old child could easily distinguish between cats and dogs, the best computer vision systems struggled with this “simple” task. The classical approach required hand-crafting features for every possible variation—different breeds, lighting conditions, poses, backgrounds. It was like trying to write rules for every possible way a cat could look. Impossible!</p>
<p>Then something revolutionary happened…</p>
</section>
<section id="the-imagenet-moment-2012" class="level2" data-number="0.2">
<h2 data-number="0.2" class="anchored" data-anchor-id="the-imagenet-moment-2012"><span class="header-section-number">0.2</span> The ImageNet Moment: 2012</h2>
<p>In 2012, a team led by Alex Krizhevsky entered the ImageNet competition with something called <strong>AlexNet</strong>—a deep neural network. The results were shocking:</p>
<ul>
<li><strong>Previous best accuracy</strong>: 74.3%</li>
<li><strong>AlexNet accuracy</strong>: 84.7%</li>
<li><strong>Improvement</strong>: A massive 10+ percentage point jump!</li>
</ul>
<p>This wasn’t just an incremental improvement—it was a paradigm shift. Deep learning had arrived, and computer vision would never be the same.</p>
</section>
<section id="what-makes-deep-learning-different" class="level2" data-number="0.3">
<h2 data-number="0.3" class="anchored" data-anchor-id="what-makes-deep-learning-different"><span class="header-section-number">0.3</span> What Makes Deep Learning Different?</h2>
<p>Let’s understand why neural networks succeeded where classical methods struggled:</p>
<section id="classical-approach-hand-crafted-features" class="level3" data-number="0.3.1">
<h3 data-number="0.3.1" class="anchored" data-anchor-id="classical-approach-hand-crafted-features"><span class="header-section-number">0.3.1</span> Classical Approach: Hand-Crafted Features</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Classical computer vision pipeline</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw">def</span> classical_cat_detector(image):</span>
<span id="cb1-3"><a href="#cb1-3"></a>    <span class="co"># Step 1: Extract hand-crafted features</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>    edges <span class="op">=</span> detect_edges(image)</span>
<span id="cb1-5"><a href="#cb1-5"></a>    corners <span class="op">=</span> detect_corners(image)</span>
<span id="cb1-6"><a href="#cb1-6"></a>    textures <span class="op">=</span> analyze_textures(image)</span>
<span id="cb1-7"><a href="#cb1-7"></a>    </span>
<span id="cb1-8"><a href="#cb1-8"></a>    <span class="co"># Step 2: Combine features with rules</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>    <span class="cf">if</span> (pointy_ears <span class="kw">and</span> whiskers <span class="kw">and</span> fur_texture):</span>
<span id="cb1-10"><a href="#cb1-10"></a>        <span class="cf">return</span> <span class="st">"cat"</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>    <span class="cf">else</span>:</span>
<span id="cb1-12"><a href="#cb1-12"></a>        <span class="cf">return</span> <span class="st">"not_cat"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Problems:</strong> - Features had to be manually designed - Rules were brittle and specific - Couldn’t adapt to new variations - Required domain expertise</p>
</section>
<section id="deep-learning-approach-learned-features" class="level3" data-number="0.3.2">
<h3 data-number="0.3.2" class="anchored" data-anchor-id="deep-learning-approach-learned-features"><span class="header-section-number">0.3.2</span> Deep Learning Approach: Learned Features</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Deep learning pipeline</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="kw">def</span> deep_cat_detector(image):</span>
<span id="cb2-3"><a href="#cb2-3"></a>    <span class="co"># The network learns its own features!</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>    features <span class="op">=</span> neural_network.extract_features(image)</span>
<span id="cb2-5"><a href="#cb2-5"></a>    prediction <span class="op">=</span> neural_network.classify(features)</span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="cf">return</span> prediction</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Advantages:</strong> - Features are learned automatically - Adapts to data variations - Improves with more data - Works across different domains</p>
</section>
</section>
<section id="your-first-neural-network" class="level2" data-number="0.4">
<h2 data-number="0.4" class="anchored" data-anchor-id="your-first-neural-network"><span class="header-section-number">0.4</span> Your First Neural Network</h2>
<p>Let’s build a simple neural network to understand the magic:</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="im">import</span> torchvision</span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-8"><a href="#cb3-8"></a></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co"># Simple neural network for image classification</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="kw">class</span> SimpleNet(nn.Module):</span>
<span id="cb3-11"><a href="#cb3-11"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">2</span>):  <span class="co"># 2 classes: cat vs dog</span></span>
<span id="cb3-12"><a href="#cb3-12"></a>        <span class="bu">super</span>(SimpleNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb3-13"><a href="#cb3-13"></a>        </span>
<span id="cb3-14"><a href="#cb3-14"></a>        <span class="co"># Flatten 224x224x3 image to 150,528 features</span></span>
<span id="cb3-15"><a href="#cb3-15"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb3-16"><a href="#cb3-16"></a>        </span>
<span id="cb3-17"><a href="#cb3-17"></a>        <span class="co"># Simple fully connected layers</span></span>
<span id="cb3-18"><a href="#cb3-18"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">224</span> <span class="op">*</span> <span class="dv">224</span> <span class="op">*</span> <span class="dv">3</span>, <span class="dv">512</span>)</span>
<span id="cb3-19"><a href="#cb3-19"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> nn.ReLU()</span>
<span id="cb3-20"><a href="#cb3-20"></a>        <span class="va">self</span>.dropout1 <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb3-21"><a href="#cb3-21"></a>        </span>
<span id="cb3-22"><a href="#cb3-22"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">512</span>, <span class="dv">128</span>)</span>
<span id="cb3-23"><a href="#cb3-23"></a>        <span class="va">self</span>.relu2 <span class="op">=</span> nn.ReLU()</span>
<span id="cb3-24"><a href="#cb3-24"></a>        <span class="va">self</span>.dropout2 <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb3-25"><a href="#cb3-25"></a>        </span>
<span id="cb3-26"><a href="#cb3-26"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, num_classes)</span>
<span id="cb3-27"><a href="#cb3-27"></a>    </span>
<span id="cb3-28"><a href="#cb3-28"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-29"><a href="#cb3-29"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb3-30"><a href="#cb3-30"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout1(<span class="va">self</span>.relu1(<span class="va">self</span>.fc1(x)))</span>
<span id="cb3-31"><a href="#cb3-31"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout2(<span class="va">self</span>.relu2(<span class="va">self</span>.fc2(x)))</span>
<span id="cb3-32"><a href="#cb3-32"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb3-33"><a href="#cb3-33"></a>        <span class="cf">return</span> x</span>
<span id="cb3-34"><a href="#cb3-34"></a></span>
<span id="cb3-35"><a href="#cb3-35"></a><span class="co"># Create the network</span></span>
<span id="cb3-36"><a href="#cb3-36"></a>simple_net <span class="op">=</span> SimpleNet(num_classes<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-37"><a href="#cb3-37"></a><span class="bu">print</span>(<span class="st">"Simple Neural Network:"</span>)</span>
<span id="cb3-38"><a href="#cb3-38"></a><span class="bu">print</span>(simple_net)</span>
<span id="cb3-39"><a href="#cb3-39"></a></span>
<span id="cb3-40"><a href="#cb3-40"></a><span class="co"># Count parameters</span></span>
<span id="cb3-41"><a href="#cb3-41"></a>total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> simple_net.parameters())</span>
<span id="cb3-42"><a href="#cb3-42"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Total parameters: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Simple Neural Network:
SimpleNet(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=150528, out_features=512, bias=True)
  (relu1): ReLU()
  (dropout1): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=512, out_features=128, bias=True)
  (relu2): ReLU()
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc3): Linear(in_features=128, out_features=2, bias=True)
)

Total parameters: 77,136,770</code></pre>
</div>
</div>
<p><strong>🎯 Try it yourself!</strong> <a href="https://colab.research.google.com/github/hasanpasha/quarto_blog_hasan/blob/main/notebooks/cv-foundations-06-why-deep-learning.ipynb">Open in Colab</a></p>
</section>
<section id="the-convolutional-revolution" class="level2" data-number="0.5">
<h2 data-number="0.5" class="anchored" data-anchor-id="the-convolutional-revolution"><span class="header-section-number">0.5</span> The Convolutional Revolution</h2>
<p>But wait—there’s a problem with our simple network. It treats each pixel independently, ignoring spatial relationships. That’s like reading a book by looking at each letter separately!</p>
<p>Enter <strong>Convolutional Neural Networks (CNNs)</strong>—networks designed specifically for images:</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">class</span> ConvNet(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb5-3"><a href="#cb5-3"></a>        <span class="bu">super</span>(ConvNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4"></a>        </span>
<span id="cb5-5"><a href="#cb5-5"></a>        <span class="co"># Convolutional layers (feature extractors)</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-7"><a href="#cb5-7"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-8"><a href="#cb5-8"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-9"><a href="#cb5-9"></a>        </span>
<span id="cb5-10"><a href="#cb5-10"></a>        <span class="co"># Pooling layers (downsampling)</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb5-12"><a href="#cb5-12"></a>        </span>
<span id="cb5-13"><a href="#cb5-13"></a>        <span class="co"># Fully connected layers (classifier)</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">128</span> <span class="op">*</span> <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">512</span>)  <span class="co"># 224/8 = 28 after 3 pooling layers</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">512</span>, num_classes)</span>
<span id="cb5-16"><a href="#cb5-16"></a>        </span>
<span id="cb5-17"><a href="#cb5-17"></a>        <span class="co"># Activation and regularization</span></span>
<span id="cb5-18"><a href="#cb5-18"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb5-19"><a href="#cb5-19"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb5-20"><a href="#cb5-20"></a>    </span>
<span id="cb5-21"><a href="#cb5-21"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-22"><a href="#cb5-22"></a>        <span class="co"># Feature extraction</span></span>
<span id="cb5-23"><a href="#cb5-23"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(<span class="va">self</span>.relu(<span class="va">self</span>.conv1(x)))  <span class="co"># 224x224 -&gt; 112x112</span></span>
<span id="cb5-24"><a href="#cb5-24"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(<span class="va">self</span>.relu(<span class="va">self</span>.conv2(x)))  <span class="co"># 112x112 -&gt; 56x56</span></span>
<span id="cb5-25"><a href="#cb5-25"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(<span class="va">self</span>.relu(<span class="va">self</span>.conv3(x)))  <span class="co"># 56x56 -&gt; 28x28</span></span>
<span id="cb5-26"><a href="#cb5-26"></a>        </span>
<span id="cb5-27"><a href="#cb5-27"></a>        <span class="co"># Flatten and classify</span></span>
<span id="cb5-28"><a href="#cb5-28"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)  <span class="co"># Flatten</span></span>
<span id="cb5-29"><a href="#cb5-29"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.relu(<span class="va">self</span>.fc1(x)))</span>
<span id="cb5-30"><a href="#cb5-30"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb5-31"><a href="#cb5-31"></a>        </span>
<span id="cb5-32"><a href="#cb5-32"></a>        <span class="cf">return</span> x</span>
<span id="cb5-33"><a href="#cb5-33"></a></span>
<span id="cb5-34"><a href="#cb5-34"></a><span class="co"># Create CNN</span></span>
<span id="cb5-35"><a href="#cb5-35"></a>cnn <span class="op">=</span> ConvNet(num_classes<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-36"><a href="#cb5-36"></a><span class="bu">print</span>(<span class="st">"Convolutional Neural Network:"</span>)</span>
<span id="cb5-37"><a href="#cb5-37"></a><span class="bu">print</span>(cnn)</span>
<span id="cb5-38"><a href="#cb5-38"></a></span>
<span id="cb5-39"><a href="#cb5-39"></a><span class="co"># Count parameters</span></span>
<span id="cb5-40"><a href="#cb5-40"></a>cnn_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> cnn.parameters())</span>
<span id="cb5-41"><a href="#cb5-41"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">CNN parameters: </span><span class="sc">{</span>cnn_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb5-42"><a href="#cb5-42"></a><span class="bu">print</span>(<span class="ss">f"Simple net parameters: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb5-43"><a href="#cb5-43"></a><span class="bu">print</span>(<span class="ss">f"CNN has </span><span class="sc">{</span>(total_params <span class="op">-</span> cnn_params) <span class="op">/</span> total_params <span class="op">*</span> <span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">% fewer parameters!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Convolutional Neural Network:
ConvNet(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc1): Linear(in_features=100352, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=2, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
)

CNN parameters: 51,475,010
Simple net parameters: 77,136,770
CNN has 33.3% fewer parameters!</code></pre>
</div>
</div>
</section>
<section id="understanding-convolutions-the-sliding-window" class="level2" data-number="0.6">
<h2 data-number="0.6" class="anchored" data-anchor-id="understanding-convolutions-the-sliding-window"><span class="header-section-number">0.6</span> Understanding Convolutions: The Sliding Window</h2>
<p>Let’s visualize what convolutions actually do:</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">def</span> visualize_convolution():</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="co">"""Show how convolution works"""</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>    <span class="co"># Create a simple image</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>    image <span class="op">=</span> np.array([</span>
<span id="cb7-5"><a href="#cb7-5"></a>        [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb7-6"><a href="#cb7-6"></a>        [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb7-7"><a href="#cb7-7"></a>        [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb7-8"><a href="#cb7-8"></a>        [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb7-9"><a href="#cb7-9"></a>        [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb7-10"><a href="#cb7-10"></a>    ])</span>
<span id="cb7-11"><a href="#cb7-11"></a>    </span>
<span id="cb7-12"><a href="#cb7-12"></a>    <span class="co"># Edge detection kernel</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>    kernel <span class="op">=</span> np.array([</span>
<span id="cb7-14"><a href="#cb7-14"></a>        [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb7-15"><a href="#cb7-15"></a>        [<span class="op">-</span><span class="dv">1</span>,  <span class="dv">8</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb7-16"><a href="#cb7-16"></a>        [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb7-17"><a href="#cb7-17"></a>    ])</span>
<span id="cb7-18"><a href="#cb7-18"></a>    </span>
<span id="cb7-19"><a href="#cb7-19"></a>    <span class="co"># Apply convolution manually</span></span>
<span id="cb7-20"><a href="#cb7-20"></a>    result <span class="op">=</span> np.zeros((<span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb7-21"><a href="#cb7-21"></a>    </span>
<span id="cb7-22"><a href="#cb7-22"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb7-23"><a href="#cb7-23"></a>    </span>
<span id="cb7-24"><a href="#cb7-24"></a>    <span class="co"># Show original image</span></span>
<span id="cb7-25"><a href="#cb7-25"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>)</span>
<span id="cb7-26"><a href="#cb7-26"></a>    plt.imshow(image, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb7-27"><a href="#cb7-27"></a>    plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb7-28"><a href="#cb7-28"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb7-29"><a href="#cb7-29"></a>    </span>
<span id="cb7-30"><a href="#cb7-30"></a>    <span class="co"># Show kernel</span></span>
<span id="cb7-31"><a href="#cb7-31"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">2</span>)</span>
<span id="cb7-32"><a href="#cb7-32"></a>    plt.imshow(kernel, cmap<span class="op">=</span><span class="st">'RdBu'</span>)</span>
<span id="cb7-33"><a href="#cb7-33"></a>    plt.title(<span class="st">"Edge Detection Kernel"</span>)</span>
<span id="cb7-34"><a href="#cb7-34"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb7-35"><a href="#cb7-35"></a>    </span>
<span id="cb7-36"><a href="#cb7-36"></a>    <span class="co"># Show convolution process</span></span>
<span id="cb7-37"><a href="#cb7-37"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb7-38"><a href="#cb7-38"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb7-39"><a href="#cb7-39"></a>            patch <span class="op">=</span> image[i:i<span class="op">+</span><span class="dv">3</span>, j:j<span class="op">+</span><span class="dv">3</span>]</span>
<span id="cb7-40"><a href="#cb7-40"></a>            result[i, j] <span class="op">=</span> np.<span class="bu">sum</span>(patch <span class="op">*</span> kernel)</span>
<span id="cb7-41"><a href="#cb7-41"></a>    </span>
<span id="cb7-42"><a href="#cb7-42"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb7-43"><a href="#cb7-43"></a>    plt.imshow(result, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb7-44"><a href="#cb7-44"></a>    plt.title(<span class="st">"Convolution Result"</span>)</span>
<span id="cb7-45"><a href="#cb7-45"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb7-46"><a href="#cb7-46"></a>    </span>
<span id="cb7-47"><a href="#cb7-47"></a>    <span class="co"># Show using PyTorch</span></span>
<span id="cb7-48"><a href="#cb7-48"></a>    image_tensor <span class="op">=</span> torch.FloatTensor(image).unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb7-49"><a href="#cb7-49"></a>    kernel_tensor <span class="op">=</span> torch.FloatTensor(kernel).unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb7-50"><a href="#cb7-50"></a>    </span>
<span id="cb7-51"><a href="#cb7-51"></a>    conv_layer <span class="op">=</span> nn.Conv2d(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-52"><a href="#cb7-52"></a>    conv_layer.weight.data <span class="op">=</span> kernel_tensor</span>
<span id="cb7-53"><a href="#cb7-53"></a>    </span>
<span id="cb7-54"><a href="#cb7-54"></a>    pytorch_result <span class="op">=</span> conv_layer(image_tensor).squeeze().detach().numpy()</span>
<span id="cb7-55"><a href="#cb7-55"></a>    </span>
<span id="cb7-56"><a href="#cb7-56"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb7-57"><a href="#cb7-57"></a>    plt.imshow(pytorch_result, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb7-58"><a href="#cb7-58"></a>    plt.title(<span class="st">"PyTorch Result"</span>)</span>
<span id="cb7-59"><a href="#cb7-59"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb7-60"><a href="#cb7-60"></a>    </span>
<span id="cb7-61"><a href="#cb7-61"></a>    plt.tight_layout()</span>
<span id="cb7-62"><a href="#cb7-62"></a>    plt.show()</span>
<span id="cb7-63"><a href="#cb7-63"></a></span>
<span id="cb7-64"><a href="#cb7-64"></a>visualize_convolution()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="07-why-deep-learning_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="transfer-learning-standing-on-giants-shoulders" class="level2" data-number="0.7">
<h2 data-number="0.7" class="anchored" data-anchor-id="transfer-learning-standing-on-giants-shoulders"><span class="header-section-number">0.7</span> Transfer Learning: Standing on Giants’ Shoulders</h2>
<p>Here’s the secret that makes deep learning practical: <strong>Transfer Learning</strong>. Instead of training from scratch, we use pre-trained models and adapt them to our needs:</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb8-2"><a href="#cb8-2"></a></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co"># Load a pre-trained ResNet model</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>pretrained_model <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="bu">print</span>(<span class="st">"Pre-trained ResNet-18:"</span>)</span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="bu">print</span>(pretrained_model)</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="co"># Modify for our task (cat vs dog classification)</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>num_classes <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>pretrained_model.fc <span class="op">=</span> nn.Linear(pretrained_model.fc.in_features, num_classes)</span>
<span id="cb8-11"><a href="#cb8-11"></a></span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Modified final layer for </span><span class="sc">{</span>num_classes<span class="sc">}</span><span class="ss"> classes"</span>)</span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="bu">print</span>(<span class="ss">f"Final layer: </span><span class="sc">{</span>pretrained_model<span class="sc">.</span>fc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-14"><a href="#cb8-14"></a></span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="co"># Freeze early layers (optional)</span></span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="cf">for</span> param <span class="kw">in</span> pretrained_model.parameters():</span>
<span id="cb8-17"><a href="#cb8-17"></a>    param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb8-18"><a href="#cb8-18"></a></span>
<span id="cb8-19"><a href="#cb8-19"></a><span class="co"># Only train the final layer</span></span>
<span id="cb8-20"><a href="#cb8-20"></a><span class="cf">for</span> param <span class="kw">in</span> pretrained_model.fc.parameters():</span>
<span id="cb8-21"><a href="#cb8-21"></a>    param.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb8-22"><a href="#cb8-22"></a></span>
<span id="cb8-23"><a href="#cb8-23"></a>trainable_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> pretrained_model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb8-24"><a href="#cb8-24"></a>total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> pretrained_model.parameters())</span>
<span id="cb8-25"><a href="#cb8-25"></a></span>
<span id="cb8-26"><a href="#cb8-26"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Trainable parameters: </span><span class="sc">{</span>trainable_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb8-27"><a href="#cb8-27"></a><span class="bu">print</span>(<span class="ss">f"Total parameters: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb8-28"><a href="#cb8-28"></a><span class="bu">print</span>(<span class="ss">f"Training only </span><span class="sc">{</span>trainable_params<span class="op">/</span>total_params<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">% of parameters!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Pre-trained ResNet-18:
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)

Modified final layer for 2 classes
Final layer: Linear(in_features=512, out_features=2, bias=True)

Trainable parameters: 1,026
Total parameters: 11,177,538
Training only 0.0% of parameters!</code></pre>
</div>
</div>
</section>
<section id="your-first-image-classifier" class="level2" data-number="0.8">
<h2 data-number="0.8" class="anchored" data-anchor-id="your-first-image-classifier"><span class="header-section-number">0.8</span> Your First Image Classifier</h2>
<p>Let’s build a complete image classification system:</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">class</span> ImageClassifier:</span>
<span id="cb10-2"><a href="#cb10-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model_name<span class="op">=</span><span class="st">'resnet18'</span>, num_classes<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb10-3"><a href="#cb10-3"></a>        <span class="va">self</span>.device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb10-4"><a href="#cb10-4"></a>        <span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-5"><a href="#cb10-5"></a>        </span>
<span id="cb10-6"><a href="#cb10-6"></a>        <span class="co"># Load pre-trained model</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>        <span class="cf">if</span> model_name <span class="op">==</span> <span class="st">'resnet18'</span>:</span>
<span id="cb10-8"><a href="#cb10-8"></a>            <span class="va">self</span>.model <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-9"><a href="#cb10-9"></a>            <span class="va">self</span>.model.fc <span class="op">=</span> nn.Linear(<span class="va">self</span>.model.fc.in_features, num_classes)</span>
<span id="cb10-10"><a href="#cb10-10"></a>        </span>
<span id="cb10-11"><a href="#cb10-11"></a>        <span class="va">self</span>.model <span class="op">=</span> <span class="va">self</span>.model.to(<span class="va">self</span>.device)</span>
<span id="cb10-12"><a href="#cb10-12"></a>        </span>
<span id="cb10-13"><a href="#cb10-13"></a>        <span class="co"># Define transforms</span></span>
<span id="cb10-14"><a href="#cb10-14"></a>        <span class="va">self</span>.transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb10-15"><a href="#cb10-15"></a>            transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb10-16"><a href="#cb10-16"></a>            transforms.ToTensor(),</span>
<span id="cb10-17"><a href="#cb10-17"></a>            transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], </span>
<span id="cb10-18"><a href="#cb10-18"></a>                               std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb10-19"><a href="#cb10-19"></a>        ])</span>
<span id="cb10-20"><a href="#cb10-20"></a>        </span>
<span id="cb10-21"><a href="#cb10-21"></a>        <span class="va">self</span>.classes <span class="op">=</span> [<span class="st">'cat'</span>, <span class="st">'dog'</span>]  <span class="co"># Update based on your classes</span></span>
<span id="cb10-22"><a href="#cb10-22"></a>    </span>
<span id="cb10-23"><a href="#cb10-23"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, image):</span>
<span id="cb10-24"><a href="#cb10-24"></a>        <span class="co">"""Predict class of a single image"""</span></span>
<span id="cb10-25"><a href="#cb10-25"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb10-26"><a href="#cb10-26"></a>        </span>
<span id="cb10-27"><a href="#cb10-27"></a>        <span class="co"># Preprocess image</span></span>
<span id="cb10-28"><a href="#cb10-28"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(image, np.ndarray):</span>
<span id="cb10-29"><a href="#cb10-29"></a>            <span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb10-30"><a href="#cb10-30"></a>            image <span class="op">=</span> Image.fromarray(image)</span>
<span id="cb10-31"><a href="#cb10-31"></a>        </span>
<span id="cb10-32"><a href="#cb10-32"></a>        input_tensor <span class="op">=</span> <span class="va">self</span>.transform(image).unsqueeze(<span class="dv">0</span>).to(<span class="va">self</span>.device)</span>
<span id="cb10-33"><a href="#cb10-33"></a>        </span>
<span id="cb10-34"><a href="#cb10-34"></a>        <span class="co"># Make prediction</span></span>
<span id="cb10-35"><a href="#cb10-35"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-36"><a href="#cb10-36"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(input_tensor)</span>
<span id="cb10-37"><a href="#cb10-37"></a>            probabilities <span class="op">=</span> torch.softmax(outputs, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-38"><a href="#cb10-38"></a>            predicted_class <span class="op">=</span> torch.argmax(probabilities, dim<span class="op">=</span><span class="dv">1</span>).item()</span>
<span id="cb10-39"><a href="#cb10-39"></a>            confidence <span class="op">=</span> probabilities[<span class="dv">0</span>][predicted_class].item()</span>
<span id="cb10-40"><a href="#cb10-40"></a>        </span>
<span id="cb10-41"><a href="#cb10-41"></a>        <span class="cf">return</span> {</span>
<span id="cb10-42"><a href="#cb10-42"></a>            <span class="st">'class'</span>: <span class="va">self</span>.classes[predicted_class],</span>
<span id="cb10-43"><a href="#cb10-43"></a>            <span class="st">'confidence'</span>: confidence,</span>
<span id="cb10-44"><a href="#cb10-44"></a>            <span class="st">'probabilities'</span>: probabilities.cpu().numpy()[<span class="dv">0</span>]</span>
<span id="cb10-45"><a href="#cb10-45"></a>        }</span>
<span id="cb10-46"><a href="#cb10-46"></a>    </span>
<span id="cb10-47"><a href="#cb10-47"></a>    <span class="kw">def</span> predict_batch(<span class="va">self</span>, images):</span>
<span id="cb10-48"><a href="#cb10-48"></a>        <span class="co">"""Predict classes for multiple images"""</span></span>
<span id="cb10-49"><a href="#cb10-49"></a>        results <span class="op">=</span> []</span>
<span id="cb10-50"><a href="#cb10-50"></a>        <span class="cf">for</span> image <span class="kw">in</span> images:</span>
<span id="cb10-51"><a href="#cb10-51"></a>            result <span class="op">=</span> <span class="va">self</span>.predict(image)</span>
<span id="cb10-52"><a href="#cb10-52"></a>            results.append(result)</span>
<span id="cb10-53"><a href="#cb10-53"></a>        <span class="cf">return</span> results</span>
<span id="cb10-54"><a href="#cb10-54"></a></span>
<span id="cb10-55"><a href="#cb10-55"></a><span class="co"># Create classifier</span></span>
<span id="cb10-56"><a href="#cb10-56"></a>classifier <span class="op">=</span> ImageClassifier()</span>
<span id="cb10-57"><a href="#cb10-57"></a></span>
<span id="cb10-58"><a href="#cb10-58"></a><span class="co"># Test with a sample image (you would load your own)</span></span>
<span id="cb10-59"><a href="#cb10-59"></a><span class="kw">def</span> test_classifier(image_path):</span>
<span id="cb10-60"><a href="#cb10-60"></a>    <span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb10-61"><a href="#cb10-61"></a>    </span>
<span id="cb10-62"><a href="#cb10-62"></a>    <span class="co"># Load image</span></span>
<span id="cb10-63"><a href="#cb10-63"></a>    image <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb10-64"><a href="#cb10-64"></a>    </span>
<span id="cb10-65"><a href="#cb10-65"></a>    <span class="co"># Make prediction</span></span>
<span id="cb10-66"><a href="#cb10-66"></a>    result <span class="op">=</span> classifier.predict(image)</span>
<span id="cb10-67"><a href="#cb10-67"></a>    </span>
<span id="cb10-68"><a href="#cb10-68"></a>    <span class="co"># Display result</span></span>
<span id="cb10-69"><a href="#cb10-69"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb10-70"><a href="#cb10-70"></a>    </span>
<span id="cb10-71"><a href="#cb10-71"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb10-72"><a href="#cb10-72"></a>    plt.imshow(image)</span>
<span id="cb10-73"><a href="#cb10-73"></a>    plt.title(<span class="ss">f"Input Image"</span>)</span>
<span id="cb10-74"><a href="#cb10-74"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb10-75"><a href="#cb10-75"></a>    </span>
<span id="cb10-76"><a href="#cb10-76"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb10-77"><a href="#cb10-77"></a>    plt.bar(classifier.classes, result[<span class="st">'probabilities'</span>])</span>
<span id="cb10-78"><a href="#cb10-78"></a>    plt.title(<span class="ss">f"Prediction: </span><span class="sc">{</span>result[<span class="st">'class'</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>result[<span class="st">'confidence'</span>]<span class="sc">:.2f}</span><span class="ss">)"</span>)</span>
<span id="cb10-79"><a href="#cb10-79"></a>    plt.ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb10-80"><a href="#cb10-80"></a>    </span>
<span id="cb10-81"><a href="#cb10-81"></a>    plt.tight_layout()</span>
<span id="cb10-82"><a href="#cb10-82"></a>    plt.show()</span>
<span id="cb10-83"><a href="#cb10-83"></a>    </span>
<span id="cb10-84"><a href="#cb10-84"></a>    <span class="cf">return</span> result</span>
<span id="cb10-85"><a href="#cb10-85"></a></span>
<span id="cb10-86"><a href="#cb10-86"></a><span class="co"># Test the classifier</span></span>
<span id="cb10-87"><a href="#cb10-87"></a><span class="co"># result = test_classifier('your_image.jpg')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Using device: cuda</code></pre>
</div>
</div>
</section>
<section id="visualizing-what-neural-networks-learn" class="level2" data-number="0.9">
<h2 data-number="0.9" class="anchored" data-anchor-id="visualizing-what-neural-networks-learn"><span class="header-section-number">0.9</span> Visualizing What Neural Networks Learn</h2>
<p>One of the coolest things about deep learning is visualizing what the network actually learns:</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">def</span> visualize_filters(model, layer_name<span class="op">=</span><span class="st">'conv1'</span>):</span>
<span id="cb12-2"><a href="#cb12-2"></a>    <span class="co">"""Visualize the filters learned by a convolutional layer"""</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>    </span>
<span id="cb12-4"><a href="#cb12-4"></a>    <span class="co"># Get the layer</span></span>
<span id="cb12-5"><a href="#cb12-5"></a>    layer <span class="op">=</span> <span class="bu">dict</span>(model.named_modules())[layer_name]</span>
<span id="cb12-6"><a href="#cb12-6"></a>    </span>
<span id="cb12-7"><a href="#cb12-7"></a>    <span class="co"># Get the weights</span></span>
<span id="cb12-8"><a href="#cb12-8"></a>    weights <span class="op">=</span> layer.weight.data.cpu()</span>
<span id="cb12-9"><a href="#cb12-9"></a>    </span>
<span id="cb12-10"><a href="#cb12-10"></a>    <span class="co"># Normalize weights for visualization</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>    weights <span class="op">=</span> (weights <span class="op">-</span> weights.<span class="bu">min</span>()) <span class="op">/</span> (weights.<span class="bu">max</span>() <span class="op">-</span> weights.<span class="bu">min</span>())</span>
<span id="cb12-12"><a href="#cb12-12"></a>    </span>
<span id="cb12-13"><a href="#cb12-13"></a>    <span class="co"># Plot filters</span></span>
<span id="cb12-14"><a href="#cb12-14"></a>    num_filters <span class="op">=</span> <span class="bu">min</span>(<span class="dv">16</span>, weights.shape[<span class="dv">0</span>])  <span class="co"># Show first 16 filters</span></span>
<span id="cb12-15"><a href="#cb12-15"></a>    </span>
<span id="cb12-16"><a href="#cb12-16"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb12-17"><a href="#cb12-17"></a>    </span>
<span id="cb12-18"><a href="#cb12-18"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_filters):</span>
<span id="cb12-19"><a href="#cb12-19"></a>        plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb12-20"><a href="#cb12-20"></a>        </span>
<span id="cb12-21"><a href="#cb12-21"></a>        <span class="cf">if</span> weights.shape[<span class="dv">1</span>] <span class="op">==</span> <span class="dv">3</span>:  <span class="co"># RGB filters</span></span>
<span id="cb12-22"><a href="#cb12-22"></a>            <span class="co"># Transpose from (C, H, W) to (H, W, C)</span></span>
<span id="cb12-23"><a href="#cb12-23"></a>            filter_img <span class="op">=</span> weights[i].permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb12-24"><a href="#cb12-24"></a>            plt.imshow(filter_img)</span>
<span id="cb12-25"><a href="#cb12-25"></a>        <span class="cf">else</span>:  <span class="co"># Grayscale filters</span></span>
<span id="cb12-26"><a href="#cb12-26"></a>            plt.imshow(weights[i, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb12-27"><a href="#cb12-27"></a>        </span>
<span id="cb12-28"><a href="#cb12-28"></a>        plt.title(<span class="ss">f"Filter </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-29"><a href="#cb12-29"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb12-30"><a href="#cb12-30"></a>    </span>
<span id="cb12-31"><a href="#cb12-31"></a>    plt.suptitle(<span class="ss">f"Learned Filters in </span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-32"><a href="#cb12-32"></a>    plt.tight_layout()</span>
<span id="cb12-33"><a href="#cb12-33"></a>    plt.show()</span>
<span id="cb12-34"><a href="#cb12-34"></a></span>
<span id="cb12-35"><a href="#cb12-35"></a><span class="co"># Visualize filters from our pre-trained model</span></span>
<span id="cb12-36"><a href="#cb12-36"></a>visualize_filters(pretrained_model, <span class="st">'conv1'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="07-why-deep-learning_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="feature-maps-seeing-through-the-networks-eyes" class="level2" data-number="0.10">
<h2 data-number="0.10" class="anchored" data-anchor-id="feature-maps-seeing-through-the-networks-eyes"><span class="header-section-number">0.10</span> Feature Maps: Seeing Through the Network’s Eyes</h2>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">def</span> visualize_feature_maps(model, image, layer_name<span class="op">=</span><span class="st">'layer1'</span>):</span>
<span id="cb13-2"><a href="#cb13-2"></a>    <span class="co">"""Visualize feature maps from a specific layer"""</span></span>
<span id="cb13-3"><a href="#cb13-3"></a>    </span>
<span id="cb13-4"><a href="#cb13-4"></a>    <span class="co"># Hook to capture feature maps</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>    feature_maps <span class="op">=</span> []</span>
<span id="cb13-6"><a href="#cb13-6"></a>    </span>
<span id="cb13-7"><a href="#cb13-7"></a>    <span class="kw">def</span> hook_fn(module, <span class="bu">input</span>, output):</span>
<span id="cb13-8"><a href="#cb13-8"></a>        feature_maps.append(output.cpu())</span>
<span id="cb13-9"><a href="#cb13-9"></a>    </span>
<span id="cb13-10"><a href="#cb13-10"></a>    <span class="co"># Register hook</span></span>
<span id="cb13-11"><a href="#cb13-11"></a>    layer <span class="op">=</span> <span class="bu">dict</span>(model.named_modules())[layer_name]</span>
<span id="cb13-12"><a href="#cb13-12"></a>    hook <span class="op">=</span> layer.register_forward_hook(hook_fn)</span>
<span id="cb13-13"><a href="#cb13-13"></a>    </span>
<span id="cb13-14"><a href="#cb13-14"></a>    <span class="co"># Forward pass</span></span>
<span id="cb13-15"><a href="#cb13-15"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb13-16"><a href="#cb13-16"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb13-17"><a href="#cb13-17"></a>        _ <span class="op">=</span> model(image.unsqueeze(<span class="dv">0</span>))</span>
<span id="cb13-18"><a href="#cb13-18"></a>    </span>
<span id="cb13-19"><a href="#cb13-19"></a>    <span class="co"># Remove hook</span></span>
<span id="cb13-20"><a href="#cb13-20"></a>    hook.remove()</span>
<span id="cb13-21"><a href="#cb13-21"></a>    </span>
<span id="cb13-22"><a href="#cb13-22"></a>    <span class="co"># Get feature maps</span></span>
<span id="cb13-23"><a href="#cb13-23"></a>    fmaps <span class="op">=</span> feature_maps[<span class="dv">0</span>].squeeze(<span class="dv">0</span>)  <span class="co"># Remove batch dimension</span></span>
<span id="cb13-24"><a href="#cb13-24"></a>    </span>
<span id="cb13-25"><a href="#cb13-25"></a>    <span class="co"># Plot feature maps</span></span>
<span id="cb13-26"><a href="#cb13-26"></a>    num_maps <span class="op">=</span> <span class="bu">min</span>(<span class="dv">16</span>, fmaps.shape[<span class="dv">0</span>])</span>
<span id="cb13-27"><a href="#cb13-27"></a>    </span>
<span id="cb13-28"><a href="#cb13-28"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb13-29"><a href="#cb13-29"></a>    </span>
<span id="cb13-30"><a href="#cb13-30"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_maps):</span>
<span id="cb13-31"><a href="#cb13-31"></a>        plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-32"><a href="#cb13-32"></a>        plt.imshow(fmaps[i], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb13-33"><a href="#cb13-33"></a>        plt.title(<span class="ss">f"Feature Map </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-34"><a href="#cb13-34"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb13-35"><a href="#cb13-35"></a>    </span>
<span id="cb13-36"><a href="#cb13-36"></a>    plt.suptitle(<span class="ss">f"Feature Maps from </span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-37"><a href="#cb13-37"></a>    plt.tight_layout()</span>
<span id="cb13-38"><a href="#cb13-38"></a>    plt.show()</span>
<span id="cb13-39"><a href="#cb13-39"></a></span>
<span id="cb13-40"><a href="#cb13-40"></a><span class="co"># Create a sample input</span></span>
<span id="cb13-41"><a href="#cb13-41"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb13-42"><a href="#cb13-42"></a>visualize_feature_maps(pretrained_model, sample_input)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="07-why-deep-learning_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="the-deep-learning-advantage-why-it-works" class="level2" data-number="0.11">
<h2 data-number="0.11" class="anchored" data-anchor-id="the-deep-learning-advantage-why-it-works"><span class="header-section-number">0.11</span> The Deep Learning Advantage: Why It Works</h2>
<p>Let’s compare classical vs deep learning approaches on a real problem:</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="kw">def</span> compare_approaches(image):</span>
<span id="cb14-2"><a href="#cb14-2"></a>    <span class="co">"""Compare classical and deep learning approaches"""</span></span>
<span id="cb14-3"><a href="#cb14-3"></a>    </span>
<span id="cb14-4"><a href="#cb14-4"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb14-5"><a href="#cb14-5"></a>    </span>
<span id="cb14-6"><a href="#cb14-6"></a>    <span class="co"># Original image</span></span>
<span id="cb14-7"><a href="#cb14-7"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb14-8"><a href="#cb14-8"></a>    plt.imshow(image)</span>
<span id="cb14-9"><a href="#cb14-9"></a>    plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb14-10"><a href="#cb14-10"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb14-11"><a href="#cb14-11"></a>    </span>
<span id="cb14-12"><a href="#cb14-12"></a>    <span class="co"># Classical approach: hand-crafted features</span></span>
<span id="cb14-13"><a href="#cb14-13"></a>    gray <span class="op">=</span> cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)</span>
<span id="cb14-14"><a href="#cb14-14"></a>    </span>
<span id="cb14-15"><a href="#cb14-15"></a>    <span class="co"># Edge features</span></span>
<span id="cb14-16"><a href="#cb14-16"></a>    edges <span class="op">=</span> cv2.Canny(gray, <span class="dv">50</span>, <span class="dv">150</span>)</span>
<span id="cb14-17"><a href="#cb14-17"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb14-18"><a href="#cb14-18"></a>    plt.imshow(edges, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb14-19"><a href="#cb14-19"></a>    plt.title(<span class="st">"Classical: Edge Features"</span>)</span>
<span id="cb14-20"><a href="#cb14-20"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb14-21"><a href="#cb14-21"></a>    </span>
<span id="cb14-22"><a href="#cb14-22"></a>    <span class="co"># Texture features (using LBP-like approach)</span></span>
<span id="cb14-23"><a href="#cb14-23"></a>    <span class="im">from</span> skimage.feature <span class="im">import</span> local_binary_pattern</span>
<span id="cb14-24"><a href="#cb14-24"></a>    lbp <span class="op">=</span> local_binary_pattern(gray, <span class="dv">24</span>, <span class="dv">8</span>, method<span class="op">=</span><span class="st">'uniform'</span>)</span>
<span id="cb14-25"><a href="#cb14-25"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb14-26"><a href="#cb14-26"></a>    plt.imshow(lbp, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb14-27"><a href="#cb14-27"></a>    plt.title(<span class="st">"Classical: Texture Features"</span>)</span>
<span id="cb14-28"><a href="#cb14-28"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb14-29"><a href="#cb14-29"></a>    </span>
<span id="cb14-30"><a href="#cb14-30"></a>    <span class="co"># Deep learning approach: learned features</span></span>
<span id="cb14-31"><a href="#cb14-31"></a>    <span class="co"># Transform image for the model</span></span>
<span id="cb14-32"><a href="#cb14-32"></a>    transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb14-33"><a href="#cb14-33"></a>        transforms.ToPILImage(),</span>
<span id="cb14-34"><a href="#cb14-34"></a>        transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb14-35"><a href="#cb14-35"></a>        transforms.ToTensor(),</span>
<span id="cb14-36"><a href="#cb14-36"></a>        transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], </span>
<span id="cb14-37"><a href="#cb14-37"></a>                           std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb14-38"><a href="#cb14-38"></a>    ])</span>
<span id="cb14-39"><a href="#cb14-39"></a>    </span>
<span id="cb14-40"><a href="#cb14-40"></a>    input_tensor <span class="op">=</span> transform(image).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb14-41"><a href="#cb14-41"></a>    </span>
<span id="cb14-42"><a href="#cb14-42"></a>    <span class="co"># Get feature maps from different layers</span></span>
<span id="cb14-43"><a href="#cb14-43"></a>    feature_maps <span class="op">=</span> []</span>
<span id="cb14-44"><a href="#cb14-44"></a>    </span>
<span id="cb14-45"><a href="#cb14-45"></a>    <span class="kw">def</span> get_features(name):</span>
<span id="cb14-46"><a href="#cb14-46"></a>        <span class="kw">def</span> hook(model, <span class="bu">input</span>, output):</span>
<span id="cb14-47"><a href="#cb14-47"></a>            feature_maps.append((name, output.cpu()))</span>
<span id="cb14-48"><a href="#cb14-48"></a>        <span class="cf">return</span> hook</span>
<span id="cb14-49"><a href="#cb14-49"></a>    </span>
<span id="cb14-50"><a href="#cb14-50"></a>    <span class="co"># Register hooks</span></span>
<span id="cb14-51"><a href="#cb14-51"></a>    pretrained_model.layer1.register_forward_hook(get_features(<span class="st">'Low-level'</span>))</span>
<span id="cb14-52"><a href="#cb14-52"></a>    pretrained_model.layer3.register_forward_hook(get_features(<span class="st">'Mid-level'</span>))</span>
<span id="cb14-53"><a href="#cb14-53"></a>    pretrained_model.layer4.register_forward_hook(get_features(<span class="st">'High-level'</span>))</span>
<span id="cb14-54"><a href="#cb14-54"></a>    </span>
<span id="cb14-55"><a href="#cb14-55"></a>    <span class="co"># Forward pass</span></span>
<span id="cb14-56"><a href="#cb14-56"></a>    pretrained_model.<span class="bu">eval</span>()</span>
<span id="cb14-57"><a href="#cb14-57"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb14-58"><a href="#cb14-58"></a>        _ <span class="op">=</span> pretrained_model(input_tensor)</span>
<span id="cb14-59"><a href="#cb14-59"></a>    </span>
<span id="cb14-60"><a href="#cb14-60"></a>    <span class="co"># Visualize learned features</span></span>
<span id="cb14-61"><a href="#cb14-61"></a>    <span class="cf">for</span> i, (name, fmaps) <span class="kw">in</span> <span class="bu">enumerate</span>(feature_maps):</span>
<span id="cb14-62"><a href="#cb14-62"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span> <span class="op">+</span> i)</span>
<span id="cb14-63"><a href="#cb14-63"></a>        <span class="co"># Average across channels for visualization</span></span>
<span id="cb14-64"><a href="#cb14-64"></a>        avg_fmap <span class="op">=</span> torch.mean(fmaps.squeeze(<span class="dv">0</span>), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-65"><a href="#cb14-65"></a>        plt.imshow(avg_fmap, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb14-66"><a href="#cb14-66"></a>        plt.title(<span class="ss">f"Deep Learning: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> Features"</span>)</span>
<span id="cb14-67"><a href="#cb14-67"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb14-68"><a href="#cb14-68"></a>    </span>
<span id="cb14-69"><a href="#cb14-69"></a>    plt.tight_layout()</span>
<span id="cb14-70"><a href="#cb14-70"></a>    plt.show()</span>
<span id="cb14-71"><a href="#cb14-71"></a></span>
<span id="cb14-72"><a href="#cb14-72"></a><span class="co"># Compare approaches (you would use your own image)</span></span>
<span id="cb14-73"><a href="#cb14-73"></a><span class="co"># compare_approaches(your_image)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="why-deep-learning-won" class="level2" data-number="0.12">
<h2 data-number="0.12" class="anchored" data-anchor-id="why-deep-learning-won"><span class="header-section-number">0.12</span> Why Deep Learning Won</h2>
<p>Here’s why deep learning revolutionized computer vision:</p>
<section id="automatic-feature-learning" class="level3" data-number="0.12.1">
<h3 data-number="0.12.1" class="anchored" data-anchor-id="automatic-feature-learning"><span class="header-section-number">0.12.1</span> 1. <strong>Automatic Feature Learning</strong></h3>
<ul>
<li>No need to hand-craft features</li>
<li>Learns optimal representations for the task</li>
<li>Adapts to data variations</li>
</ul>
</section>
<section id="hierarchical-representations" class="level3" data-number="0.12.2">
<h3 data-number="0.12.2" class="anchored" data-anchor-id="hierarchical-representations"><span class="header-section-number">0.12.2</span> 2. <strong>Hierarchical Representations</strong></h3>
<ul>
<li>Low-level features (edges, textures)</li>
<li>Mid-level features (parts, patterns)<br>
</li>
<li>High-level features (objects, concepts)</li>
</ul>
</section>
<section id="end-to-end-learning" class="level3" data-number="0.12.3">
<h3 data-number="0.12.3" class="anchored" data-anchor-id="end-to-end-learning"><span class="header-section-number">0.12.3</span> 3. <strong>End-to-End Learning</strong></h3>
<ul>
<li>Optimizes entire pipeline together</li>
<li>Features and classifier learned jointly</li>
<li>Better overall performance</li>
</ul>
</section>
<section id="scalability" class="level3" data-number="0.12.4">
<h3 data-number="0.12.4" class="anchored" data-anchor-id="scalability"><span class="header-section-number">0.12.4</span> 4. <strong>Scalability</strong></h3>
<ul>
<li>Performance improves with more data</li>
<li>Can handle complex, real-world variations</li>
<li>Generalizes across domains</li>
</ul>
</section>
</section>
<section id="the-modern-deep-learning-pipeline" class="level2" data-number="0.13">
<h2 data-number="0.13" class="anchored" data-anchor-id="the-modern-deep-learning-pipeline"><span class="header-section-number">0.13</span> The Modern Deep Learning Pipeline</h2>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="kw">class</span> ModernVisionPipeline:</span>
<span id="cb15-2"><a href="#cb15-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb15-3"><a href="#cb15-3"></a>        <span class="va">self</span>.device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb15-4"><a href="#cb15-4"></a>        </span>
<span id="cb15-5"><a href="#cb15-5"></a>        <span class="co"># 1. Data preprocessing</span></span>
<span id="cb15-6"><a href="#cb15-6"></a>        <span class="va">self</span>.transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb15-7"><a href="#cb15-7"></a>            transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb15-8"><a href="#cb15-8"></a>            transforms.RandomHorizontalFlip(p<span class="op">=</span><span class="fl">0.5</span>),  <span class="co"># Data augmentation</span></span>
<span id="cb15-9"><a href="#cb15-9"></a>            transforms.RandomRotation(<span class="dv">10</span>),</span>
<span id="cb15-10"><a href="#cb15-10"></a>            transforms.ColorJitter(brightness<span class="op">=</span><span class="fl">0.2</span>, contrast<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb15-11"><a href="#cb15-11"></a>            transforms.ToTensor(),</span>
<span id="cb15-12"><a href="#cb15-12"></a>            transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], </span>
<span id="cb15-13"><a href="#cb15-13"></a>                               std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb15-14"><a href="#cb15-14"></a>        ])</span>
<span id="cb15-15"><a href="#cb15-15"></a>        </span>
<span id="cb15-16"><a href="#cb15-16"></a>        <span class="co"># 2. Model architecture</span></span>
<span id="cb15-17"><a href="#cb15-17"></a>        <span class="va">self</span>.model <span class="op">=</span> models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-18"><a href="#cb15-18"></a>        </span>
<span id="cb15-19"><a href="#cb15-19"></a>        <span class="co"># 3. Transfer learning setup</span></span>
<span id="cb15-20"><a href="#cb15-20"></a>        <span class="co"># Freeze early layers</span></span>
<span id="cb15-21"><a href="#cb15-21"></a>        <span class="cf">for</span> param <span class="kw">in</span> <span class="bu">list</span>(<span class="va">self</span>.model.parameters())[:<span class="op">-</span><span class="dv">10</span>]:</span>
<span id="cb15-22"><a href="#cb15-22"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb15-23"><a href="#cb15-23"></a>        </span>
<span id="cb15-24"><a href="#cb15-24"></a>        <span class="co"># 4. Optimizer and loss</span></span>
<span id="cb15-25"><a href="#cb15-25"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> optim.Adam(</span>
<span id="cb15-26"><a href="#cb15-26"></a>            <span class="bu">filter</span>(<span class="kw">lambda</span> p: p.requires_grad, <span class="va">self</span>.model.parameters()),</span>
<span id="cb15-27"><a href="#cb15-27"></a>            lr<span class="op">=</span><span class="fl">0.001</span></span>
<span id="cb15-28"><a href="#cb15-28"></a>        )</span>
<span id="cb15-29"><a href="#cb15-29"></a>        <span class="va">self</span>.criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb15-30"><a href="#cb15-30"></a>    </span>
<span id="cb15-31"><a href="#cb15-31"></a>    <span class="kw">def</span> train_step(<span class="va">self</span>, images, labels):</span>
<span id="cb15-32"><a href="#cb15-32"></a>        <span class="co">"""Single training step"""</span></span>
<span id="cb15-33"><a href="#cb15-33"></a>        <span class="va">self</span>.model.train()</span>
<span id="cb15-34"><a href="#cb15-34"></a>        </span>
<span id="cb15-35"><a href="#cb15-35"></a>        <span class="co"># Forward pass</span></span>
<span id="cb15-36"><a href="#cb15-36"></a>        outputs <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb15-37"><a href="#cb15-37"></a>        loss <span class="op">=</span> <span class="va">self</span>.criterion(outputs, labels)</span>
<span id="cb15-38"><a href="#cb15-38"></a>        </span>
<span id="cb15-39"><a href="#cb15-39"></a>        <span class="co"># Backward pass</span></span>
<span id="cb15-40"><a href="#cb15-40"></a>        <span class="va">self</span>.optimizer.zero_grad()</span>
<span id="cb15-41"><a href="#cb15-41"></a>        loss.backward()</span>
<span id="cb15-42"><a href="#cb15-42"></a>        <span class="va">self</span>.optimizer.step()</span>
<span id="cb15-43"><a href="#cb15-43"></a>        </span>
<span id="cb15-44"><a href="#cb15-44"></a>        <span class="cf">return</span> loss.item()</span>
<span id="cb15-45"><a href="#cb15-45"></a>    </span>
<span id="cb15-46"><a href="#cb15-46"></a>    <span class="kw">def</span> evaluate(<span class="va">self</span>, images, labels):</span>
<span id="cb15-47"><a href="#cb15-47"></a>        <span class="co">"""Evaluation step"""</span></span>
<span id="cb15-48"><a href="#cb15-48"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb15-49"><a href="#cb15-49"></a>        </span>
<span id="cb15-50"><a href="#cb15-50"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb15-51"><a href="#cb15-51"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb15-52"><a href="#cb15-52"></a>            _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs, <span class="dv">1</span>)</span>
<span id="cb15-53"><a href="#cb15-53"></a>            accuracy <span class="op">=</span> (predicted <span class="op">==</span> labels).<span class="bu">float</span>().mean()</span>
<span id="cb15-54"><a href="#cb15-54"></a>        </span>
<span id="cb15-55"><a href="#cb15-55"></a>        <span class="cf">return</span> accuracy.item()</span>
<span id="cb15-56"><a href="#cb15-56"></a></span>
<span id="cb15-57"><a href="#cb15-57"></a><span class="co"># Create modern pipeline</span></span>
<span id="cb15-58"><a href="#cb15-58"></a>pipeline <span class="op">=</span> ModernVisionPipeline()</span>
<span id="cb15-59"><a href="#cb15-59"></a><span class="bu">print</span>(<span class="st">"Modern deep learning pipeline ready!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Modern deep learning pipeline ready!</code></pre>
</div>
</div>
</section>
<section id="whats-coming-next" class="level2" data-number="0.14">
<h2 data-number="0.14" class="anchored" data-anchor-id="whats-coming-next"><span class="header-section-number">0.14</span> What’s Coming Next?</h2>
<p>In our next post, <a href="../07-modern-vision-models/"><strong>“Modern Vision Models: CNNs, Vision Transformers, and DINOv2”</strong></a>, we’ll explore:</p>
<ul>
<li><strong>State-of-the-art architectures</strong> (ResNet, EfficientNet, Vision Transformers)</li>
<li><strong>Foundation models</strong> like DINOv2</li>
<li><strong>Self-supervised learning</strong> (learning without labels)</li>
<li><strong>Building your own DINOv2 feature extractor</strong></li>
</ul>
<p>You’ve just learned why deep learning revolutionized computer vision—next, we’ll explore the cutting-edge models that are shaping the future!</p>
</section>
<section id="key-takeaways" class="level2" data-number="0.15">
<h2 data-number="0.15" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">0.15</span> Key Takeaways</h2>
<ul>
<li><strong>Classical methods hit a wall</strong> with complex real-world variations</li>
<li><strong>Deep learning learns features automatically</strong> instead of hand-crafting them</li>
<li><strong>CNNs are designed for images</strong> with spatial understanding</li>
<li><strong>Transfer learning</strong> makes deep learning practical for everyone</li>
<li><strong>Hierarchical features</strong> enable understanding at multiple levels</li>
<li><strong>Modern pipelines</strong> combine data augmentation, pre-training, and fine-tuning</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hands-On Lab
</div>
</div>
<div class="callout-body-container callout-body">
<p>Ready to build your first deep learning classifier? Try the complete interactive notebook: <a href="https://colab.research.google.com/drive/1Deep_Learning_Basics_123456"><strong>Deep Learning Basics Lab</strong></a></p>
<p>Train your own cat vs dog classifier and see the power of neural networks!</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Series Navigation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Previous</strong>: <a href="../../../posts/series/cv-foundations/06-feature-magic.html">Feature Magic: What Makes Images Unique</a></li>
<li><strong>Next</strong>: <a href="../../../posts/series/cv-foundations/08-modern-vision-models.html">Modern Vision Models: CNNs, Vision Transformers, and DINOv2</a></li>
<li><strong>Series Home</strong>: <a href="../../../posts/series/computer-vision-foundations.html">Computer Vision Foundations</a></li>
</ul>
</div>
</div>
<hr>
<p><em>You’ve just witnessed the deep learning revolution! From struggling with cat vs dog classification to achieving superhuman performance on complex tasks—this is why deep learning changed everything. Next, we’ll explore the latest and greatest models that are pushing the boundaries even further.</em></p>


<!-- -->

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div>CC BY-NC-SA 4.0</div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{2025,
  author = {, Hasan},
  title = {Why {Deep} {Learning?} {When} {Classical} {Methods} {Hit} the
    {Wall}},
  date = {2025-01-22},
  url = {https://hasangoni.quarto.pub/hasan-blog-post/posts/series/cv-foundations/07-why-deep-learning.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Hasan. 2025. <span>“Why Deep Learning? When Classical Methods Hit the
Wall.”</span> January 22, 2025. <a href="https://hasangoni.quarto.pub/hasan-blog-post/posts/series/cv-foundations/07-why-deep-learning.html">https://hasangoni.quarto.pub/hasan-blog-post/posts/series/cv-foundations/07-why-deep-learning.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="yourusername/quarto_blog_hasan" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<script src="https://giscus.app/client.js" data-repo="HasanGoni/quarto_blog_hasan" data-repo-id="" data-category="General" data-category-id="" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb17" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1"></a><span class="co">---</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="an">title:</span><span class="co"> "Why Deep Learning? When Classical Methods Hit the Wall"</span></span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="an">author:</span><span class="co"> "Hasan"</span></span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="an">date:</span><span class="co"> 2025-01-22</span></span>
<span id="cb17-5"><a href="#cb17-5"></a><span class="an">categories:</span><span class="co"> [computer-vision, deep-learning, neural-networks, pytorch]</span></span>
<span id="cb17-6"><a href="#cb17-6"></a><span class="an">tags:</span><span class="co"> [cnn, transfer-learning, pytorch, classification]</span></span>
<span id="cb17-7"><a href="#cb17-7"></a><span class="an">image:</span><span class="co"> "https://images.unsplash.com/photo-1677442136019-21780ecad995?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2032&amp;q=80"</span></span>
<span id="cb17-8"><a href="#cb17-8"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb17-9"><a href="#cb17-9"></a><span class="an">series:</span></span>
<span id="cb17-10"><a href="#cb17-10"></a><span class="co">  name: "Computer Vision Foundations"</span></span>
<span id="cb17-11"><a href="#cb17-11"></a><span class="co">  number: 7</span></span>
<span id="cb17-12"><a href="#cb17-12"></a><span class="an">format:</span></span>
<span id="cb17-13"><a href="#cb17-13"></a><span class="co">  html: default</span></span>
<span id="cb17-14"><a href="#cb17-14"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb17-15"><a href="#cb17-15"></a><span class="co">---</span></span>
<span id="cb17-16"><a href="#cb17-16"></a></span>
<span id="cb17-17"><a href="#cb17-17"></a><span class="fu">## The Great Computer Vision Crisis of 2010</span></span>
<span id="cb17-18"><a href="#cb17-18"></a></span>
<span id="cb17-19"><a href="#cb17-19"></a>Picture this: It's 2010, and computer vision researchers are frustrated. They've spent decades perfecting edge detection, feature matching, and object recognition. Their algorithms can find corners, match keypoints, and even stitch panoramas.</span>
<span id="cb17-20"><a href="#cb17-20"></a></span>
<span id="cb17-21"><a href="#cb17-21"></a>But there's one problem they can't solve: **telling cats from dogs.**</span>
<span id="cb17-22"><a href="#cb17-22"></a></span>
<span id="cb17-23"><a href="#cb17-23"></a>Seriously! While a 3-year-old child could easily distinguish between cats and dogs, the best computer vision systems struggled with this "simple" task. The classical approach required hand-crafting features for every possible variation—different breeds, lighting conditions, poses, backgrounds. It was like trying to write rules for every possible way a cat could look. Impossible!</span>
<span id="cb17-24"><a href="#cb17-24"></a></span>
<span id="cb17-25"><a href="#cb17-25"></a>Then something revolutionary happened...</span>
<span id="cb17-26"><a href="#cb17-26"></a></span>
<span id="cb17-27"><a href="#cb17-27"></a><span class="fu">## The ImageNet Moment: 2012</span></span>
<span id="cb17-28"><a href="#cb17-28"></a></span>
<span id="cb17-29"><a href="#cb17-29"></a>In 2012, a team led by Alex Krizhevsky entered the ImageNet competition with something called **AlexNet**—a deep neural network. The results were shocking:</span>
<span id="cb17-30"><a href="#cb17-30"></a></span>
<span id="cb17-31"><a href="#cb17-31"></a><span class="ss">- </span>**Previous best accuracy**: 74.3%</span>
<span id="cb17-32"><a href="#cb17-32"></a><span class="ss">- </span>**AlexNet accuracy**: 84.7%</span>
<span id="cb17-33"><a href="#cb17-33"></a><span class="ss">- </span>**Improvement**: A massive 10+ percentage point jump!</span>
<span id="cb17-34"><a href="#cb17-34"></a></span>
<span id="cb17-35"><a href="#cb17-35"></a>This wasn't just an incremental improvement—it was a paradigm shift. Deep learning had arrived, and computer vision would never be the same.</span>
<span id="cb17-36"><a href="#cb17-36"></a></span>
<span id="cb17-37"><a href="#cb17-37"></a><span class="fu">## What Makes Deep Learning Different?</span></span>
<span id="cb17-38"><a href="#cb17-38"></a></span>
<span id="cb17-39"><a href="#cb17-39"></a>Let's understand why neural networks succeeded where classical methods struggled:</span>
<span id="cb17-40"><a href="#cb17-40"></a></span>
<span id="cb17-41"><a href="#cb17-41"></a><span class="fu">### Classical Approach: Hand-Crafted Features</span></span>
<span id="cb17-42"><a href="#cb17-42"></a><span class="in">```python</span></span>
<span id="cb17-43"><a href="#cb17-43"></a><span class="co"># Classical computer vision pipeline</span></span>
<span id="cb17-44"><a href="#cb17-44"></a><span class="kw">def</span> classical_cat_detector(image):</span>
<span id="cb17-45"><a href="#cb17-45"></a>    <span class="co"># Step 1: Extract hand-crafted features</span></span>
<span id="cb17-46"><a href="#cb17-46"></a>    edges <span class="op">=</span> detect_edges(image)</span>
<span id="cb17-47"><a href="#cb17-47"></a>    corners <span class="op">=</span> detect_corners(image)</span>
<span id="cb17-48"><a href="#cb17-48"></a>    textures <span class="op">=</span> analyze_textures(image)</span>
<span id="cb17-49"><a href="#cb17-49"></a>    </span>
<span id="cb17-50"><a href="#cb17-50"></a>    <span class="co"># Step 2: Combine features with rules</span></span>
<span id="cb17-51"><a href="#cb17-51"></a>    <span class="cf">if</span> (pointy_ears <span class="kw">and</span> whiskers <span class="kw">and</span> fur_texture):</span>
<span id="cb17-52"><a href="#cb17-52"></a>        <span class="cf">return</span> <span class="st">"cat"</span></span>
<span id="cb17-53"><a href="#cb17-53"></a>    <span class="cf">else</span>:</span>
<span id="cb17-54"><a href="#cb17-54"></a>        <span class="cf">return</span> <span class="st">"not_cat"</span></span>
<span id="cb17-55"><a href="#cb17-55"></a><span class="in">```</span></span>
<span id="cb17-56"><a href="#cb17-56"></a></span>
<span id="cb17-57"><a href="#cb17-57"></a>**Problems:**</span>
<span id="cb17-58"><a href="#cb17-58"></a><span class="ss">- </span>Features had to be manually designed</span>
<span id="cb17-59"><a href="#cb17-59"></a><span class="ss">- </span>Rules were brittle and specific</span>
<span id="cb17-60"><a href="#cb17-60"></a><span class="ss">- </span>Couldn't adapt to new variations</span>
<span id="cb17-61"><a href="#cb17-61"></a><span class="ss">- </span>Required domain expertise</span>
<span id="cb17-62"><a href="#cb17-62"></a></span>
<span id="cb17-63"><a href="#cb17-63"></a><span class="fu">### Deep Learning Approach: Learned Features</span></span>
<span id="cb17-64"><a href="#cb17-64"></a><span class="in">```python</span></span>
<span id="cb17-65"><a href="#cb17-65"></a><span class="co"># Deep learning pipeline</span></span>
<span id="cb17-66"><a href="#cb17-66"></a><span class="kw">def</span> deep_cat_detector(image):</span>
<span id="cb17-67"><a href="#cb17-67"></a>    <span class="co"># The network learns its own features!</span></span>
<span id="cb17-68"><a href="#cb17-68"></a>    features <span class="op">=</span> neural_network.extract_features(image)</span>
<span id="cb17-69"><a href="#cb17-69"></a>    prediction <span class="op">=</span> neural_network.classify(features)</span>
<span id="cb17-70"><a href="#cb17-70"></a>    <span class="cf">return</span> prediction</span>
<span id="cb17-71"><a href="#cb17-71"></a><span class="in">```</span></span>
<span id="cb17-72"><a href="#cb17-72"></a></span>
<span id="cb17-73"><a href="#cb17-73"></a>**Advantages:**</span>
<span id="cb17-74"><a href="#cb17-74"></a><span class="ss">- </span>Features are learned automatically</span>
<span id="cb17-75"><a href="#cb17-75"></a><span class="ss">- </span>Adapts to data variations</span>
<span id="cb17-76"><a href="#cb17-76"></a><span class="ss">- </span>Improves with more data</span>
<span id="cb17-77"><a href="#cb17-77"></a><span class="ss">- </span>Works across different domains</span>
<span id="cb17-78"><a href="#cb17-78"></a></span>
<span id="cb17-79"><a href="#cb17-79"></a><span class="fu">## Your First Neural Network</span></span>
<span id="cb17-80"><a href="#cb17-80"></a></span>
<span id="cb17-81"><a href="#cb17-81"></a>Let's build a simple neural network to understand the magic:</span>
<span id="cb17-82"><a href="#cb17-82"></a></span>
<span id="cb17-85"><a href="#cb17-85"></a><span class="in">```{python}</span></span>
<span id="cb17-86"><a href="#cb17-86"></a><span class="co">#| eval: true</span></span>
<span id="cb17-87"><a href="#cb17-87"></a><span class="im">import</span> torch</span>
<span id="cb17-88"><a href="#cb17-88"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb17-89"><a href="#cb17-89"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb17-90"><a href="#cb17-90"></a><span class="im">import</span> torchvision</span>
<span id="cb17-91"><a href="#cb17-91"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb17-92"><a href="#cb17-92"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-93"><a href="#cb17-93"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-94"><a href="#cb17-94"></a></span>
<span id="cb17-95"><a href="#cb17-95"></a><span class="co"># Simple neural network for image classification</span></span>
<span id="cb17-96"><a href="#cb17-96"></a><span class="kw">class</span> SimpleNet(nn.Module):</span>
<span id="cb17-97"><a href="#cb17-97"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">2</span>):  <span class="co"># 2 classes: cat vs dog</span></span>
<span id="cb17-98"><a href="#cb17-98"></a>        <span class="bu">super</span>(SimpleNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb17-99"><a href="#cb17-99"></a>        </span>
<span id="cb17-100"><a href="#cb17-100"></a>        <span class="co"># Flatten 224x224x3 image to 150,528 features</span></span>
<span id="cb17-101"><a href="#cb17-101"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb17-102"><a href="#cb17-102"></a>        </span>
<span id="cb17-103"><a href="#cb17-103"></a>        <span class="co"># Simple fully connected layers</span></span>
<span id="cb17-104"><a href="#cb17-104"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">224</span> <span class="op">*</span> <span class="dv">224</span> <span class="op">*</span> <span class="dv">3</span>, <span class="dv">512</span>)</span>
<span id="cb17-105"><a href="#cb17-105"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> nn.ReLU()</span>
<span id="cb17-106"><a href="#cb17-106"></a>        <span class="va">self</span>.dropout1 <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb17-107"><a href="#cb17-107"></a>        </span>
<span id="cb17-108"><a href="#cb17-108"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">512</span>, <span class="dv">128</span>)</span>
<span id="cb17-109"><a href="#cb17-109"></a>        <span class="va">self</span>.relu2 <span class="op">=</span> nn.ReLU()</span>
<span id="cb17-110"><a href="#cb17-110"></a>        <span class="va">self</span>.dropout2 <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb17-111"><a href="#cb17-111"></a>        </span>
<span id="cb17-112"><a href="#cb17-112"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, num_classes)</span>
<span id="cb17-113"><a href="#cb17-113"></a>    </span>
<span id="cb17-114"><a href="#cb17-114"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb17-115"><a href="#cb17-115"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb17-116"><a href="#cb17-116"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout1(<span class="va">self</span>.relu1(<span class="va">self</span>.fc1(x)))</span>
<span id="cb17-117"><a href="#cb17-117"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout2(<span class="va">self</span>.relu2(<span class="va">self</span>.fc2(x)))</span>
<span id="cb17-118"><a href="#cb17-118"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb17-119"><a href="#cb17-119"></a>        <span class="cf">return</span> x</span>
<span id="cb17-120"><a href="#cb17-120"></a></span>
<span id="cb17-121"><a href="#cb17-121"></a><span class="co"># Create the network</span></span>
<span id="cb17-122"><a href="#cb17-122"></a>simple_net <span class="op">=</span> SimpleNet(num_classes<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-123"><a href="#cb17-123"></a><span class="bu">print</span>(<span class="st">"Simple Neural Network:"</span>)</span>
<span id="cb17-124"><a href="#cb17-124"></a><span class="bu">print</span>(simple_net)</span>
<span id="cb17-125"><a href="#cb17-125"></a></span>
<span id="cb17-126"><a href="#cb17-126"></a><span class="co"># Count parameters</span></span>
<span id="cb17-127"><a href="#cb17-127"></a>total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> simple_net.parameters())</span>
<span id="cb17-128"><a href="#cb17-128"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Total parameters: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb17-129"><a href="#cb17-129"></a><span class="in">```</span></span>
<span id="cb17-130"><a href="#cb17-130"></a></span>
<span id="cb17-131"><a href="#cb17-131"></a>**🎯 Try it yourself!** <span class="co">[</span><span class="ot">Open in Colab</span><span class="co">](https://colab.research.google.com/github/hasanpasha/quarto_blog_hasan/blob/main/notebooks/cv-foundations-06-why-deep-learning.ipynb)</span></span>
<span id="cb17-132"><a href="#cb17-132"></a></span>
<span id="cb17-133"><a href="#cb17-133"></a><span class="fu">## The Convolutional Revolution</span></span>
<span id="cb17-134"><a href="#cb17-134"></a></span>
<span id="cb17-135"><a href="#cb17-135"></a>But wait—there's a problem with our simple network. It treats each pixel independently, ignoring spatial relationships. That's like reading a book by looking at each letter separately!</span>
<span id="cb17-136"><a href="#cb17-136"></a></span>
<span id="cb17-137"><a href="#cb17-137"></a>Enter **Convolutional Neural Networks (CNNs)**—networks designed specifically for images:</span>
<span id="cb17-138"><a href="#cb17-138"></a></span>
<span id="cb17-141"><a href="#cb17-141"></a><span class="in">```{python}</span></span>
<span id="cb17-142"><a href="#cb17-142"></a><span class="co">#| eval: true</span></span>
<span id="cb17-143"><a href="#cb17-143"></a><span class="kw">class</span> ConvNet(nn.Module):</span>
<span id="cb17-144"><a href="#cb17-144"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb17-145"><a href="#cb17-145"></a>        <span class="bu">super</span>(ConvNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb17-146"><a href="#cb17-146"></a>        </span>
<span id="cb17-147"><a href="#cb17-147"></a>        <span class="co"># Convolutional layers (feature extractors)</span></span>
<span id="cb17-148"><a href="#cb17-148"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-149"><a href="#cb17-149"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-150"><a href="#cb17-150"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-151"><a href="#cb17-151"></a>        </span>
<span id="cb17-152"><a href="#cb17-152"></a>        <span class="co"># Pooling layers (downsampling)</span></span>
<span id="cb17-153"><a href="#cb17-153"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb17-154"><a href="#cb17-154"></a>        </span>
<span id="cb17-155"><a href="#cb17-155"></a>        <span class="co"># Fully connected layers (classifier)</span></span>
<span id="cb17-156"><a href="#cb17-156"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">128</span> <span class="op">*</span> <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">512</span>)  <span class="co"># 224/8 = 28 after 3 pooling layers</span></span>
<span id="cb17-157"><a href="#cb17-157"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">512</span>, num_classes)</span>
<span id="cb17-158"><a href="#cb17-158"></a>        </span>
<span id="cb17-159"><a href="#cb17-159"></a>        <span class="co"># Activation and regularization</span></span>
<span id="cb17-160"><a href="#cb17-160"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb17-161"><a href="#cb17-161"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb17-162"><a href="#cb17-162"></a>    </span>
<span id="cb17-163"><a href="#cb17-163"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb17-164"><a href="#cb17-164"></a>        <span class="co"># Feature extraction</span></span>
<span id="cb17-165"><a href="#cb17-165"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(<span class="va">self</span>.relu(<span class="va">self</span>.conv1(x)))  <span class="co"># 224x224 -&gt; 112x112</span></span>
<span id="cb17-166"><a href="#cb17-166"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(<span class="va">self</span>.relu(<span class="va">self</span>.conv2(x)))  <span class="co"># 112x112 -&gt; 56x56</span></span>
<span id="cb17-167"><a href="#cb17-167"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(<span class="va">self</span>.relu(<span class="va">self</span>.conv3(x)))  <span class="co"># 56x56 -&gt; 28x28</span></span>
<span id="cb17-168"><a href="#cb17-168"></a>        </span>
<span id="cb17-169"><a href="#cb17-169"></a>        <span class="co"># Flatten and classify</span></span>
<span id="cb17-170"><a href="#cb17-170"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)  <span class="co"># Flatten</span></span>
<span id="cb17-171"><a href="#cb17-171"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.relu(<span class="va">self</span>.fc1(x)))</span>
<span id="cb17-172"><a href="#cb17-172"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb17-173"><a href="#cb17-173"></a>        </span>
<span id="cb17-174"><a href="#cb17-174"></a>        <span class="cf">return</span> x</span>
<span id="cb17-175"><a href="#cb17-175"></a></span>
<span id="cb17-176"><a href="#cb17-176"></a><span class="co"># Create CNN</span></span>
<span id="cb17-177"><a href="#cb17-177"></a>cnn <span class="op">=</span> ConvNet(num_classes<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-178"><a href="#cb17-178"></a><span class="bu">print</span>(<span class="st">"Convolutional Neural Network:"</span>)</span>
<span id="cb17-179"><a href="#cb17-179"></a><span class="bu">print</span>(cnn)</span>
<span id="cb17-180"><a href="#cb17-180"></a></span>
<span id="cb17-181"><a href="#cb17-181"></a><span class="co"># Count parameters</span></span>
<span id="cb17-182"><a href="#cb17-182"></a>cnn_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> cnn.parameters())</span>
<span id="cb17-183"><a href="#cb17-183"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">CNN parameters: </span><span class="sc">{</span>cnn_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb17-184"><a href="#cb17-184"></a><span class="bu">print</span>(<span class="ss">f"Simple net parameters: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb17-185"><a href="#cb17-185"></a><span class="bu">print</span>(<span class="ss">f"CNN has </span><span class="sc">{</span>(total_params <span class="op">-</span> cnn_params) <span class="op">/</span> total_params <span class="op">*</span> <span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">% fewer parameters!"</span>)</span>
<span id="cb17-186"><a href="#cb17-186"></a><span class="in">```</span></span>
<span id="cb17-187"><a href="#cb17-187"></a></span>
<span id="cb17-188"><a href="#cb17-188"></a><span class="fu">## Understanding Convolutions: The Sliding Window</span></span>
<span id="cb17-189"><a href="#cb17-189"></a></span>
<span id="cb17-190"><a href="#cb17-190"></a>Let's visualize what convolutions actually do:</span>
<span id="cb17-191"><a href="#cb17-191"></a></span>
<span id="cb17-194"><a href="#cb17-194"></a><span class="in">```{python}</span></span>
<span id="cb17-195"><a href="#cb17-195"></a><span class="co">#| eval: true</span></span>
<span id="cb17-196"><a href="#cb17-196"></a><span class="kw">def</span> visualize_convolution():</span>
<span id="cb17-197"><a href="#cb17-197"></a>    <span class="co">"""Show how convolution works"""</span></span>
<span id="cb17-198"><a href="#cb17-198"></a>    <span class="co"># Create a simple image</span></span>
<span id="cb17-199"><a href="#cb17-199"></a>    image <span class="op">=</span> np.array([</span>
<span id="cb17-200"><a href="#cb17-200"></a>        [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb17-201"><a href="#cb17-201"></a>        [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb17-202"><a href="#cb17-202"></a>        [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb17-203"><a href="#cb17-203"></a>        [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb17-204"><a href="#cb17-204"></a>        [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb17-205"><a href="#cb17-205"></a>    ])</span>
<span id="cb17-206"><a href="#cb17-206"></a>    </span>
<span id="cb17-207"><a href="#cb17-207"></a>    <span class="co"># Edge detection kernel</span></span>
<span id="cb17-208"><a href="#cb17-208"></a>    kernel <span class="op">=</span> np.array([</span>
<span id="cb17-209"><a href="#cb17-209"></a>        [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb17-210"><a href="#cb17-210"></a>        [<span class="op">-</span><span class="dv">1</span>,  <span class="dv">8</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb17-211"><a href="#cb17-211"></a>        [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb17-212"><a href="#cb17-212"></a>    ])</span>
<span id="cb17-213"><a href="#cb17-213"></a>    </span>
<span id="cb17-214"><a href="#cb17-214"></a>    <span class="co"># Apply convolution manually</span></span>
<span id="cb17-215"><a href="#cb17-215"></a>    result <span class="op">=</span> np.zeros((<span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb17-216"><a href="#cb17-216"></a>    </span>
<span id="cb17-217"><a href="#cb17-217"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb17-218"><a href="#cb17-218"></a>    </span>
<span id="cb17-219"><a href="#cb17-219"></a>    <span class="co"># Show original image</span></span>
<span id="cb17-220"><a href="#cb17-220"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>)</span>
<span id="cb17-221"><a href="#cb17-221"></a>    plt.imshow(image, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb17-222"><a href="#cb17-222"></a>    plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb17-223"><a href="#cb17-223"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-224"><a href="#cb17-224"></a>    </span>
<span id="cb17-225"><a href="#cb17-225"></a>    <span class="co"># Show kernel</span></span>
<span id="cb17-226"><a href="#cb17-226"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">2</span>)</span>
<span id="cb17-227"><a href="#cb17-227"></a>    plt.imshow(kernel, cmap<span class="op">=</span><span class="st">'RdBu'</span>)</span>
<span id="cb17-228"><a href="#cb17-228"></a>    plt.title(<span class="st">"Edge Detection Kernel"</span>)</span>
<span id="cb17-229"><a href="#cb17-229"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-230"><a href="#cb17-230"></a>    </span>
<span id="cb17-231"><a href="#cb17-231"></a>    <span class="co"># Show convolution process</span></span>
<span id="cb17-232"><a href="#cb17-232"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb17-233"><a href="#cb17-233"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb17-234"><a href="#cb17-234"></a>            patch <span class="op">=</span> image[i:i<span class="op">+</span><span class="dv">3</span>, j:j<span class="op">+</span><span class="dv">3</span>]</span>
<span id="cb17-235"><a href="#cb17-235"></a>            result[i, j] <span class="op">=</span> np.<span class="bu">sum</span>(patch <span class="op">*</span> kernel)</span>
<span id="cb17-236"><a href="#cb17-236"></a>    </span>
<span id="cb17-237"><a href="#cb17-237"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb17-238"><a href="#cb17-238"></a>    plt.imshow(result, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb17-239"><a href="#cb17-239"></a>    plt.title(<span class="st">"Convolution Result"</span>)</span>
<span id="cb17-240"><a href="#cb17-240"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-241"><a href="#cb17-241"></a>    </span>
<span id="cb17-242"><a href="#cb17-242"></a>    <span class="co"># Show using PyTorch</span></span>
<span id="cb17-243"><a href="#cb17-243"></a>    image_tensor <span class="op">=</span> torch.FloatTensor(image).unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb17-244"><a href="#cb17-244"></a>    kernel_tensor <span class="op">=</span> torch.FloatTensor(kernel).unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb17-245"><a href="#cb17-245"></a>    </span>
<span id="cb17-246"><a href="#cb17-246"></a>    conv_layer <span class="op">=</span> nn.Conv2d(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-247"><a href="#cb17-247"></a>    conv_layer.weight.data <span class="op">=</span> kernel_tensor</span>
<span id="cb17-248"><a href="#cb17-248"></a>    </span>
<span id="cb17-249"><a href="#cb17-249"></a>    pytorch_result <span class="op">=</span> conv_layer(image_tensor).squeeze().detach().numpy()</span>
<span id="cb17-250"><a href="#cb17-250"></a>    </span>
<span id="cb17-251"><a href="#cb17-251"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb17-252"><a href="#cb17-252"></a>    plt.imshow(pytorch_result, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb17-253"><a href="#cb17-253"></a>    plt.title(<span class="st">"PyTorch Result"</span>)</span>
<span id="cb17-254"><a href="#cb17-254"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-255"><a href="#cb17-255"></a>    </span>
<span id="cb17-256"><a href="#cb17-256"></a>    plt.tight_layout()</span>
<span id="cb17-257"><a href="#cb17-257"></a>    plt.show()</span>
<span id="cb17-258"><a href="#cb17-258"></a></span>
<span id="cb17-259"><a href="#cb17-259"></a>visualize_convolution()</span>
<span id="cb17-260"><a href="#cb17-260"></a><span class="in">```</span></span>
<span id="cb17-261"><a href="#cb17-261"></a></span>
<span id="cb17-262"><a href="#cb17-262"></a><span class="fu">## Transfer Learning: Standing on Giants' Shoulders</span></span>
<span id="cb17-263"><a href="#cb17-263"></a></span>
<span id="cb17-264"><a href="#cb17-264"></a>Here's the secret that makes deep learning practical: **Transfer Learning**. Instead of training from scratch, we use pre-trained models and adapt them to our needs:</span>
<span id="cb17-265"><a href="#cb17-265"></a></span>
<span id="cb17-268"><a href="#cb17-268"></a><span class="in">```{python}</span></span>
<span id="cb17-269"><a href="#cb17-269"></a><span class="co">#| eval: true</span></span>
<span id="cb17-270"><a href="#cb17-270"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb17-271"><a href="#cb17-271"></a></span>
<span id="cb17-272"><a href="#cb17-272"></a><span class="co"># Load a pre-trained ResNet model</span></span>
<span id="cb17-273"><a href="#cb17-273"></a>pretrained_model <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-274"><a href="#cb17-274"></a><span class="bu">print</span>(<span class="st">"Pre-trained ResNet-18:"</span>)</span>
<span id="cb17-275"><a href="#cb17-275"></a><span class="bu">print</span>(pretrained_model)</span>
<span id="cb17-276"><a href="#cb17-276"></a></span>
<span id="cb17-277"><a href="#cb17-277"></a><span class="co"># Modify for our task (cat vs dog classification)</span></span>
<span id="cb17-278"><a href="#cb17-278"></a>num_classes <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb17-279"><a href="#cb17-279"></a>pretrained_model.fc <span class="op">=</span> nn.Linear(pretrained_model.fc.in_features, num_classes)</span>
<span id="cb17-280"><a href="#cb17-280"></a></span>
<span id="cb17-281"><a href="#cb17-281"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Modified final layer for </span><span class="sc">{</span>num_classes<span class="sc">}</span><span class="ss"> classes"</span>)</span>
<span id="cb17-282"><a href="#cb17-282"></a><span class="bu">print</span>(<span class="ss">f"Final layer: </span><span class="sc">{</span>pretrained_model<span class="sc">.</span>fc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-283"><a href="#cb17-283"></a></span>
<span id="cb17-284"><a href="#cb17-284"></a><span class="co"># Freeze early layers (optional)</span></span>
<span id="cb17-285"><a href="#cb17-285"></a><span class="cf">for</span> param <span class="kw">in</span> pretrained_model.parameters():</span>
<span id="cb17-286"><a href="#cb17-286"></a>    param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb17-287"><a href="#cb17-287"></a></span>
<span id="cb17-288"><a href="#cb17-288"></a><span class="co"># Only train the final layer</span></span>
<span id="cb17-289"><a href="#cb17-289"></a><span class="cf">for</span> param <span class="kw">in</span> pretrained_model.fc.parameters():</span>
<span id="cb17-290"><a href="#cb17-290"></a>    param.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb17-291"><a href="#cb17-291"></a></span>
<span id="cb17-292"><a href="#cb17-292"></a>trainable_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> pretrained_model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb17-293"><a href="#cb17-293"></a>total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> pretrained_model.parameters())</span>
<span id="cb17-294"><a href="#cb17-294"></a></span>
<span id="cb17-295"><a href="#cb17-295"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Trainable parameters: </span><span class="sc">{</span>trainable_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb17-296"><a href="#cb17-296"></a><span class="bu">print</span>(<span class="ss">f"Total parameters: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb17-297"><a href="#cb17-297"></a><span class="bu">print</span>(<span class="ss">f"Training only </span><span class="sc">{</span>trainable_params<span class="op">/</span>total_params<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">% of parameters!"</span>)</span>
<span id="cb17-298"><a href="#cb17-298"></a><span class="in">```</span></span>
<span id="cb17-299"><a href="#cb17-299"></a></span>
<span id="cb17-300"><a href="#cb17-300"></a><span class="fu">## Your First Image Classifier</span></span>
<span id="cb17-301"><a href="#cb17-301"></a></span>
<span id="cb17-302"><a href="#cb17-302"></a>Let's build a complete image classification system:</span>
<span id="cb17-303"><a href="#cb17-303"></a></span>
<span id="cb17-306"><a href="#cb17-306"></a><span class="in">```{python}</span></span>
<span id="cb17-307"><a href="#cb17-307"></a><span class="co">#| eval: true</span></span>
<span id="cb17-308"><a href="#cb17-308"></a><span class="kw">class</span> ImageClassifier:</span>
<span id="cb17-309"><a href="#cb17-309"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model_name<span class="op">=</span><span class="st">'resnet18'</span>, num_classes<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb17-310"><a href="#cb17-310"></a>        <span class="va">self</span>.device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb17-311"><a href="#cb17-311"></a>        <span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-312"><a href="#cb17-312"></a>        </span>
<span id="cb17-313"><a href="#cb17-313"></a>        <span class="co"># Load pre-trained model</span></span>
<span id="cb17-314"><a href="#cb17-314"></a>        <span class="cf">if</span> model_name <span class="op">==</span> <span class="st">'resnet18'</span>:</span>
<span id="cb17-315"><a href="#cb17-315"></a>            <span class="va">self</span>.model <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-316"><a href="#cb17-316"></a>            <span class="va">self</span>.model.fc <span class="op">=</span> nn.Linear(<span class="va">self</span>.model.fc.in_features, num_classes)</span>
<span id="cb17-317"><a href="#cb17-317"></a>        </span>
<span id="cb17-318"><a href="#cb17-318"></a>        <span class="va">self</span>.model <span class="op">=</span> <span class="va">self</span>.model.to(<span class="va">self</span>.device)</span>
<span id="cb17-319"><a href="#cb17-319"></a>        </span>
<span id="cb17-320"><a href="#cb17-320"></a>        <span class="co"># Define transforms</span></span>
<span id="cb17-321"><a href="#cb17-321"></a>        <span class="va">self</span>.transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb17-322"><a href="#cb17-322"></a>            transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb17-323"><a href="#cb17-323"></a>            transforms.ToTensor(),</span>
<span id="cb17-324"><a href="#cb17-324"></a>            transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], </span>
<span id="cb17-325"><a href="#cb17-325"></a>                               std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb17-326"><a href="#cb17-326"></a>        ])</span>
<span id="cb17-327"><a href="#cb17-327"></a>        </span>
<span id="cb17-328"><a href="#cb17-328"></a>        <span class="va">self</span>.classes <span class="op">=</span> [<span class="st">'cat'</span>, <span class="st">'dog'</span>]  <span class="co"># Update based on your classes</span></span>
<span id="cb17-329"><a href="#cb17-329"></a>    </span>
<span id="cb17-330"><a href="#cb17-330"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, image):</span>
<span id="cb17-331"><a href="#cb17-331"></a>        <span class="co">"""Predict class of a single image"""</span></span>
<span id="cb17-332"><a href="#cb17-332"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb17-333"><a href="#cb17-333"></a>        </span>
<span id="cb17-334"><a href="#cb17-334"></a>        <span class="co"># Preprocess image</span></span>
<span id="cb17-335"><a href="#cb17-335"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(image, np.ndarray):</span>
<span id="cb17-336"><a href="#cb17-336"></a>            <span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb17-337"><a href="#cb17-337"></a>            image <span class="op">=</span> Image.fromarray(image)</span>
<span id="cb17-338"><a href="#cb17-338"></a>        </span>
<span id="cb17-339"><a href="#cb17-339"></a>        input_tensor <span class="op">=</span> <span class="va">self</span>.transform(image).unsqueeze(<span class="dv">0</span>).to(<span class="va">self</span>.device)</span>
<span id="cb17-340"><a href="#cb17-340"></a>        </span>
<span id="cb17-341"><a href="#cb17-341"></a>        <span class="co"># Make prediction</span></span>
<span id="cb17-342"><a href="#cb17-342"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-343"><a href="#cb17-343"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(input_tensor)</span>
<span id="cb17-344"><a href="#cb17-344"></a>            probabilities <span class="op">=</span> torch.softmax(outputs, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-345"><a href="#cb17-345"></a>            predicted_class <span class="op">=</span> torch.argmax(probabilities, dim<span class="op">=</span><span class="dv">1</span>).item()</span>
<span id="cb17-346"><a href="#cb17-346"></a>            confidence <span class="op">=</span> probabilities[<span class="dv">0</span>][predicted_class].item()</span>
<span id="cb17-347"><a href="#cb17-347"></a>        </span>
<span id="cb17-348"><a href="#cb17-348"></a>        <span class="cf">return</span> {</span>
<span id="cb17-349"><a href="#cb17-349"></a>            <span class="st">'class'</span>: <span class="va">self</span>.classes[predicted_class],</span>
<span id="cb17-350"><a href="#cb17-350"></a>            <span class="st">'confidence'</span>: confidence,</span>
<span id="cb17-351"><a href="#cb17-351"></a>            <span class="st">'probabilities'</span>: probabilities.cpu().numpy()[<span class="dv">0</span>]</span>
<span id="cb17-352"><a href="#cb17-352"></a>        }</span>
<span id="cb17-353"><a href="#cb17-353"></a>    </span>
<span id="cb17-354"><a href="#cb17-354"></a>    <span class="kw">def</span> predict_batch(<span class="va">self</span>, images):</span>
<span id="cb17-355"><a href="#cb17-355"></a>        <span class="co">"""Predict classes for multiple images"""</span></span>
<span id="cb17-356"><a href="#cb17-356"></a>        results <span class="op">=</span> []</span>
<span id="cb17-357"><a href="#cb17-357"></a>        <span class="cf">for</span> image <span class="kw">in</span> images:</span>
<span id="cb17-358"><a href="#cb17-358"></a>            result <span class="op">=</span> <span class="va">self</span>.predict(image)</span>
<span id="cb17-359"><a href="#cb17-359"></a>            results.append(result)</span>
<span id="cb17-360"><a href="#cb17-360"></a>        <span class="cf">return</span> results</span>
<span id="cb17-361"><a href="#cb17-361"></a></span>
<span id="cb17-362"><a href="#cb17-362"></a><span class="co"># Create classifier</span></span>
<span id="cb17-363"><a href="#cb17-363"></a>classifier <span class="op">=</span> ImageClassifier()</span>
<span id="cb17-364"><a href="#cb17-364"></a></span>
<span id="cb17-365"><a href="#cb17-365"></a><span class="co"># Test with a sample image (you would load your own)</span></span>
<span id="cb17-366"><a href="#cb17-366"></a><span class="kw">def</span> test_classifier(image_path):</span>
<span id="cb17-367"><a href="#cb17-367"></a>    <span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb17-368"><a href="#cb17-368"></a>    </span>
<span id="cb17-369"><a href="#cb17-369"></a>    <span class="co"># Load image</span></span>
<span id="cb17-370"><a href="#cb17-370"></a>    image <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb17-371"><a href="#cb17-371"></a>    </span>
<span id="cb17-372"><a href="#cb17-372"></a>    <span class="co"># Make prediction</span></span>
<span id="cb17-373"><a href="#cb17-373"></a>    result <span class="op">=</span> classifier.predict(image)</span>
<span id="cb17-374"><a href="#cb17-374"></a>    </span>
<span id="cb17-375"><a href="#cb17-375"></a>    <span class="co"># Display result</span></span>
<span id="cb17-376"><a href="#cb17-376"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb17-377"><a href="#cb17-377"></a>    </span>
<span id="cb17-378"><a href="#cb17-378"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb17-379"><a href="#cb17-379"></a>    plt.imshow(image)</span>
<span id="cb17-380"><a href="#cb17-380"></a>    plt.title(<span class="ss">f"Input Image"</span>)</span>
<span id="cb17-381"><a href="#cb17-381"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-382"><a href="#cb17-382"></a>    </span>
<span id="cb17-383"><a href="#cb17-383"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb17-384"><a href="#cb17-384"></a>    plt.bar(classifier.classes, result[<span class="st">'probabilities'</span>])</span>
<span id="cb17-385"><a href="#cb17-385"></a>    plt.title(<span class="ss">f"Prediction: </span><span class="sc">{</span>result[<span class="st">'class'</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>result[<span class="st">'confidence'</span>]<span class="sc">:.2f}</span><span class="ss">)"</span>)</span>
<span id="cb17-386"><a href="#cb17-386"></a>    plt.ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb17-387"><a href="#cb17-387"></a>    </span>
<span id="cb17-388"><a href="#cb17-388"></a>    plt.tight_layout()</span>
<span id="cb17-389"><a href="#cb17-389"></a>    plt.show()</span>
<span id="cb17-390"><a href="#cb17-390"></a>    </span>
<span id="cb17-391"><a href="#cb17-391"></a>    <span class="cf">return</span> result</span>
<span id="cb17-392"><a href="#cb17-392"></a></span>
<span id="cb17-393"><a href="#cb17-393"></a><span class="co"># Test the classifier</span></span>
<span id="cb17-394"><a href="#cb17-394"></a><span class="co"># result = test_classifier('your_image.jpg')</span></span>
<span id="cb17-395"><a href="#cb17-395"></a><span class="in">```</span></span>
<span id="cb17-396"><a href="#cb17-396"></a></span>
<span id="cb17-397"><a href="#cb17-397"></a><span class="fu">## Visualizing What Neural Networks Learn</span></span>
<span id="cb17-398"><a href="#cb17-398"></a></span>
<span id="cb17-399"><a href="#cb17-399"></a>One of the coolest things about deep learning is visualizing what the network actually learns:</span>
<span id="cb17-400"><a href="#cb17-400"></a></span>
<span id="cb17-403"><a href="#cb17-403"></a><span class="in">```{python}</span></span>
<span id="cb17-404"><a href="#cb17-404"></a><span class="co">#| eval: true</span></span>
<span id="cb17-405"><a href="#cb17-405"></a><span class="kw">def</span> visualize_filters(model, layer_name<span class="op">=</span><span class="st">'conv1'</span>):</span>
<span id="cb17-406"><a href="#cb17-406"></a>    <span class="co">"""Visualize the filters learned by a convolutional layer"""</span></span>
<span id="cb17-407"><a href="#cb17-407"></a>    </span>
<span id="cb17-408"><a href="#cb17-408"></a>    <span class="co"># Get the layer</span></span>
<span id="cb17-409"><a href="#cb17-409"></a>    layer <span class="op">=</span> <span class="bu">dict</span>(model.named_modules())[layer_name]</span>
<span id="cb17-410"><a href="#cb17-410"></a>    </span>
<span id="cb17-411"><a href="#cb17-411"></a>    <span class="co"># Get the weights</span></span>
<span id="cb17-412"><a href="#cb17-412"></a>    weights <span class="op">=</span> layer.weight.data.cpu()</span>
<span id="cb17-413"><a href="#cb17-413"></a>    </span>
<span id="cb17-414"><a href="#cb17-414"></a>    <span class="co"># Normalize weights for visualization</span></span>
<span id="cb17-415"><a href="#cb17-415"></a>    weights <span class="op">=</span> (weights <span class="op">-</span> weights.<span class="bu">min</span>()) <span class="op">/</span> (weights.<span class="bu">max</span>() <span class="op">-</span> weights.<span class="bu">min</span>())</span>
<span id="cb17-416"><a href="#cb17-416"></a>    </span>
<span id="cb17-417"><a href="#cb17-417"></a>    <span class="co"># Plot filters</span></span>
<span id="cb17-418"><a href="#cb17-418"></a>    num_filters <span class="op">=</span> <span class="bu">min</span>(<span class="dv">16</span>, weights.shape[<span class="dv">0</span>])  <span class="co"># Show first 16 filters</span></span>
<span id="cb17-419"><a href="#cb17-419"></a>    </span>
<span id="cb17-420"><a href="#cb17-420"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb17-421"><a href="#cb17-421"></a>    </span>
<span id="cb17-422"><a href="#cb17-422"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_filters):</span>
<span id="cb17-423"><a href="#cb17-423"></a>        plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb17-424"><a href="#cb17-424"></a>        </span>
<span id="cb17-425"><a href="#cb17-425"></a>        <span class="cf">if</span> weights.shape[<span class="dv">1</span>] <span class="op">==</span> <span class="dv">3</span>:  <span class="co"># RGB filters</span></span>
<span id="cb17-426"><a href="#cb17-426"></a>            <span class="co"># Transpose from (C, H, W) to (H, W, C)</span></span>
<span id="cb17-427"><a href="#cb17-427"></a>            filter_img <span class="op">=</span> weights[i].permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb17-428"><a href="#cb17-428"></a>            plt.imshow(filter_img)</span>
<span id="cb17-429"><a href="#cb17-429"></a>        <span class="cf">else</span>:  <span class="co"># Grayscale filters</span></span>
<span id="cb17-430"><a href="#cb17-430"></a>            plt.imshow(weights[i, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb17-431"><a href="#cb17-431"></a>        </span>
<span id="cb17-432"><a href="#cb17-432"></a>        plt.title(<span class="ss">f"Filter </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-433"><a href="#cb17-433"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-434"><a href="#cb17-434"></a>    </span>
<span id="cb17-435"><a href="#cb17-435"></a>    plt.suptitle(<span class="ss">f"Learned Filters in </span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-436"><a href="#cb17-436"></a>    plt.tight_layout()</span>
<span id="cb17-437"><a href="#cb17-437"></a>    plt.show()</span>
<span id="cb17-438"><a href="#cb17-438"></a></span>
<span id="cb17-439"><a href="#cb17-439"></a><span class="co"># Visualize filters from our pre-trained model</span></span>
<span id="cb17-440"><a href="#cb17-440"></a>visualize_filters(pretrained_model, <span class="st">'conv1'</span>)</span>
<span id="cb17-441"><a href="#cb17-441"></a><span class="in">```</span></span>
<span id="cb17-442"><a href="#cb17-442"></a></span>
<span id="cb17-443"><a href="#cb17-443"></a><span class="fu">## Feature Maps: Seeing Through the Network's Eyes</span></span>
<span id="cb17-444"><a href="#cb17-444"></a></span>
<span id="cb17-447"><a href="#cb17-447"></a><span class="in">```{python}</span></span>
<span id="cb17-448"><a href="#cb17-448"></a><span class="co">#| eval: true</span></span>
<span id="cb17-449"><a href="#cb17-449"></a><span class="kw">def</span> visualize_feature_maps(model, image, layer_name<span class="op">=</span><span class="st">'layer1'</span>):</span>
<span id="cb17-450"><a href="#cb17-450"></a>    <span class="co">"""Visualize feature maps from a specific layer"""</span></span>
<span id="cb17-451"><a href="#cb17-451"></a>    </span>
<span id="cb17-452"><a href="#cb17-452"></a>    <span class="co"># Hook to capture feature maps</span></span>
<span id="cb17-453"><a href="#cb17-453"></a>    feature_maps <span class="op">=</span> []</span>
<span id="cb17-454"><a href="#cb17-454"></a>    </span>
<span id="cb17-455"><a href="#cb17-455"></a>    <span class="kw">def</span> hook_fn(module, <span class="bu">input</span>, output):</span>
<span id="cb17-456"><a href="#cb17-456"></a>        feature_maps.append(output.cpu())</span>
<span id="cb17-457"><a href="#cb17-457"></a>    </span>
<span id="cb17-458"><a href="#cb17-458"></a>    <span class="co"># Register hook</span></span>
<span id="cb17-459"><a href="#cb17-459"></a>    layer <span class="op">=</span> <span class="bu">dict</span>(model.named_modules())[layer_name]</span>
<span id="cb17-460"><a href="#cb17-460"></a>    hook <span class="op">=</span> layer.register_forward_hook(hook_fn)</span>
<span id="cb17-461"><a href="#cb17-461"></a>    </span>
<span id="cb17-462"><a href="#cb17-462"></a>    <span class="co"># Forward pass</span></span>
<span id="cb17-463"><a href="#cb17-463"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb17-464"><a href="#cb17-464"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-465"><a href="#cb17-465"></a>        _ <span class="op">=</span> model(image.unsqueeze(<span class="dv">0</span>))</span>
<span id="cb17-466"><a href="#cb17-466"></a>    </span>
<span id="cb17-467"><a href="#cb17-467"></a>    <span class="co"># Remove hook</span></span>
<span id="cb17-468"><a href="#cb17-468"></a>    hook.remove()</span>
<span id="cb17-469"><a href="#cb17-469"></a>    </span>
<span id="cb17-470"><a href="#cb17-470"></a>    <span class="co"># Get feature maps</span></span>
<span id="cb17-471"><a href="#cb17-471"></a>    fmaps <span class="op">=</span> feature_maps[<span class="dv">0</span>].squeeze(<span class="dv">0</span>)  <span class="co"># Remove batch dimension</span></span>
<span id="cb17-472"><a href="#cb17-472"></a>    </span>
<span id="cb17-473"><a href="#cb17-473"></a>    <span class="co"># Plot feature maps</span></span>
<span id="cb17-474"><a href="#cb17-474"></a>    num_maps <span class="op">=</span> <span class="bu">min</span>(<span class="dv">16</span>, fmaps.shape[<span class="dv">0</span>])</span>
<span id="cb17-475"><a href="#cb17-475"></a>    </span>
<span id="cb17-476"><a href="#cb17-476"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb17-477"><a href="#cb17-477"></a>    </span>
<span id="cb17-478"><a href="#cb17-478"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_maps):</span>
<span id="cb17-479"><a href="#cb17-479"></a>        plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb17-480"><a href="#cb17-480"></a>        plt.imshow(fmaps[i], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb17-481"><a href="#cb17-481"></a>        plt.title(<span class="ss">f"Feature Map </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-482"><a href="#cb17-482"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-483"><a href="#cb17-483"></a>    </span>
<span id="cb17-484"><a href="#cb17-484"></a>    plt.suptitle(<span class="ss">f"Feature Maps from </span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-485"><a href="#cb17-485"></a>    plt.tight_layout()</span>
<span id="cb17-486"><a href="#cb17-486"></a>    plt.show()</span>
<span id="cb17-487"><a href="#cb17-487"></a></span>
<span id="cb17-488"><a href="#cb17-488"></a><span class="co"># Create a sample input</span></span>
<span id="cb17-489"><a href="#cb17-489"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb17-490"><a href="#cb17-490"></a>visualize_feature_maps(pretrained_model, sample_input)</span>
<span id="cb17-491"><a href="#cb17-491"></a><span class="in">```</span></span>
<span id="cb17-492"><a href="#cb17-492"></a></span>
<span id="cb17-493"><a href="#cb17-493"></a><span class="fu">## The Deep Learning Advantage: Why It Works</span></span>
<span id="cb17-494"><a href="#cb17-494"></a></span>
<span id="cb17-495"><a href="#cb17-495"></a>Let's compare classical vs deep learning approaches on a real problem:</span>
<span id="cb17-496"><a href="#cb17-496"></a></span>
<span id="cb17-499"><a href="#cb17-499"></a><span class="in">```{python}</span></span>
<span id="cb17-500"><a href="#cb17-500"></a><span class="co">#| eval: true</span></span>
<span id="cb17-501"><a href="#cb17-501"></a><span class="kw">def</span> compare_approaches(image):</span>
<span id="cb17-502"><a href="#cb17-502"></a>    <span class="co">"""Compare classical and deep learning approaches"""</span></span>
<span id="cb17-503"><a href="#cb17-503"></a>    </span>
<span id="cb17-504"><a href="#cb17-504"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb17-505"><a href="#cb17-505"></a>    </span>
<span id="cb17-506"><a href="#cb17-506"></a>    <span class="co"># Original image</span></span>
<span id="cb17-507"><a href="#cb17-507"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb17-508"><a href="#cb17-508"></a>    plt.imshow(image)</span>
<span id="cb17-509"><a href="#cb17-509"></a>    plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb17-510"><a href="#cb17-510"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-511"><a href="#cb17-511"></a>    </span>
<span id="cb17-512"><a href="#cb17-512"></a>    <span class="co"># Classical approach: hand-crafted features</span></span>
<span id="cb17-513"><a href="#cb17-513"></a>    gray <span class="op">=</span> cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)</span>
<span id="cb17-514"><a href="#cb17-514"></a>    </span>
<span id="cb17-515"><a href="#cb17-515"></a>    <span class="co"># Edge features</span></span>
<span id="cb17-516"><a href="#cb17-516"></a>    edges <span class="op">=</span> cv2.Canny(gray, <span class="dv">50</span>, <span class="dv">150</span>)</span>
<span id="cb17-517"><a href="#cb17-517"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb17-518"><a href="#cb17-518"></a>    plt.imshow(edges, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb17-519"><a href="#cb17-519"></a>    plt.title(<span class="st">"Classical: Edge Features"</span>)</span>
<span id="cb17-520"><a href="#cb17-520"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-521"><a href="#cb17-521"></a>    </span>
<span id="cb17-522"><a href="#cb17-522"></a>    <span class="co"># Texture features (using LBP-like approach)</span></span>
<span id="cb17-523"><a href="#cb17-523"></a>    <span class="im">from</span> skimage.feature <span class="im">import</span> local_binary_pattern</span>
<span id="cb17-524"><a href="#cb17-524"></a>    lbp <span class="op">=</span> local_binary_pattern(gray, <span class="dv">24</span>, <span class="dv">8</span>, method<span class="op">=</span><span class="st">'uniform'</span>)</span>
<span id="cb17-525"><a href="#cb17-525"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb17-526"><a href="#cb17-526"></a>    plt.imshow(lbp, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb17-527"><a href="#cb17-527"></a>    plt.title(<span class="st">"Classical: Texture Features"</span>)</span>
<span id="cb17-528"><a href="#cb17-528"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-529"><a href="#cb17-529"></a>    </span>
<span id="cb17-530"><a href="#cb17-530"></a>    <span class="co"># Deep learning approach: learned features</span></span>
<span id="cb17-531"><a href="#cb17-531"></a>    <span class="co"># Transform image for the model</span></span>
<span id="cb17-532"><a href="#cb17-532"></a>    transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb17-533"><a href="#cb17-533"></a>        transforms.ToPILImage(),</span>
<span id="cb17-534"><a href="#cb17-534"></a>        transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb17-535"><a href="#cb17-535"></a>        transforms.ToTensor(),</span>
<span id="cb17-536"><a href="#cb17-536"></a>        transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], </span>
<span id="cb17-537"><a href="#cb17-537"></a>                           std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb17-538"><a href="#cb17-538"></a>    ])</span>
<span id="cb17-539"><a href="#cb17-539"></a>    </span>
<span id="cb17-540"><a href="#cb17-540"></a>    input_tensor <span class="op">=</span> transform(image).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb17-541"><a href="#cb17-541"></a>    </span>
<span id="cb17-542"><a href="#cb17-542"></a>    <span class="co"># Get feature maps from different layers</span></span>
<span id="cb17-543"><a href="#cb17-543"></a>    feature_maps <span class="op">=</span> []</span>
<span id="cb17-544"><a href="#cb17-544"></a>    </span>
<span id="cb17-545"><a href="#cb17-545"></a>    <span class="kw">def</span> get_features(name):</span>
<span id="cb17-546"><a href="#cb17-546"></a>        <span class="kw">def</span> hook(model, <span class="bu">input</span>, output):</span>
<span id="cb17-547"><a href="#cb17-547"></a>            feature_maps.append((name, output.cpu()))</span>
<span id="cb17-548"><a href="#cb17-548"></a>        <span class="cf">return</span> hook</span>
<span id="cb17-549"><a href="#cb17-549"></a>    </span>
<span id="cb17-550"><a href="#cb17-550"></a>    <span class="co"># Register hooks</span></span>
<span id="cb17-551"><a href="#cb17-551"></a>    pretrained_model.layer1.register_forward_hook(get_features(<span class="st">'Low-level'</span>))</span>
<span id="cb17-552"><a href="#cb17-552"></a>    pretrained_model.layer3.register_forward_hook(get_features(<span class="st">'Mid-level'</span>))</span>
<span id="cb17-553"><a href="#cb17-553"></a>    pretrained_model.layer4.register_forward_hook(get_features(<span class="st">'High-level'</span>))</span>
<span id="cb17-554"><a href="#cb17-554"></a>    </span>
<span id="cb17-555"><a href="#cb17-555"></a>    <span class="co"># Forward pass</span></span>
<span id="cb17-556"><a href="#cb17-556"></a>    pretrained_model.<span class="bu">eval</span>()</span>
<span id="cb17-557"><a href="#cb17-557"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-558"><a href="#cb17-558"></a>        _ <span class="op">=</span> pretrained_model(input_tensor)</span>
<span id="cb17-559"><a href="#cb17-559"></a>    </span>
<span id="cb17-560"><a href="#cb17-560"></a>    <span class="co"># Visualize learned features</span></span>
<span id="cb17-561"><a href="#cb17-561"></a>    <span class="cf">for</span> i, (name, fmaps) <span class="kw">in</span> <span class="bu">enumerate</span>(feature_maps):</span>
<span id="cb17-562"><a href="#cb17-562"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span> <span class="op">+</span> i)</span>
<span id="cb17-563"><a href="#cb17-563"></a>        <span class="co"># Average across channels for visualization</span></span>
<span id="cb17-564"><a href="#cb17-564"></a>        avg_fmap <span class="op">=</span> torch.mean(fmaps.squeeze(<span class="dv">0</span>), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-565"><a href="#cb17-565"></a>        plt.imshow(avg_fmap, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb17-566"><a href="#cb17-566"></a>        plt.title(<span class="ss">f"Deep Learning: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> Features"</span>)</span>
<span id="cb17-567"><a href="#cb17-567"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb17-568"><a href="#cb17-568"></a>    </span>
<span id="cb17-569"><a href="#cb17-569"></a>    plt.tight_layout()</span>
<span id="cb17-570"><a href="#cb17-570"></a>    plt.show()</span>
<span id="cb17-571"><a href="#cb17-571"></a></span>
<span id="cb17-572"><a href="#cb17-572"></a><span class="co"># Compare approaches (you would use your own image)</span></span>
<span id="cb17-573"><a href="#cb17-573"></a><span class="co"># compare_approaches(your_image)</span></span>
<span id="cb17-574"><a href="#cb17-574"></a><span class="in">```</span></span>
<span id="cb17-575"><a href="#cb17-575"></a></span>
<span id="cb17-576"><a href="#cb17-576"></a><span class="fu">## Why Deep Learning Won</span></span>
<span id="cb17-577"><a href="#cb17-577"></a></span>
<span id="cb17-578"><a href="#cb17-578"></a>Here's why deep learning revolutionized computer vision:</span>
<span id="cb17-579"><a href="#cb17-579"></a></span>
<span id="cb17-580"><a href="#cb17-580"></a><span class="fu">### 1. **Automatic Feature Learning**</span></span>
<span id="cb17-581"><a href="#cb17-581"></a><span class="ss">- </span>No need to hand-craft features</span>
<span id="cb17-582"><a href="#cb17-582"></a><span class="ss">- </span>Learns optimal representations for the task</span>
<span id="cb17-583"><a href="#cb17-583"></a><span class="ss">- </span>Adapts to data variations</span>
<span id="cb17-584"><a href="#cb17-584"></a></span>
<span id="cb17-585"><a href="#cb17-585"></a><span class="fu">### 2. **Hierarchical Representations**</span></span>
<span id="cb17-586"><a href="#cb17-586"></a><span class="ss">- </span>Low-level features (edges, textures)</span>
<span id="cb17-587"><a href="#cb17-587"></a><span class="ss">- </span>Mid-level features (parts, patterns)  </span>
<span id="cb17-588"><a href="#cb17-588"></a><span class="ss">- </span>High-level features (objects, concepts)</span>
<span id="cb17-589"><a href="#cb17-589"></a></span>
<span id="cb17-590"><a href="#cb17-590"></a><span class="fu">### 3. **End-to-End Learning**</span></span>
<span id="cb17-591"><a href="#cb17-591"></a><span class="ss">- </span>Optimizes entire pipeline together</span>
<span id="cb17-592"><a href="#cb17-592"></a><span class="ss">- </span>Features and classifier learned jointly</span>
<span id="cb17-593"><a href="#cb17-593"></a><span class="ss">- </span>Better overall performance</span>
<span id="cb17-594"><a href="#cb17-594"></a></span>
<span id="cb17-595"><a href="#cb17-595"></a><span class="fu">### 4. **Scalability**</span></span>
<span id="cb17-596"><a href="#cb17-596"></a><span class="ss">- </span>Performance improves with more data</span>
<span id="cb17-597"><a href="#cb17-597"></a><span class="ss">- </span>Can handle complex, real-world variations</span>
<span id="cb17-598"><a href="#cb17-598"></a><span class="ss">- </span>Generalizes across domains</span>
<span id="cb17-599"><a href="#cb17-599"></a></span>
<span id="cb17-600"><a href="#cb17-600"></a><span class="fu">## The Modern Deep Learning Pipeline</span></span>
<span id="cb17-601"><a href="#cb17-601"></a></span>
<span id="cb17-604"><a href="#cb17-604"></a><span class="in">```{python}</span></span>
<span id="cb17-605"><a href="#cb17-605"></a><span class="co">#| eval: true</span></span>
<span id="cb17-606"><a href="#cb17-606"></a><span class="kw">class</span> ModernVisionPipeline:</span>
<span id="cb17-607"><a href="#cb17-607"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb17-608"><a href="#cb17-608"></a>        <span class="va">self</span>.device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb17-609"><a href="#cb17-609"></a>        </span>
<span id="cb17-610"><a href="#cb17-610"></a>        <span class="co"># 1. Data preprocessing</span></span>
<span id="cb17-611"><a href="#cb17-611"></a>        <span class="va">self</span>.transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb17-612"><a href="#cb17-612"></a>            transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb17-613"><a href="#cb17-613"></a>            transforms.RandomHorizontalFlip(p<span class="op">=</span><span class="fl">0.5</span>),  <span class="co"># Data augmentation</span></span>
<span id="cb17-614"><a href="#cb17-614"></a>            transforms.RandomRotation(<span class="dv">10</span>),</span>
<span id="cb17-615"><a href="#cb17-615"></a>            transforms.ColorJitter(brightness<span class="op">=</span><span class="fl">0.2</span>, contrast<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb17-616"><a href="#cb17-616"></a>            transforms.ToTensor(),</span>
<span id="cb17-617"><a href="#cb17-617"></a>            transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], </span>
<span id="cb17-618"><a href="#cb17-618"></a>                               std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb17-619"><a href="#cb17-619"></a>        ])</span>
<span id="cb17-620"><a href="#cb17-620"></a>        </span>
<span id="cb17-621"><a href="#cb17-621"></a>        <span class="co"># 2. Model architecture</span></span>
<span id="cb17-622"><a href="#cb17-622"></a>        <span class="va">self</span>.model <span class="op">=</span> models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-623"><a href="#cb17-623"></a>        </span>
<span id="cb17-624"><a href="#cb17-624"></a>        <span class="co"># 3. Transfer learning setup</span></span>
<span id="cb17-625"><a href="#cb17-625"></a>        <span class="co"># Freeze early layers</span></span>
<span id="cb17-626"><a href="#cb17-626"></a>        <span class="cf">for</span> param <span class="kw">in</span> <span class="bu">list</span>(<span class="va">self</span>.model.parameters())[:<span class="op">-</span><span class="dv">10</span>]:</span>
<span id="cb17-627"><a href="#cb17-627"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb17-628"><a href="#cb17-628"></a>        </span>
<span id="cb17-629"><a href="#cb17-629"></a>        <span class="co"># 4. Optimizer and loss</span></span>
<span id="cb17-630"><a href="#cb17-630"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> optim.Adam(</span>
<span id="cb17-631"><a href="#cb17-631"></a>            <span class="bu">filter</span>(<span class="kw">lambda</span> p: p.requires_grad, <span class="va">self</span>.model.parameters()),</span>
<span id="cb17-632"><a href="#cb17-632"></a>            lr<span class="op">=</span><span class="fl">0.001</span></span>
<span id="cb17-633"><a href="#cb17-633"></a>        )</span>
<span id="cb17-634"><a href="#cb17-634"></a>        <span class="va">self</span>.criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb17-635"><a href="#cb17-635"></a>    </span>
<span id="cb17-636"><a href="#cb17-636"></a>    <span class="kw">def</span> train_step(<span class="va">self</span>, images, labels):</span>
<span id="cb17-637"><a href="#cb17-637"></a>        <span class="co">"""Single training step"""</span></span>
<span id="cb17-638"><a href="#cb17-638"></a>        <span class="va">self</span>.model.train()</span>
<span id="cb17-639"><a href="#cb17-639"></a>        </span>
<span id="cb17-640"><a href="#cb17-640"></a>        <span class="co"># Forward pass</span></span>
<span id="cb17-641"><a href="#cb17-641"></a>        outputs <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb17-642"><a href="#cb17-642"></a>        loss <span class="op">=</span> <span class="va">self</span>.criterion(outputs, labels)</span>
<span id="cb17-643"><a href="#cb17-643"></a>        </span>
<span id="cb17-644"><a href="#cb17-644"></a>        <span class="co"># Backward pass</span></span>
<span id="cb17-645"><a href="#cb17-645"></a>        <span class="va">self</span>.optimizer.zero_grad()</span>
<span id="cb17-646"><a href="#cb17-646"></a>        loss.backward()</span>
<span id="cb17-647"><a href="#cb17-647"></a>        <span class="va">self</span>.optimizer.step()</span>
<span id="cb17-648"><a href="#cb17-648"></a>        </span>
<span id="cb17-649"><a href="#cb17-649"></a>        <span class="cf">return</span> loss.item()</span>
<span id="cb17-650"><a href="#cb17-650"></a>    </span>
<span id="cb17-651"><a href="#cb17-651"></a>    <span class="kw">def</span> evaluate(<span class="va">self</span>, images, labels):</span>
<span id="cb17-652"><a href="#cb17-652"></a>        <span class="co">"""Evaluation step"""</span></span>
<span id="cb17-653"><a href="#cb17-653"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb17-654"><a href="#cb17-654"></a>        </span>
<span id="cb17-655"><a href="#cb17-655"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-656"><a href="#cb17-656"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb17-657"><a href="#cb17-657"></a>            _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs, <span class="dv">1</span>)</span>
<span id="cb17-658"><a href="#cb17-658"></a>            accuracy <span class="op">=</span> (predicted <span class="op">==</span> labels).<span class="bu">float</span>().mean()</span>
<span id="cb17-659"><a href="#cb17-659"></a>        </span>
<span id="cb17-660"><a href="#cb17-660"></a>        <span class="cf">return</span> accuracy.item()</span>
<span id="cb17-661"><a href="#cb17-661"></a></span>
<span id="cb17-662"><a href="#cb17-662"></a><span class="co"># Create modern pipeline</span></span>
<span id="cb17-663"><a href="#cb17-663"></a>pipeline <span class="op">=</span> ModernVisionPipeline()</span>
<span id="cb17-664"><a href="#cb17-664"></a><span class="bu">print</span>(<span class="st">"Modern deep learning pipeline ready!"</span>)</span>
<span id="cb17-665"><a href="#cb17-665"></a><span class="in">```</span></span>
<span id="cb17-666"><a href="#cb17-666"></a></span>
<span id="cb17-667"><a href="#cb17-667"></a><span class="fu">## What's Coming Next?</span></span>
<span id="cb17-668"><a href="#cb17-668"></a></span>
<span id="cb17-669"><a href="#cb17-669"></a>In our next post, <span class="co">[</span><span class="ot">**"Modern Vision Models: CNNs, Vision Transformers, and DINOv2"**</span><span class="co">](../07-modern-vision-models/)</span>, we'll explore:</span>
<span id="cb17-670"><a href="#cb17-670"></a></span>
<span id="cb17-671"><a href="#cb17-671"></a><span class="ss">- </span>**State-of-the-art architectures** (ResNet, EfficientNet, Vision Transformers)</span>
<span id="cb17-672"><a href="#cb17-672"></a><span class="ss">- </span>**Foundation models** like DINOv2</span>
<span id="cb17-673"><a href="#cb17-673"></a><span class="ss">- </span>**Self-supervised learning** (learning without labels)</span>
<span id="cb17-674"><a href="#cb17-674"></a><span class="ss">- </span>**Building your own DINOv2 feature extractor**</span>
<span id="cb17-675"><a href="#cb17-675"></a></span>
<span id="cb17-676"><a href="#cb17-676"></a>You've just learned why deep learning revolutionized computer vision—next, we'll explore the cutting-edge models that are shaping the future!</span>
<span id="cb17-677"><a href="#cb17-677"></a></span>
<span id="cb17-678"><a href="#cb17-678"></a><span class="fu">## Key Takeaways</span></span>
<span id="cb17-679"><a href="#cb17-679"></a></span>
<span id="cb17-680"><a href="#cb17-680"></a><span class="ss">- </span>**Classical methods hit a wall** with complex real-world variations</span>
<span id="cb17-681"><a href="#cb17-681"></a><span class="ss">- </span>**Deep learning learns features automatically** instead of hand-crafting them</span>
<span id="cb17-682"><a href="#cb17-682"></a><span class="ss">- </span>**CNNs are designed for images** with spatial understanding</span>
<span id="cb17-683"><a href="#cb17-683"></a><span class="ss">- </span>**Transfer learning** makes deep learning practical for everyone</span>
<span id="cb17-684"><a href="#cb17-684"></a><span class="ss">- </span>**Hierarchical features** enable understanding at multiple levels</span>
<span id="cb17-685"><a href="#cb17-685"></a><span class="ss">- </span>**Modern pipelines** combine data augmentation, pre-training, and fine-tuning</span>
<span id="cb17-686"><a href="#cb17-686"></a></span>
<span id="cb17-687"><a href="#cb17-687"></a>:::{.callout-tip}</span>
<span id="cb17-688"><a href="#cb17-688"></a><span class="fu">## Hands-On Lab</span></span>
<span id="cb17-689"><a href="#cb17-689"></a>Ready to build your first deep learning classifier? Try the complete interactive notebook: <span class="co">[</span><span class="ot">**Deep Learning Basics Lab**</span><span class="co">](https://colab.research.google.com/drive/1Deep_Learning_Basics_123456)</span></span>
<span id="cb17-690"><a href="#cb17-690"></a></span>
<span id="cb17-691"><a href="#cb17-691"></a>Train your own cat vs dog classifier and see the power of neural networks!</span>
<span id="cb17-692"><a href="#cb17-692"></a>:::</span>
<span id="cb17-693"><a href="#cb17-693"></a></span>
<span id="cb17-694"><a href="#cb17-694"></a>:::{.callout-note}</span>
<span id="cb17-695"><a href="#cb17-695"></a><span class="fu">## Series Navigation</span></span>
<span id="cb17-696"><a href="#cb17-696"></a><span class="ss">- </span>**Previous**: <span class="co">[</span><span class="ot">Feature Magic: What Makes Images Unique</span><span class="co">](06-feature-magic.qmd)</span></span>
<span id="cb17-697"><a href="#cb17-697"></a><span class="ss">- </span>**Next**: <span class="co">[</span><span class="ot">Modern Vision Models: CNNs, Vision Transformers, and DINOv2</span><span class="co">](08-modern-vision-models.qmd)</span></span>
<span id="cb17-698"><a href="#cb17-698"></a><span class="ss">- </span>**Series Home**: <span class="co">[</span><span class="ot">Computer Vision Foundations</span><span class="co">](../computer-vision-foundations.qmd)</span></span>
<span id="cb17-699"><a href="#cb17-699"></a>:::</span>
<span id="cb17-700"><a href="#cb17-700"></a></span>
<span id="cb17-701"><a href="#cb17-701"></a>---</span>
<span id="cb17-702"><a href="#cb17-702"></a></span>
<span id="cb17-703"><a href="#cb17-703"></a>*You've just witnessed the deep learning revolution! From struggling with cat vs dog classification to achieving superhuman performance on complex tasks—this is why deep learning changed everything. Next, we'll explore the latest and greatest models that are pushing the boundaries even further.* </span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



<script src="../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>