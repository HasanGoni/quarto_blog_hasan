<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hasan">
<meta name="dcterms.date" content="2025-01-22">

<title>Hasan’s Data Science &amp; AI Blog - Modern Vision Models: CNNs, Vision Transformers, and DINOv2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Hasan’s Data Science &amp; AI Blog - Modern Vision Models: CNNs, Vision Transformers, and DINOv2">
<meta property="og:description" content="">
<meta property="og:image" content="https://images.unsplash.com/photo-1620712943543-bcc4688e7485?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2065&amp;q=80">
<meta property="og:site-name" content="Hasan's Data Science &amp; AI Blog">
<meta name="twitter:title" content="Hasan’s Data Science &amp; AI Blog - Modern Vision Models: CNNs, Vision Transformers, and DINOv2">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://images.unsplash.com/photo-1620712943543-bcc4688e7485?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2065&amp;q=80">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Hasan’s Data Science &amp; AI Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-series" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Series</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-series">    
        <li>
    <a class="dropdown-item" href="../../../posts/series/data-science-steps.html" rel="" target="">
 <span class="dropdown-text">Data Science Steps</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/computer-vision-with-pytorch.qmd" rel="" target="">
 <span class="dropdown-text">Computer Vision</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/vlm.html" rel="" target="">
 <span class="dropdown-text">VLM Series</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/anomaly-detection/index.html" rel="" target="">
 <span class="dropdown-text">Anomaly Detection</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../categories.html" rel="" target="">
 <span class="menu-text">Categories</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../tags.html" rel="" target="">
 <span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/HasanGoni" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hasangoni" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Modern Vision Models: CNNs, Vision Transformers, and DINOv2</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Modern Vision Models: CNNs, Vision Transformers, and DINOv2</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">computer-vision</div>
                <div class="quarto-category">transformers</div>
                <div class="quarto-category">foundation-models</div>
                <div class="quarto-category">dinov2</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Hasan </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 22, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">June 28, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Data Science</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/data-science-steps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part01/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 01</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part02/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 02</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part03/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 03</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part04/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 04</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part05/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 05</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part06/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 06</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Computer Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
 <span class="menu-text">posts/series/computer-vision-with-pytorch.qmd</span>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Anomaly Detection</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/anomaly-detection/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/anomaly-detection/finding-the-oddballs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Finding the Oddballs</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">VLM Series</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/vlm-qwen3-14b/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Qwen3-14B</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-evolution-of-vision-from-alexnet-to-dinov2" id="toc-the-evolution-of-vision-from-alexnet-to-dinov2" class="nav-link active" data-scroll-target="#the-evolution-of-vision-from-alexnet-to-dinov2"><span class="header-section-number">0.1</span> The Evolution of Vision: From AlexNet to DINOv2</a></li>
  <li><a href="#the-cnn-dynasty-resnet-efficientnet-and-beyond" id="toc-the-cnn-dynasty-resnet-efficientnet-and-beyond" class="nav-link" data-scroll-target="#the-cnn-dynasty-resnet-efficientnet-and-beyond"><span class="header-section-number">0.2</span> The CNN Dynasty: ResNet, EfficientNet, and Beyond</a>
  <ul class="collapse">
  <li><a href="#resnet-the-skip-connection-revolution" id="toc-resnet-the-skip-connection-revolution" class="nav-link" data-scroll-target="#resnet-the-skip-connection-revolution"><span class="header-section-number">0.2.1</span> ResNet: The Skip Connection Revolution</a></li>
  <li><a href="#efficientnet-scaling-done-right" id="toc-efficientnet-scaling-done-right" class="nav-link" data-scroll-target="#efficientnet-scaling-done-right"><span class="header-section-number">0.2.2</span> EfficientNet: Scaling Done Right</a></li>
  </ul></li>
  <li><a href="#the-transformer-revolution-vision-meets-attention" id="toc-the-transformer-revolution-vision-meets-attention" class="nav-link" data-scroll-target="#the-transformer-revolution-vision-meets-attention"><span class="header-section-number">0.3</span> The Transformer Revolution: Vision Meets Attention</a>
  <ul class="collapse">
  <li><a href="#understanding-vision-transformers" id="toc-understanding-vision-transformers" class="nav-link" data-scroll-target="#understanding-vision-transformers"><span class="header-section-number">0.3.1</span> Understanding Vision Transformers</a></li>
  <li><a href="#visualizing-attention-what-does-the-model-look-at" id="toc-visualizing-attention-what-does-the-model-look-at" class="nav-link" data-scroll-target="#visualizing-attention-what-does-the-model-look-at"><span class="header-section-number">0.3.2</span> Visualizing Attention: What Does the Model Look At?</a></li>
  </ul></li>
  <li><a href="#foundation-models-the-dinov2-revolution" id="toc-foundation-models-the-dinov2-revolution" class="nav-link" data-scroll-target="#foundation-models-the-dinov2-revolution"><span class="header-section-number">0.4</span> Foundation Models: The DINOv2 Revolution</a>
  <ul class="collapse">
  <li><a href="#using-dinov2-with-huggingface" id="toc-using-dinov2-with-huggingface" class="nav-link" data-scroll-target="#using-dinov2-with-huggingface"><span class="header-section-number">0.4.1</span> Using DINOv2 with HuggingFace</a></li>
  <li><a href="#building-a-dinov2-powered-image-similarity-engine" id="toc-building-a-dinov2-powered-image-similarity-engine" class="nav-link" data-scroll-target="#building-a-dinov2-powered-image-similarity-engine"><span class="header-section-number">0.4.2</span> Building a DINOv2-Powered Image Similarity Engine</a></li>
  </ul></li>
  <li><a href="#comparing-all-approaches-the-ultimate-showdown" id="toc-comparing-all-approaches-the-ultimate-showdown" class="nav-link" data-scroll-target="#comparing-all-approaches-the-ultimate-showdown"><span class="header-section-number">0.5</span> Comparing All Approaches: The Ultimate Showdown</a></li>
  <li><a href="#the-future-whats-next" id="toc-the-future-whats-next" class="nav-link" data-scroll-target="#the-future-whats-next"><span class="header-section-number">0.6</span> The Future: What’s Next?</a>
  <ul class="collapse">
  <li><a href="#multimodal-foundation-models" id="toc-multimodal-foundation-models" class="nav-link" data-scroll-target="#multimodal-foundation-models"><span class="header-section-number">0.6.1</span> 1. <strong>Multimodal Foundation Models</strong></a></li>
  <li><a href="#efficient-architectures" id="toc-efficient-architectures" class="nav-link" data-scroll-target="#efficient-architectures"><span class="header-section-number">0.6.2</span> 2. <strong>Efficient Architectures</strong></a></li>
  <li><a href="#self-supervised-learning" id="toc-self-supervised-learning" class="nav-link" data-scroll-target="#self-supervised-learning"><span class="header-section-number">0.6.3</span> 3. <strong>Self-Supervised Learning</strong></a></li>
  </ul></li>
  <li><a href="#your-challenge-build-a-modern-vision-pipeline" id="toc-your-challenge-build-a-modern-vision-pipeline" class="nav-link" data-scroll-target="#your-challenge-build-a-modern-vision-pipeline"><span class="header-section-number">0.7</span> Your Challenge: Build a Modern Vision Pipeline</a></li>
  <li><a href="#whats-coming-next" id="toc-whats-coming-next" class="nav-link" data-scroll-target="#whats-coming-next"><span class="header-section-number">0.8</span> What’s Coming Next?</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">0.9</span> Key Takeaways</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="the-evolution-of-vision-from-alexnet-to-dinov2" class="level2" data-number="0.1">
<h2 data-number="0.1" class="anchored" data-anchor-id="the-evolution-of-vision-from-alexnet-to-dinov2"><span class="header-section-number">0.1</span> The Evolution of Vision: From AlexNet to DINOv2</h2>
<p>Remember when we thought AlexNet was revolutionary in 2012? That was just the beginning! In the past decade, computer vision has evolved at breakneck speed:</p>
<ul>
<li><strong>2012</strong>: AlexNet - 8 layers, 60M parameters</li>
<li><strong>2015</strong>: ResNet - 152 layers, skip connections</li>
<li><strong>2017</strong>: Attention mechanisms emerge</li>
<li><strong>2020</strong>: Vision Transformers - “Attention is all you need” for vision</li>
<li><strong>2023</strong>: DINOv2 - Foundation models that understand everything</li>
</ul>
<p>Today, we’re going to explore this incredible journey and show you how to use the most powerful vision models ever created!</p>
</section>
<section id="the-cnn-dynasty-resnet-efficientnet-and-beyond" class="level2" data-number="0.2">
<h2 data-number="0.2" class="anchored" data-anchor-id="the-cnn-dynasty-resnet-efficientnet-and-beyond"><span class="header-section-number">0.2</span> The CNN Dynasty: ResNet, EfficientNet, and Beyond</h2>
<p>Before transformers took over, CNNs ruled the vision world. Let’s explore the key innovations:</p>
<section id="resnet-the-skip-connection-revolution" class="level3" data-number="0.2.1">
<h3 data-number="0.2.1" class="anchored" data-anchor-id="resnet-the-skip-connection-revolution"><span class="header-section-number">0.2.1</span> ResNet: The Skip Connection Revolution</h3>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># Understanding ResNet's key innovation: skip connections</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="kw">class</span> ResidualBlock(nn.Module):</span>
<span id="cb1-9"><a href="#cb1-9"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, stride<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb1-10"><a href="#cb1-10"></a>        <span class="bu">super</span>(ResidualBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-11"><a href="#cb1-11"></a>        </span>
<span id="cb1-12"><a href="#cb1-12"></a>        <span class="co"># Main path</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_channels, out_channels, <span class="dv">3</span>, stride, <span class="dv">1</span>)</span>
<span id="cb1-14"><a href="#cb1-14"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb1-15"><a href="#cb1-15"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(out_channels, out_channels, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-16"><a href="#cb1-16"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb1-17"><a href="#cb1-17"></a>        </span>
<span id="cb1-18"><a href="#cb1-18"></a>        <span class="co"># Skip connection (the magic!)</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>        <span class="va">self</span>.skip <span class="op">=</span> nn.Sequential()</span>
<span id="cb1-20"><a href="#cb1-20"></a>        <span class="cf">if</span> stride <span class="op">!=</span> <span class="dv">1</span> <span class="kw">or</span> in_channels <span class="op">!=</span> out_channels:</span>
<span id="cb1-21"><a href="#cb1-21"></a>            <span class="va">self</span>.skip <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-22"><a href="#cb1-22"></a>                nn.Conv2d(in_channels, out_channels, <span class="dv">1</span>, stride),</span>
<span id="cb1-23"><a href="#cb1-23"></a>                nn.BatchNorm2d(out_channels)</span>
<span id="cb1-24"><a href="#cb1-24"></a>            )</span>
<span id="cb1-25"><a href="#cb1-25"></a>    </span>
<span id="cb1-26"><a href="#cb1-26"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-27"><a href="#cb1-27"></a>        <span class="co"># Main path</span></span>
<span id="cb1-28"><a href="#cb1-28"></a>        out <span class="op">=</span> torch.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x)))</span>
<span id="cb1-29"><a href="#cb1-29"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(<span class="va">self</span>.conv2(out))</span>
<span id="cb1-30"><a href="#cb1-30"></a>        </span>
<span id="cb1-31"><a href="#cb1-31"></a>        <span class="co"># Add skip connection (this is the key!)</span></span>
<span id="cb1-32"><a href="#cb1-32"></a>        out <span class="op">+=</span> <span class="va">self</span>.skip(x)</span>
<span id="cb1-33"><a href="#cb1-33"></a>        out <span class="op">=</span> torch.relu(out)</span>
<span id="cb1-34"><a href="#cb1-34"></a>        </span>
<span id="cb1-35"><a href="#cb1-35"></a>        <span class="cf">return</span> out</span>
<span id="cb1-36"><a href="#cb1-36"></a></span>
<span id="cb1-37"><a href="#cb1-37"></a><span class="co"># Create a simple ResNet-like model</span></span>
<span id="cb1-38"><a href="#cb1-38"></a><span class="kw">class</span> MiniResNet(nn.Module):</span>
<span id="cb1-39"><a href="#cb1-39"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb1-40"><a href="#cb1-40"></a>        <span class="bu">super</span>(MiniResNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-41"><a href="#cb1-41"></a>        </span>
<span id="cb1-42"><a href="#cb1-42"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb1-43"><a href="#cb1-43"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm2d(<span class="dv">64</span>)</span>
<span id="cb1-44"><a href="#cb1-44"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-45"><a href="#cb1-45"></a>        </span>
<span id="cb1-46"><a href="#cb1-46"></a>        <span class="co"># Stack residual blocks</span></span>
<span id="cb1-47"><a href="#cb1-47"></a>        <span class="va">self</span>.layer1 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-48"><a href="#cb1-48"></a>        <span class="va">self</span>.layer2 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-49"><a href="#cb1-49"></a>        <span class="va">self</span>.layer3 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-50"><a href="#cb1-50"></a>        </span>
<span id="cb1-51"><a href="#cb1-51"></a>        <span class="va">self</span>.avgpool <span class="op">=</span> nn.AdaptiveAvgPool2d((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb1-52"><a href="#cb1-52"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">256</span>, num_classes)</span>
<span id="cb1-53"><a href="#cb1-53"></a>    </span>
<span id="cb1-54"><a href="#cb1-54"></a>    <span class="kw">def</span> _make_layer(<span class="va">self</span>, in_channels, out_channels, num_blocks, stride):</span>
<span id="cb1-55"><a href="#cb1-55"></a>        layers <span class="op">=</span> []</span>
<span id="cb1-56"><a href="#cb1-56"></a>        layers.append(ResidualBlock(in_channels, out_channels, stride))</span>
<span id="cb1-57"><a href="#cb1-57"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_blocks):</span>
<span id="cb1-58"><a href="#cb1-58"></a>            layers.append(ResidualBlock(out_channels, out_channels))</span>
<span id="cb1-59"><a href="#cb1-59"></a>        <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb1-60"><a href="#cb1-60"></a>    </span>
<span id="cb1-61"><a href="#cb1-61"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-62"><a href="#cb1-62"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(torch.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x))))</span>
<span id="cb1-63"><a href="#cb1-63"></a>        x <span class="op">=</span> <span class="va">self</span>.layer1(x)</span>
<span id="cb1-64"><a href="#cb1-64"></a>        x <span class="op">=</span> <span class="va">self</span>.layer2(x)</span>
<span id="cb1-65"><a href="#cb1-65"></a>        x <span class="op">=</span> <span class="va">self</span>.layer3(x)</span>
<span id="cb1-66"><a href="#cb1-66"></a>        x <span class="op">=</span> <span class="va">self</span>.avgpool(x)</span>
<span id="cb1-67"><a href="#cb1-67"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-68"><a href="#cb1-68"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb1-69"><a href="#cb1-69"></a>        <span class="cf">return</span> x</span>
<span id="cb1-70"><a href="#cb1-70"></a></span>
<span id="cb1-71"><a href="#cb1-71"></a><span class="co"># Compare with official ResNet</span></span>
<span id="cb1-72"><a href="#cb1-72"></a>mini_resnet <span class="op">=</span> MiniResNet(num_classes<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb1-73"><a href="#cb1-73"></a>official_resnet <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-74"><a href="#cb1-74"></a></span>
<span id="cb1-75"><a href="#cb1-75"></a><span class="bu">print</span>(<span class="st">"Mini ResNet:"</span>)</span>
<span id="cb1-76"><a href="#cb1-76"></a><span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> mini_resnet.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb1-77"><a href="#cb1-77"></a></span>
<span id="cb1-78"><a href="#cb1-78"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Official ResNet-18:"</span>)</span>
<span id="cb1-79"><a href="#cb1-79"></a><span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> official_resnet.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>🎯 Try it yourself!</strong> <a href="https://colab.research.google.com/github/hasanpasha/quarto_blog_hasan/blob/main/notebooks/cv-foundations-07-modern-vision-models.ipynb">Open in Colab</a></p>
</section>
<section id="efficientnet-scaling-done-right" class="level3" data-number="0.2.2">
<h3 data-number="0.2.2" class="anchored" data-anchor-id="efficientnet-scaling-done-right"><span class="header-section-number">0.2.2</span> EfficientNet: Scaling Done Right</h3>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># EfficientNet's key insight: compound scaling</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">from</span> torchvision.models <span class="im">import</span> efficientnet_b0, efficientnet_b7</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co"># Load different EfficientNet variants</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>efficient_b0 <span class="op">=</span> efficientnet_b0(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-6"><a href="#cb2-6"></a>efficient_b7 <span class="op">=</span> efficientnet_b7(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="kw">def</span> model_info(model, name):</span>
<span id="cb2-9"><a href="#cb2-9"></a>    total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb2-10"><a href="#cb2-10"></a>    <span class="cf">return</span> {</span>
<span id="cb2-11"><a href="#cb2-11"></a>        <span class="st">'name'</span>: name,</span>
<span id="cb2-12"><a href="#cb2-12"></a>        <span class="st">'parameters'</span>: total_params,</span>
<span id="cb2-13"><a href="#cb2-13"></a>        <span class="st">'size_mb'</span>: total_params <span class="op">*</span> <span class="dv">4</span> <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)  <span class="co"># Rough estimate</span></span>
<span id="cb2-14"><a href="#cb2-14"></a>    }</span>
<span id="cb2-15"><a href="#cb2-15"></a></span>
<span id="cb2-16"><a href="#cb2-16"></a>models_comparison <span class="op">=</span> [</span>
<span id="cb2-17"><a href="#cb2-17"></a>    model_info(efficient_b0, <span class="st">'EfficientNet-B0'</span>),</span>
<span id="cb2-18"><a href="#cb2-18"></a>    model_info(efficient_b7, <span class="st">'EfficientNet-B7'</span>),</span>
<span id="cb2-19"><a href="#cb2-19"></a>    model_info(official_resnet, <span class="st">'ResNet-18'</span>)</span>
<span id="cb2-20"><a href="#cb2-20"></a>]</span>
<span id="cb2-21"><a href="#cb2-21"></a></span>
<span id="cb2-22"><a href="#cb2-22"></a><span class="bu">print</span>(<span class="st">"Model Comparison:"</span>)</span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb2-24"><a href="#cb2-24"></a><span class="cf">for</span> info <span class="kw">in</span> models_comparison:</span>
<span id="cb2-25"><a href="#cb2-25"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>info[<span class="st">'name'</span>]<span class="sc">:20}</span><span class="ss"> | </span><span class="sc">{</span>info[<span class="st">'parameters'</span>]<span class="sc">:</span><span class="op">&gt;</span><span class="dv">10</span><span class="sc">,}</span><span class="ss"> params | </span><span class="sc">{</span>info[<span class="st">'size_mb'</span>]<span class="sc">:&gt;6.1f}</span><span class="ss"> MB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="the-transformer-revolution-vision-meets-attention" class="level2" data-number="0.3">
<h2 data-number="0.3" class="anchored" data-anchor-id="the-transformer-revolution-vision-meets-attention"><span class="header-section-number">0.3</span> The Transformer Revolution: Vision Meets Attention</h2>
<p>In 2020, everything changed when researchers asked: “What if we applied transformers to vision?”</p>
<section id="understanding-vision-transformers" class="level3" data-number="0.3.1">
<h3 data-number="0.3.1" class="anchored" data-anchor-id="understanding-vision-transformers"><span class="header-section-number">0.3.1</span> Understanding Vision Transformers</h3>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">class</span> PatchEmbedding(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="co">"""Convert image to sequence of patches"""</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_size<span class="op">=</span><span class="dv">224</span>, patch_size<span class="op">=</span><span class="dv">16</span>, in_channels<span class="op">=</span><span class="dv">3</span>, embed_dim<span class="op">=</span><span class="dv">768</span>):</span>
<span id="cb3-4"><a href="#cb3-4"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-5"><a href="#cb3-5"></a>        <span class="va">self</span>.img_size <span class="op">=</span> img_size</span>
<span id="cb3-6"><a href="#cb3-6"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb3-7"><a href="#cb3-7"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> (img_size <span class="op">//</span> patch_size) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>        </span>
<span id="cb3-9"><a href="#cb3-9"></a>        <span class="co"># Patch embedding using convolution</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>        <span class="va">self</span>.projection <span class="op">=</span> nn.Conv2d(</span>
<span id="cb3-11"><a href="#cb3-11"></a>            in_channels, embed_dim, </span>
<span id="cb3-12"><a href="#cb3-12"></a>            kernel_size<span class="op">=</span>patch_size, stride<span class="op">=</span>patch_size</span>
<span id="cb3-13"><a href="#cb3-13"></a>        )</span>
<span id="cb3-14"><a href="#cb3-14"></a>    </span>
<span id="cb3-15"><a href="#cb3-15"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-16"><a href="#cb3-16"></a>        <span class="co"># x shape: (batch_size, channels, height, width)</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>        x <span class="op">=</span> <span class="va">self</span>.projection(x)  <span class="co"># (batch_size, embed_dim, num_patches_h, num_patches_w)</span></span>
<span id="cb3-18"><a href="#cb3-18"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>)        <span class="co"># (batch_size, embed_dim, num_patches)</span></span>
<span id="cb3-19"><a href="#cb3-19"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)   <span class="co"># (batch_size, num_patches, embed_dim)</span></span>
<span id="cb3-20"><a href="#cb3-20"></a>        <span class="cf">return</span> x</span>
<span id="cb3-21"><a href="#cb3-21"></a></span>
<span id="cb3-22"><a href="#cb3-22"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb3-23"><a href="#cb3-23"></a>    <span class="co">"""Multi-head self-attention mechanism"""</span></span>
<span id="cb3-24"><a href="#cb3-24"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb3-25"><a href="#cb3-25"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-26"><a href="#cb3-26"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb3-27"><a href="#cb3-27"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb3-28"><a href="#cb3-28"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> embed_dim <span class="op">//</span> num_heads</span>
<span id="cb3-29"><a href="#cb3-29"></a>        </span>
<span id="cb3-30"><a href="#cb3-30"></a>        <span class="va">self</span>.qkv <span class="op">=</span> nn.Linear(embed_dim, embed_dim <span class="op">*</span> <span class="dv">3</span>)</span>
<span id="cb3-31"><a href="#cb3-31"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb3-32"><a href="#cb3-32"></a>    </span>
<span id="cb3-33"><a href="#cb3-33"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-34"><a href="#cb3-34"></a>        batch_size, seq_len, embed_dim <span class="op">=</span> x.shape</span>
<span id="cb3-35"><a href="#cb3-35"></a>        </span>
<span id="cb3-36"><a href="#cb3-36"></a>        <span class="co"># Generate Q, K, V</span></span>
<span id="cb3-37"><a href="#cb3-37"></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv(x).reshape(batch_size, seq_len, <span class="dv">3</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb3-38"><a href="#cb3-38"></a>        qkv <span class="op">=</span> qkv.permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>)  <span class="co"># (3, batch_size, num_heads, seq_len, head_dim)</span></span>
<span id="cb3-39"><a href="#cb3-39"></a>        q, k, v <span class="op">=</span> qkv[<span class="dv">0</span>], qkv[<span class="dv">1</span>], qkv[<span class="dv">2</span>]</span>
<span id="cb3-40"><a href="#cb3-40"></a>        </span>
<span id="cb3-41"><a href="#cb3-41"></a>        <span class="co"># Compute attention</span></span>
<span id="cb3-42"><a href="#cb3-42"></a>        attn <span class="op">=</span> (q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> (<span class="va">self</span>.head_dim <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb3-43"><a href="#cb3-43"></a>        attn <span class="op">=</span> torch.softmax(attn, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-44"><a href="#cb3-44"></a>        </span>
<span id="cb3-45"><a href="#cb3-45"></a>        <span class="co"># Apply attention to values</span></span>
<span id="cb3-46"><a href="#cb3-46"></a>        out <span class="op">=</span> (attn <span class="op">@</span> v).transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, seq_len, embed_dim)</span>
<span id="cb3-47"><a href="#cb3-47"></a>        out <span class="op">=</span> <span class="va">self</span>.proj(out)</span>
<span id="cb3-48"><a href="#cb3-48"></a>        </span>
<span id="cb3-49"><a href="#cb3-49"></a>        <span class="cf">return</span> out, attn</span>
<span id="cb3-50"><a href="#cb3-50"></a></span>
<span id="cb3-51"><a href="#cb3-51"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb3-52"><a href="#cb3-52"></a>    <span class="co">"""Single transformer encoder block"""</span></span>
<span id="cb3-53"><a href="#cb3-53"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">12</span>, mlp_ratio<span class="op">=</span><span class="fl">4.0</span>):</span>
<span id="cb3-54"><a href="#cb3-54"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-55"><a href="#cb3-55"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb3-56"><a href="#cb3-56"></a>        <span class="va">self</span>.attn <span class="op">=</span> MultiHeadAttention(embed_dim, num_heads)</span>
<span id="cb3-57"><a href="#cb3-57"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb3-58"><a href="#cb3-58"></a>        </span>
<span id="cb3-59"><a href="#cb3-59"></a>        <span class="co"># MLP</span></span>
<span id="cb3-60"><a href="#cb3-60"></a>        mlp_hidden_dim <span class="op">=</span> <span class="bu">int</span>(embed_dim <span class="op">*</span> mlp_ratio)</span>
<span id="cb3-61"><a href="#cb3-61"></a>        <span class="va">self</span>.mlp <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-62"><a href="#cb3-62"></a>            nn.Linear(embed_dim, mlp_hidden_dim),</span>
<span id="cb3-63"><a href="#cb3-63"></a>            nn.GELU(),</span>
<span id="cb3-64"><a href="#cb3-64"></a>            nn.Linear(mlp_hidden_dim, embed_dim)</span>
<span id="cb3-65"><a href="#cb3-65"></a>        )</span>
<span id="cb3-66"><a href="#cb3-66"></a>    </span>
<span id="cb3-67"><a href="#cb3-67"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-68"><a href="#cb3-68"></a>        <span class="co"># Self-attention with residual connection</span></span>
<span id="cb3-69"><a href="#cb3-69"></a>        attn_out, attn_weights <span class="op">=</span> <span class="va">self</span>.attn(<span class="va">self</span>.norm1(x))</span>
<span id="cb3-70"><a href="#cb3-70"></a>        x <span class="op">=</span> x <span class="op">+</span> attn_out</span>
<span id="cb3-71"><a href="#cb3-71"></a>        </span>
<span id="cb3-72"><a href="#cb3-72"></a>        <span class="co"># MLP with residual connection</span></span>
<span id="cb3-73"><a href="#cb3-73"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.norm2(x))</span>
<span id="cb3-74"><a href="#cb3-74"></a>        </span>
<span id="cb3-75"><a href="#cb3-75"></a>        <span class="cf">return</span> x, attn_weights</span>
<span id="cb3-76"><a href="#cb3-76"></a></span>
<span id="cb3-77"><a href="#cb3-77"></a><span class="kw">class</span> SimpleViT(nn.Module):</span>
<span id="cb3-78"><a href="#cb3-78"></a>    <span class="co">"""Simplified Vision Transformer"""</span></span>
<span id="cb3-79"><a href="#cb3-79"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_size<span class="op">=</span><span class="dv">224</span>, patch_size<span class="op">=</span><span class="dv">16</span>, num_classes<span class="op">=</span><span class="dv">1000</span>, </span>
<span id="cb3-80"><a href="#cb3-80"></a>                 embed_dim<span class="op">=</span><span class="dv">768</span>, depth<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb3-81"><a href="#cb3-81"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-82"><a href="#cb3-82"></a>        </span>
<span id="cb3-83"><a href="#cb3-83"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb3-84"><a href="#cb3-84"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> PatchEmbedding(img_size, patch_size, <span class="dv">3</span>, embed_dim)</span>
<span id="cb3-85"><a href="#cb3-85"></a>        num_patches <span class="op">=</span> <span class="va">self</span>.patch_embed.num_patches</span>
<span id="cb3-86"><a href="#cb3-86"></a>        </span>
<span id="cb3-87"><a href="#cb3-87"></a>        <span class="co"># Class token and position embedding</span></span>
<span id="cb3-88"><a href="#cb3-88"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, embed_dim))</span>
<span id="cb3-89"><a href="#cb3-89"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, num_patches <span class="op">+</span> <span class="dv">1</span>, embed_dim))</span>
<span id="cb3-90"><a href="#cb3-90"></a>        </span>
<span id="cb3-91"><a href="#cb3-91"></a>        <span class="co"># Transformer blocks</span></span>
<span id="cb3-92"><a href="#cb3-92"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.ModuleList([</span>
<span id="cb3-93"><a href="#cb3-93"></a>            TransformerBlock(embed_dim, num_heads) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth)</span>
<span id="cb3-94"><a href="#cb3-94"></a>        ])</span>
<span id="cb3-95"><a href="#cb3-95"></a>        </span>
<span id="cb3-96"><a href="#cb3-96"></a>        <span class="co"># Classification head</span></span>
<span id="cb3-97"><a href="#cb3-97"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb3-98"><a href="#cb3-98"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(embed_dim, num_classes)</span>
<span id="cb3-99"><a href="#cb3-99"></a>    </span>
<span id="cb3-100"><a href="#cb3-100"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-101"><a href="#cb3-101"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb3-102"><a href="#cb3-102"></a>        </span>
<span id="cb3-103"><a href="#cb3-103"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb3-104"><a href="#cb3-104"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)  <span class="co"># (batch_size, num_patches, embed_dim)</span></span>
<span id="cb3-105"><a href="#cb3-105"></a>        </span>
<span id="cb3-106"><a href="#cb3-106"></a>        <span class="co"># Add class token</span></span>
<span id="cb3-107"><a href="#cb3-107"></a>        cls_tokens <span class="op">=</span> <span class="va">self</span>.cls_token.expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-108"><a href="#cb3-108"></a>        x <span class="op">=</span> torch.cat((cls_tokens, x), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-109"><a href="#cb3-109"></a>        </span>
<span id="cb3-110"><a href="#cb3-110"></a>        <span class="co"># Add position embedding</span></span>
<span id="cb3-111"><a href="#cb3-111"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed</span>
<span id="cb3-112"><a href="#cb3-112"></a>        </span>
<span id="cb3-113"><a href="#cb3-113"></a>        <span class="co"># Apply transformer blocks</span></span>
<span id="cb3-114"><a href="#cb3-114"></a>        attention_maps <span class="op">=</span> []</span>
<span id="cb3-115"><a href="#cb3-115"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.blocks:</span>
<span id="cb3-116"><a href="#cb3-116"></a>            x, attn <span class="op">=</span> block(x)</span>
<span id="cb3-117"><a href="#cb3-117"></a>            attention_maps.append(attn)</span>
<span id="cb3-118"><a href="#cb3-118"></a>        </span>
<span id="cb3-119"><a href="#cb3-119"></a>        <span class="co"># Classification</span></span>
<span id="cb3-120"><a href="#cb3-120"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb3-121"><a href="#cb3-121"></a>        cls_token_final <span class="op">=</span> x[:, <span class="dv">0</span>]  <span class="co"># Use class token for classification</span></span>
<span id="cb3-122"><a href="#cb3-122"></a>        out <span class="op">=</span> <span class="va">self</span>.head(cls_token_final)</span>
<span id="cb3-123"><a href="#cb3-123"></a>        </span>
<span id="cb3-124"><a href="#cb3-124"></a>        <span class="cf">return</span> out, attention_maps</span>
<span id="cb3-125"><a href="#cb3-125"></a></span>
<span id="cb3-126"><a href="#cb3-126"></a><span class="co"># Create a simple ViT</span></span>
<span id="cb3-127"><a href="#cb3-127"></a>simple_vit <span class="op">=</span> SimpleViT(depth<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">8</span>)  <span class="co"># Smaller for demo</span></span>
<span id="cb3-128"><a href="#cb3-128"></a>vit_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> simple_vit.parameters())</span>
<span id="cb3-129"><a href="#cb3-129"></a></span>
<span id="cb3-130"><a href="#cb3-130"></a><span class="bu">print</span>(<span class="ss">f"Simple ViT parameters: </span><span class="sc">{</span>vit_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb3-131"><a href="#cb3-131"></a></span>
<span id="cb3-132"><a href="#cb3-132"></a><span class="co"># Test with dummy input</span></span>
<span id="cb3-133"><a href="#cb3-133"></a>dummy_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb3-134"><a href="#cb3-134"></a>output, attention_maps <span class="op">=</span> simple_vit(dummy_input)</span>
<span id="cb3-135"><a href="#cb3-135"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-136"><a href="#cb3-136"></a><span class="bu">print</span>(<span class="ss">f"Number of attention maps: </span><span class="sc">{</span><span class="bu">len</span>(attention_maps)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="visualizing-attention-what-does-the-model-look-at" class="level3" data-number="0.3.2">
<h3 data-number="0.3.2" class="anchored" data-anchor-id="visualizing-attention-what-does-the-model-look-at"><span class="header-section-number">0.3.2</span> Visualizing Attention: What Does the Model Look At?</h3>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">def</span> visualize_attention(image, attention_maps, patch_size<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="co">"""Visualize what the vision transformer is looking at"""</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>    </span>
<span id="cb4-4"><a href="#cb4-4"></a>    <span class="co"># Use attention from the last layer, first head</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>    attn <span class="op">=</span> attention_maps[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>, <span class="dv">0</span>]  <span class="co"># (seq_len, seq_len)</span></span>
<span id="cb4-6"><a href="#cb4-6"></a>    </span>
<span id="cb4-7"><a href="#cb4-7"></a>    <span class="co"># Get attention from class token to all patches</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>    cls_attn <span class="op">=</span> attn[<span class="dv">0</span>, <span class="dv">1</span>:]  <span class="co"># Exclude class token to class token attention</span></span>
<span id="cb4-9"><a href="#cb4-9"></a>    </span>
<span id="cb4-10"><a href="#cb4-10"></a>    <span class="co"># Reshape to spatial dimensions</span></span>
<span id="cb4-11"><a href="#cb4-11"></a>    num_patches_per_side <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(cls_attn) <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb4-12"><a href="#cb4-12"></a>    attn_map <span class="op">=</span> cls_attn.reshape(num_patches_per_side, num_patches_per_side)</span>
<span id="cb4-13"><a href="#cb4-13"></a>    </span>
<span id="cb4-14"><a href="#cb4-14"></a>    <span class="co"># Resize to image size</span></span>
<span id="cb4-15"><a href="#cb4-15"></a>    attn_map <span class="op">=</span> torch.nn.functional.interpolate(</span>
<span id="cb4-16"><a href="#cb4-16"></a>        attn_map.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb4-17"><a href="#cb4-17"></a>        size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb4-18"><a href="#cb4-18"></a>        mode<span class="op">=</span><span class="st">'bilinear'</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>    ).squeeze()</span>
<span id="cb4-20"><a href="#cb4-20"></a>    </span>
<span id="cb4-21"><a href="#cb4-21"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb4-22"><a href="#cb4-22"></a>    </span>
<span id="cb4-23"><a href="#cb4-23"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb4-24"><a href="#cb4-24"></a>    plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb4-25"><a href="#cb4-25"></a>    plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb4-26"><a href="#cb4-26"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-27"><a href="#cb4-27"></a>    </span>
<span id="cb4-28"><a href="#cb4-28"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb4-29"><a href="#cb4-29"></a>    plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb4-30"><a href="#cb4-30"></a>    plt.imshow(attn_map.detach().numpy(), alpha<span class="op">=</span><span class="fl">0.6</span>, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb4-31"><a href="#cb4-31"></a>    plt.title(<span class="st">"Attention Map"</span>)</span>
<span id="cb4-32"><a href="#cb4-32"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-33"><a href="#cb4-33"></a>    </span>
<span id="cb4-34"><a href="#cb4-34"></a>    plt.tight_layout()</span>
<span id="cb4-35"><a href="#cb4-35"></a>    plt.show()</span>
<span id="cb4-36"><a href="#cb4-36"></a></span>
<span id="cb4-37"><a href="#cb4-37"></a><span class="co"># Visualize attention (you would use a real image)</span></span>
<span id="cb4-38"><a href="#cb4-38"></a>dummy_image <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb4-39"><a href="#cb4-39"></a>visualize_attention(dummy_image, attention_maps)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="foundation-models-the-dinov2-revolution" class="level2" data-number="0.4">
<h2 data-number="0.4" class="anchored" data-anchor-id="foundation-models-the-dinov2-revolution"><span class="header-section-number">0.4</span> Foundation Models: The DINOv2 Revolution</h2>
<p>Now we reach the cutting edge: <strong>Foundation Models</strong>. DINOv2 (Distillation with No Labels v2) represents a paradigm shift:</p>
<ul>
<li><strong>Self-supervised learning</strong>: No labels needed!</li>
<li><strong>Universal features</strong>: Works for any vision task</li>
<li><strong>Incredible performance</strong>: Often beats supervised methods</li>
</ul>
<section id="using-dinov2-with-huggingface" class="level3" data-number="0.4.1">
<h3 data-number="0.4.1" class="anchored" data-anchor-id="using-dinov2-with-huggingface"><span class="header-section-number">0.4.1</span> Using DINOv2 with HuggingFace</h3>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># Install required packages</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co"># !pip install transformers torch torchvision</span></span>
<span id="cb5-3"><a href="#cb5-3"></a></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="im">from</span> transformers <span class="im">import</span> AutoImageProcessor, AutoModel</span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="im">import</span> requests</span>
<span id="cb5-7"><a href="#cb5-7"></a></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co"># Load DINOv2 model and processor</span></span>
<span id="cb5-9"><a href="#cb5-9"></a>model_name <span class="op">=</span> <span class="st">"facebook/dinov2-base"</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>processor <span class="op">=</span> AutoImageProcessor.from_pretrained(model_name)</span>
<span id="cb5-11"><a href="#cb5-11"></a>model <span class="op">=</span> AutoModel.from_pretrained(model_name)</span>
<span id="cb5-12"><a href="#cb5-12"></a></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="bu">print</span>(<span class="ss">f"Loaded DINOv2 model: </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="bu">print</span>(<span class="ss">f"Model parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="kw">def</span> extract_dinov2_features(image_path_or_url):</span>
<span id="cb5-17"><a href="#cb5-17"></a>    <span class="co">"""Extract features using DINOv2"""</span></span>
<span id="cb5-18"><a href="#cb5-18"></a>    </span>
<span id="cb5-19"><a href="#cb5-19"></a>    <span class="co"># Load image</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>    <span class="cf">if</span> image_path_or_url.startswith(<span class="st">'http'</span>):</span>
<span id="cb5-21"><a href="#cb5-21"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(image_path_or_url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb5-22"><a href="#cb5-22"></a>    <span class="cf">else</span>:</span>
<span id="cb5-23"><a href="#cb5-23"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(image_path_or_url)</span>
<span id="cb5-24"><a href="#cb5-24"></a>    </span>
<span id="cb5-25"><a href="#cb5-25"></a>    <span class="co"># Process image</span></span>
<span id="cb5-26"><a href="#cb5-26"></a>    inputs <span class="op">=</span> processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb5-27"><a href="#cb5-27"></a>    </span>
<span id="cb5-28"><a href="#cb5-28"></a>    <span class="co"># Extract features</span></span>
<span id="cb5-29"><a href="#cb5-29"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-30"><a href="#cb5-30"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb5-31"><a href="#cb5-31"></a>        features <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb5-32"><a href="#cb5-32"></a>        </span>
<span id="cb5-33"><a href="#cb5-33"></a>        <span class="co"># Get CLS token (global image representation)</span></span>
<span id="cb5-34"><a href="#cb5-34"></a>        cls_features <span class="op">=</span> features[:, <span class="dv">0</span>]  <span class="co"># Shape: (1, 768)</span></span>
<span id="cb5-35"><a href="#cb5-35"></a>        </span>
<span id="cb5-36"><a href="#cb5-36"></a>        <span class="co"># Get patch features (local representations)</span></span>
<span id="cb5-37"><a href="#cb5-37"></a>        patch_features <span class="op">=</span> features[:, <span class="dv">1</span>:]  <span class="co"># Shape: (1, num_patches, 768)</span></span>
<span id="cb5-38"><a href="#cb5-38"></a>    </span>
<span id="cb5-39"><a href="#cb5-39"></a>    <span class="cf">return</span> {</span>
<span id="cb5-40"><a href="#cb5-40"></a>        <span class="st">'cls_features'</span>: cls_features,</span>
<span id="cb5-41"><a href="#cb5-41"></a>        <span class="st">'patch_features'</span>: patch_features,</span>
<span id="cb5-42"><a href="#cb5-42"></a>        <span class="st">'image'</span>: image</span>
<span id="cb5-43"><a href="#cb5-43"></a>    }</span>
<span id="cb5-44"><a href="#cb5-44"></a></span>
<span id="cb5-45"><a href="#cb5-45"></a><span class="co"># Example usage</span></span>
<span id="cb5-46"><a href="#cb5-46"></a><span class="kw">def</span> demo_dinov2_features():</span>
<span id="cb5-47"><a href="#cb5-47"></a>    <span class="co">"""Demonstrate DINOv2 feature extraction"""</span></span>
<span id="cb5-48"><a href="#cb5-48"></a>    </span>
<span id="cb5-49"><a href="#cb5-49"></a>    <span class="co"># Create dummy image for demo (you would use real images)</span></span>
<span id="cb5-50"><a href="#cb5-50"></a>    dummy_image <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb5-51"><a href="#cb5-51"></a>    </span>
<span id="cb5-52"><a href="#cb5-52"></a>    <span class="co"># Save temporarily</span></span>
<span id="cb5-53"><a href="#cb5-53"></a>    dummy_image.save(<span class="st">'temp_image.jpg'</span>)</span>
<span id="cb5-54"><a href="#cb5-54"></a>    </span>
<span id="cb5-55"><a href="#cb5-55"></a>    <span class="co"># Extract features</span></span>
<span id="cb5-56"><a href="#cb5-56"></a>    result <span class="op">=</span> extract_dinov2_features(<span class="st">'temp_image.jpg'</span>)</span>
<span id="cb5-57"><a href="#cb5-57"></a>    </span>
<span id="cb5-58"><a href="#cb5-58"></a>    <span class="bu">print</span>(<span class="st">"DINOv2 Feature Extraction Results:"</span>)</span>
<span id="cb5-59"><a href="#cb5-59"></a>    <span class="bu">print</span>(<span class="ss">f"Global features shape: </span><span class="sc">{</span>result[<span class="st">'cls_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-60"><a href="#cb5-60"></a>    <span class="bu">print</span>(<span class="ss">f"Patch features shape: </span><span class="sc">{</span>result[<span class="st">'patch_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-61"><a href="#cb5-61"></a>    </span>
<span id="cb5-62"><a href="#cb5-62"></a>    <span class="co"># Visualize features</span></span>
<span id="cb5-63"><a href="#cb5-63"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb5-64"><a href="#cb5-64"></a>    </span>
<span id="cb5-65"><a href="#cb5-65"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb5-66"><a href="#cb5-66"></a>    plt.imshow(result[<span class="st">'image'</span>])</span>
<span id="cb5-67"><a href="#cb5-67"></a>    plt.title(<span class="st">"Input Image"</span>)</span>
<span id="cb5-68"><a href="#cb5-68"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb5-69"><a href="#cb5-69"></a>    </span>
<span id="cb5-70"><a href="#cb5-70"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb5-71"><a href="#cb5-71"></a>    plt.plot(result[<span class="st">'cls_features'</span>].squeeze().numpy())</span>
<span id="cb5-72"><a href="#cb5-72"></a>    plt.title(<span class="st">"Global Features (768 dimensions)"</span>)</span>
<span id="cb5-73"><a href="#cb5-73"></a>    plt.xlabel(<span class="st">"Dimension"</span>)</span>
<span id="cb5-74"><a href="#cb5-74"></a>    plt.ylabel(<span class="st">"Value"</span>)</span>
<span id="cb5-75"><a href="#cb5-75"></a>    </span>
<span id="cb5-76"><a href="#cb5-76"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb5-77"><a href="#cb5-77"></a>    <span class="co"># Visualize patch features as heatmap</span></span>
<span id="cb5-78"><a href="#cb5-78"></a>    patch_norms <span class="op">=</span> torch.norm(result[<span class="st">'patch_features'</span>].squeeze(), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-79"><a href="#cb5-79"></a>    patch_size <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(patch_norms) <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb5-80"><a href="#cb5-80"></a>    patch_map <span class="op">=</span> patch_norms.reshape(patch_size, patch_size)</span>
<span id="cb5-81"><a href="#cb5-81"></a>    plt.imshow(patch_map.numpy(), cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb5-82"><a href="#cb5-82"></a>    plt.title(<span class="st">"Patch Feature Magnitudes"</span>)</span>
<span id="cb5-83"><a href="#cb5-83"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb5-84"><a href="#cb5-84"></a>    </span>
<span id="cb5-85"><a href="#cb5-85"></a>    plt.tight_layout()</span>
<span id="cb5-86"><a href="#cb5-86"></a>    plt.show()</span>
<span id="cb5-87"><a href="#cb5-87"></a>    </span>
<span id="cb5-88"><a href="#cb5-88"></a>    <span class="cf">return</span> result</span>
<span id="cb5-89"><a href="#cb5-89"></a></span>
<span id="cb5-90"><a href="#cb5-90"></a>demo_result <span class="op">=</span> demo_dinov2_features()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="building-a-dinov2-powered-image-similarity-engine" class="level3" data-number="0.4.2">
<h3 data-number="0.4.2" class="anchored" data-anchor-id="building-a-dinov2-powered-image-similarity-engine"><span class="header-section-number">0.4.2</span> Building a DINOv2-Powered Image Similarity Engine</h3>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">class</span> DINOv2SimilarityEngine:</span>
<span id="cb6-2"><a href="#cb6-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb6-3"><a href="#cb6-3"></a>        <span class="va">self</span>.processor <span class="op">=</span> AutoImageProcessor.from_pretrained(<span class="st">"facebook/dinov2-base"</span>)</span>
<span id="cb6-4"><a href="#cb6-4"></a>        <span class="va">self</span>.model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">"facebook/dinov2-base"</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb6-6"><a href="#cb6-6"></a>        <span class="va">self</span>.image_database <span class="op">=</span> {}</span>
<span id="cb6-7"><a href="#cb6-7"></a>    </span>
<span id="cb6-8"><a href="#cb6-8"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, image):</span>
<span id="cb6-9"><a href="#cb6-9"></a>        <span class="co">"""Extract DINOv2 features from an image"""</span></span>
<span id="cb6-10"><a href="#cb6-10"></a>        inputs <span class="op">=</span> <span class="va">self</span>.processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb6-11"><a href="#cb6-11"></a>        </span>
<span id="cb6-12"><a href="#cb6-12"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-13"><a href="#cb6-13"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(<span class="op">**</span>inputs)</span>
<span id="cb6-14"><a href="#cb6-14"></a>            <span class="co"># Use CLS token as global image representation</span></span>
<span id="cb6-15"><a href="#cb6-15"></a>            features <span class="op">=</span> outputs.last_hidden_state[:, <span class="dv">0</span>]</span>
<span id="cb6-16"><a href="#cb6-16"></a>        </span>
<span id="cb6-17"><a href="#cb6-17"></a>        <span class="cf">return</span> features</span>
<span id="cb6-18"><a href="#cb6-18"></a>    </span>
<span id="cb6-19"><a href="#cb6-19"></a>    <span class="kw">def</span> add_image(<span class="va">self</span>, image_id, image):</span>
<span id="cb6-20"><a href="#cb6-20"></a>        <span class="co">"""Add an image to the database"""</span></span>
<span id="cb6-21"><a href="#cb6-21"></a>        features <span class="op">=</span> <span class="va">self</span>.extract_features(image)</span>
<span id="cb6-22"><a href="#cb6-22"></a>        <span class="va">self</span>.image_database[image_id] <span class="op">=</span> {</span>
<span id="cb6-23"><a href="#cb6-23"></a>            <span class="st">'features'</span>: features,</span>
<span id="cb6-24"><a href="#cb6-24"></a>            <span class="st">'image'</span>: image</span>
<span id="cb6-25"><a href="#cb6-25"></a>        }</span>
<span id="cb6-26"><a href="#cb6-26"></a>        <span class="bu">print</span>(<span class="ss">f"Added image '</span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ss">' to database"</span>)</span>
<span id="cb6-27"><a href="#cb6-27"></a>    </span>
<span id="cb6-28"><a href="#cb6-28"></a>    <span class="kw">def</span> find_similar_images(<span class="va">self</span>, query_image, top_k<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb6-29"><a href="#cb6-29"></a>        <span class="co">"""Find most similar images in the database"""</span></span>
<span id="cb6-30"><a href="#cb6-30"></a>        query_features <span class="op">=</span> <span class="va">self</span>.extract_features(query_image)</span>
<span id="cb6-31"><a href="#cb6-31"></a>        </span>
<span id="cb6-32"><a href="#cb6-32"></a>        similarities <span class="op">=</span> {}</span>
<span id="cb6-33"><a href="#cb6-33"></a>        </span>
<span id="cb6-34"><a href="#cb6-34"></a>        <span class="cf">for</span> image_id, data <span class="kw">in</span> <span class="va">self</span>.image_database.items():</span>
<span id="cb6-35"><a href="#cb6-35"></a>            <span class="co"># Compute cosine similarity</span></span>
<span id="cb6-36"><a href="#cb6-36"></a>            similarity <span class="op">=</span> torch.cosine_similarity(</span>
<span id="cb6-37"><a href="#cb6-37"></a>                query_features, data[<span class="st">'features'</span>], dim<span class="op">=</span><span class="dv">1</span></span>
<span id="cb6-38"><a href="#cb6-38"></a>            ).item()</span>
<span id="cb6-39"><a href="#cb6-39"></a>            similarities[image_id] <span class="op">=</span> similarity</span>
<span id="cb6-40"><a href="#cb6-40"></a>        </span>
<span id="cb6-41"><a href="#cb6-41"></a>        <span class="co"># Sort by similarity</span></span>
<span id="cb6-42"><a href="#cb6-42"></a>        sorted_similarities <span class="op">=</span> <span class="bu">sorted</span>(</span>
<span id="cb6-43"><a href="#cb6-43"></a>            similarities.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-44"><a href="#cb6-44"></a>        )</span>
<span id="cb6-45"><a href="#cb6-45"></a>        </span>
<span id="cb6-46"><a href="#cb6-46"></a>        <span class="cf">return</span> sorted_similarities[:top_k]</span>
<span id="cb6-47"><a href="#cb6-47"></a>    </span>
<span id="cb6-48"><a href="#cb6-48"></a>    <span class="kw">def</span> visualize_results(<span class="va">self</span>, query_image, similar_images):</span>
<span id="cb6-49"><a href="#cb6-49"></a>        <span class="co">"""Visualize similarity search results"""</span></span>
<span id="cb6-50"><a href="#cb6-50"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">8</span>))</span>
<span id="cb6-51"><a href="#cb6-51"></a>        </span>
<span id="cb6-52"><a href="#cb6-52"></a>        <span class="co"># Query image</span></span>
<span id="cb6-53"><a href="#cb6-53"></a>        plt.subplot(<span class="dv">2</span>, <span class="bu">len</span>(similar_images) <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb6-54"><a href="#cb6-54"></a>        plt.imshow(query_image)</span>
<span id="cb6-55"><a href="#cb6-55"></a>        plt.title(<span class="st">"Query Image"</span>)</span>
<span id="cb6-56"><a href="#cb6-56"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb6-57"><a href="#cb6-57"></a>        </span>
<span id="cb6-58"><a href="#cb6-58"></a>        <span class="co"># Similar images</span></span>
<span id="cb6-59"><a href="#cb6-59"></a>        <span class="cf">for</span> i, (image_id, similarity) <span class="kw">in</span> <span class="bu">enumerate</span>(similar_images):</span>
<span id="cb6-60"><a href="#cb6-60"></a>            plt.subplot(<span class="dv">2</span>, <span class="bu">len</span>(similar_images) <span class="op">+</span> <span class="dv">1</span>, i <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb6-61"><a href="#cb6-61"></a>            plt.imshow(<span class="va">self</span>.image_database[image_id][<span class="st">'image'</span>])</span>
<span id="cb6-62"><a href="#cb6-62"></a>            plt.title(<span class="ss">f"</span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ch">\n</span><span class="ss">Similarity: </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb6-63"><a href="#cb6-63"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb6-64"><a href="#cb6-64"></a>        </span>
<span id="cb6-65"><a href="#cb6-65"></a>        plt.tight_layout()</span>
<span id="cb6-66"><a href="#cb6-66"></a>        plt.show()</span>
<span id="cb6-67"><a href="#cb6-67"></a></span>
<span id="cb6-68"><a href="#cb6-68"></a><span class="co"># Create similarity engine</span></span>
<span id="cb6-69"><a href="#cb6-69"></a>similarity_engine <span class="op">=</span> DINOv2SimilarityEngine()</span>
<span id="cb6-70"><a href="#cb6-70"></a></span>
<span id="cb6-71"><a href="#cb6-71"></a><span class="co"># Demo with dummy images (you would use real images)</span></span>
<span id="cb6-72"><a href="#cb6-72"></a><span class="kw">def</span> demo_similarity_engine():</span>
<span id="cb6-73"><a href="#cb6-73"></a>    <span class="co">"""Demonstrate the similarity engine"""</span></span>
<span id="cb6-74"><a href="#cb6-74"></a>    </span>
<span id="cb6-75"><a href="#cb6-75"></a>    <span class="co"># Create some dummy images with different colors</span></span>
<span id="cb6-76"><a href="#cb6-76"></a>    colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'yellow'</span>, <span class="st">'purple'</span>]</span>
<span id="cb6-77"><a href="#cb6-77"></a>    </span>
<span id="cb6-78"><a href="#cb6-78"></a>    <span class="cf">for</span> color <span class="kw">in</span> colors:</span>
<span id="cb6-79"><a href="#cb6-79"></a>        dummy_img <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span>color)</span>
<span id="cb6-80"><a href="#cb6-80"></a>        similarity_engine.add_image(<span class="ss">f"</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss">_image"</span>, dummy_img)</span>
<span id="cb6-81"><a href="#cb6-81"></a>    </span>
<span id="cb6-82"><a href="#cb6-82"></a>    <span class="co"># Query with a red image</span></span>
<span id="cb6-83"><a href="#cb6-83"></a>    query_img <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb6-84"><a href="#cb6-84"></a>    </span>
<span id="cb6-85"><a href="#cb6-85"></a>    <span class="co"># Find similar images</span></span>
<span id="cb6-86"><a href="#cb6-86"></a>    similar <span class="op">=</span> similarity_engine.find_similar_images(query_img, top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb6-87"><a href="#cb6-87"></a>    </span>
<span id="cb6-88"><a href="#cb6-88"></a>    <span class="bu">print</span>(<span class="st">"Most similar images:"</span>)</span>
<span id="cb6-89"><a href="#cb6-89"></a>    <span class="cf">for</span> image_id, similarity <span class="kw">in</span> similar:</span>
<span id="cb6-90"><a href="#cb6-90"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb6-91"><a href="#cb6-91"></a>    </span>
<span id="cb6-92"><a href="#cb6-92"></a>    <span class="co"># Visualize results</span></span>
<span id="cb6-93"><a href="#cb6-93"></a>    similarity_engine.visualize_results(query_img, similar)</span>
<span id="cb6-94"><a href="#cb6-94"></a></span>
<span id="cb6-95"><a href="#cb6-95"></a>demo_similarity_engine()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="comparing-all-approaches-the-ultimate-showdown" class="level2" data-number="0.5">
<h2 data-number="0.5" class="anchored" data-anchor-id="comparing-all-approaches-the-ultimate-showdown"><span class="header-section-number">0.5</span> Comparing All Approaches: The Ultimate Showdown</h2>
<p>Let’s compare all the approaches we’ve learned:</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">class</span> VisionModelComparison:</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb7-3"><a href="#cb7-3"></a>        <span class="va">self</span>.models <span class="op">=</span> {</span>
<span id="cb7-4"><a href="#cb7-4"></a>            <span class="st">'ResNet-18'</span>: models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb7-5"><a href="#cb7-5"></a>            <span class="st">'ResNet-50'</span>: models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb7-6"><a href="#cb7-6"></a>            <span class="st">'EfficientNet-B0'</span>: efficientnet_b0(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb7-7"><a href="#cb7-7"></a>            <span class="st">'ViT-Base'</span>: <span class="va">None</span>,  <span class="co"># Would load from transformers</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>            <span class="st">'DINOv2-Base'</span>: <span class="va">None</span>  <span class="co"># Already loaded above</span></span>
<span id="cb7-9"><a href="#cb7-9"></a>        }</span>
<span id="cb7-10"><a href="#cb7-10"></a>    </span>
<span id="cb7-11"><a href="#cb7-11"></a>    <span class="kw">def</span> compare_models(<span class="va">self</span>):</span>
<span id="cb7-12"><a href="#cb7-12"></a>        <span class="co">"""Compare different vision models"""</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>        comparison_data <span class="op">=</span> []</span>
<span id="cb7-14"><a href="#cb7-14"></a>        </span>
<span id="cb7-15"><a href="#cb7-15"></a>        <span class="cf">for</span> name, model <span class="kw">in</span> <span class="va">self</span>.models.items():</span>
<span id="cb7-16"><a href="#cb7-16"></a>            <span class="cf">if</span> model <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-17"><a href="#cb7-17"></a>                params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb7-18"><a href="#cb7-18"></a>                size_mb <span class="op">=</span> params <span class="op">*</span> <span class="dv">4</span> <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)</span>
<span id="cb7-19"><a href="#cb7-19"></a>                </span>
<span id="cb7-20"><a href="#cb7-20"></a>                comparison_data.append({</span>
<span id="cb7-21"><a href="#cb7-21"></a>                    <span class="st">'Model'</span>: name,</span>
<span id="cb7-22"><a href="#cb7-22"></a>                    <span class="st">'Parameters (M)'</span>: <span class="ss">f"</span><span class="sc">{</span>params <span class="op">/</span> <span class="fl">1e6</span><span class="sc">:.1f}</span><span class="ss">"</span>,</span>
<span id="cb7-23"><a href="#cb7-23"></a>                    <span class="st">'Size (MB)'</span>: <span class="ss">f"</span><span class="sc">{</span>size_mb<span class="sc">:.1f}</span><span class="ss">"</span>,</span>
<span id="cb7-24"><a href="#cb7-24"></a>                    <span class="st">'Year'</span>: <span class="va">self</span>.get_year(name),</span>
<span id="cb7-25"><a href="#cb7-25"></a>                    <span class="st">'Type'</span>: <span class="va">self</span>.get_type(name)</span>
<span id="cb7-26"><a href="#cb7-26"></a>                })</span>
<span id="cb7-27"><a href="#cb7-27"></a>        </span>
<span id="cb7-28"><a href="#cb7-28"></a>        <span class="cf">return</span> comparison_data</span>
<span id="cb7-29"><a href="#cb7-29"></a>    </span>
<span id="cb7-30"><a href="#cb7-30"></a>    <span class="kw">def</span> get_year(<span class="va">self</span>, name):</span>
<span id="cb7-31"><a href="#cb7-31"></a>        year_map <span class="op">=</span> {</span>
<span id="cb7-32"><a href="#cb7-32"></a>            <span class="st">'ResNet-18'</span>: <span class="dv">2015</span>,</span>
<span id="cb7-33"><a href="#cb7-33"></a>            <span class="st">'ResNet-50'</span>: <span class="dv">2015</span>,</span>
<span id="cb7-34"><a href="#cb7-34"></a>            <span class="st">'EfficientNet-B0'</span>: <span class="dv">2019</span>,</span>
<span id="cb7-35"><a href="#cb7-35"></a>            <span class="st">'ViT-Base'</span>: <span class="dv">2020</span>,</span>
<span id="cb7-36"><a href="#cb7-36"></a>            <span class="st">'DINOv2-Base'</span>: <span class="dv">2023</span></span>
<span id="cb7-37"><a href="#cb7-37"></a>        }</span>
<span id="cb7-38"><a href="#cb7-38"></a>        <span class="cf">return</span> year_map.get(name, <span class="st">'Unknown'</span>)</span>
<span id="cb7-39"><a href="#cb7-39"></a>    </span>
<span id="cb7-40"><a href="#cb7-40"></a>    <span class="kw">def</span> get_type(<span class="va">self</span>, name):</span>
<span id="cb7-41"><a href="#cb7-41"></a>        <span class="cf">if</span> <span class="st">'ResNet'</span> <span class="kw">in</span> name <span class="kw">or</span> <span class="st">'EfficientNet'</span> <span class="kw">in</span> name:</span>
<span id="cb7-42"><a href="#cb7-42"></a>            <span class="cf">return</span> <span class="st">'CNN'</span></span>
<span id="cb7-43"><a href="#cb7-43"></a>        <span class="cf">elif</span> <span class="st">'ViT'</span> <span class="kw">in</span> name:</span>
<span id="cb7-44"><a href="#cb7-44"></a>            <span class="cf">return</span> <span class="st">'Transformer'</span></span>
<span id="cb7-45"><a href="#cb7-45"></a>        <span class="cf">elif</span> <span class="st">'DINOv2'</span> <span class="kw">in</span> name:</span>
<span id="cb7-46"><a href="#cb7-46"></a>            <span class="cf">return</span> <span class="st">'Foundation Model'</span></span>
<span id="cb7-47"><a href="#cb7-47"></a>        <span class="cf">return</span> <span class="st">'Unknown'</span></span>
<span id="cb7-48"><a href="#cb7-48"></a>    </span>
<span id="cb7-49"><a href="#cb7-49"></a>    <span class="kw">def</span> visualize_comparison(<span class="va">self</span>, data):</span>
<span id="cb7-50"><a href="#cb7-50"></a>        <span class="co">"""Visualize model comparison"""</span></span>
<span id="cb7-51"><a href="#cb7-51"></a>        <span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-52"><a href="#cb7-52"></a>        </span>
<span id="cb7-53"><a href="#cb7-53"></a>        df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb7-54"><a href="#cb7-54"></a>        </span>
<span id="cb7-55"><a href="#cb7-55"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb7-56"><a href="#cb7-56"></a>        </span>
<span id="cb7-57"><a href="#cb7-57"></a>        <span class="co"># Parameters vs Year</span></span>
<span id="cb7-58"><a href="#cb7-58"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb7-59"><a href="#cb7-59"></a>        <span class="cf">for</span> model_type <span class="kw">in</span> df[<span class="st">'Type'</span>].unique():</span>
<span id="cb7-60"><a href="#cb7-60"></a>            subset <span class="op">=</span> df[df[<span class="st">'Type'</span>] <span class="op">==</span> model_type]</span>
<span id="cb7-61"><a href="#cb7-61"></a>            plt.scatter(subset[<span class="st">'Year'</span>], subset[<span class="st">'Parameters (M)'</span>].astype(<span class="bu">float</span>), </span>
<span id="cb7-62"><a href="#cb7-62"></a>                       label<span class="op">=</span>model_type, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb7-63"><a href="#cb7-63"></a>        </span>
<span id="cb7-64"><a href="#cb7-64"></a>        plt.xlabel(<span class="st">'Year'</span>)</span>
<span id="cb7-65"><a href="#cb7-65"></a>        plt.ylabel(<span class="st">'Parameters (Millions)'</span>)</span>
<span id="cb7-66"><a href="#cb7-66"></a>        plt.title(<span class="st">'Model Size Evolution'</span>)</span>
<span id="cb7-67"><a href="#cb7-67"></a>        plt.legend()</span>
<span id="cb7-68"><a href="#cb7-68"></a>        plt.grid(<span class="va">True</span>)</span>
<span id="cb7-69"><a href="#cb7-69"></a>        </span>
<span id="cb7-70"><a href="#cb7-70"></a>        <span class="co"># Model types distribution</span></span>
<span id="cb7-71"><a href="#cb7-71"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb7-72"><a href="#cb7-72"></a>        type_counts <span class="op">=</span> df[<span class="st">'Type'</span>].value_counts()</span>
<span id="cb7-73"><a href="#cb7-73"></a>        plt.pie(type_counts.values, labels<span class="op">=</span>type_counts.index, autopct<span class="op">=</span><span class="st">'</span><span class="sc">%1.1f%%</span><span class="st">'</span>)</span>
<span id="cb7-74"><a href="#cb7-74"></a>        plt.title(<span class="st">'Model Types Distribution'</span>)</span>
<span id="cb7-75"><a href="#cb7-75"></a>        </span>
<span id="cb7-76"><a href="#cb7-76"></a>        <span class="co"># Size comparison</span></span>
<span id="cb7-77"><a href="#cb7-77"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb7-78"><a href="#cb7-78"></a>        plt.bar(df[<span class="st">'Model'</span>], df[<span class="st">'Size (MB)'</span>].astype(<span class="bu">float</span>))</span>
<span id="cb7-79"><a href="#cb7-79"></a>        plt.xticks(rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb7-80"><a href="#cb7-80"></a>        plt.ylabel(<span class="st">'Size (MB)'</span>)</span>
<span id="cb7-81"><a href="#cb7-81"></a>        plt.title(<span class="st">'Model Size Comparison'</span>)</span>
<span id="cb7-82"><a href="#cb7-82"></a>        </span>
<span id="cb7-83"><a href="#cb7-83"></a>        <span class="co"># Timeline</span></span>
<span id="cb7-84"><a href="#cb7-84"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb7-85"><a href="#cb7-85"></a>        timeline_data <span class="op">=</span> df.sort_values(<span class="st">'Year'</span>)</span>
<span id="cb7-86"><a href="#cb7-86"></a>        plt.plot(timeline_data[<span class="st">'Year'</span>], <span class="bu">range</span>(<span class="bu">len</span>(timeline_data)), <span class="st">'o-'</span>)</span>
<span id="cb7-87"><a href="#cb7-87"></a>        <span class="cf">for</span> i, (idx, row) <span class="kw">in</span> <span class="bu">enumerate</span>(timeline_data.iterrows()):</span>
<span id="cb7-88"><a href="#cb7-88"></a>            plt.annotate(row[<span class="st">'Model'</span>], (row[<span class="st">'Year'</span>], i), </span>
<span id="cb7-89"><a href="#cb7-89"></a>                        xytext<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">0</span>), textcoords<span class="op">=</span><span class="st">'offset points'</span>)</span>
<span id="cb7-90"><a href="#cb7-90"></a>        plt.xlabel(<span class="st">'Year'</span>)</span>
<span id="cb7-91"><a href="#cb7-91"></a>        plt.ylabel(<span class="st">'Model Index'</span>)</span>
<span id="cb7-92"><a href="#cb7-92"></a>        plt.title(<span class="st">'Vision Models Timeline'</span>)</span>
<span id="cb7-93"><a href="#cb7-93"></a>        plt.grid(<span class="va">True</span>)</span>
<span id="cb7-94"><a href="#cb7-94"></a>        </span>
<span id="cb7-95"><a href="#cb7-95"></a>        plt.tight_layout()</span>
<span id="cb7-96"><a href="#cb7-96"></a>        plt.show()</span>
<span id="cb7-97"><a href="#cb7-97"></a></span>
<span id="cb7-98"><a href="#cb7-98"></a><span class="co"># Run comparison</span></span>
<span id="cb7-99"><a href="#cb7-99"></a>comparison <span class="op">=</span> VisionModelComparison()</span>
<span id="cb7-100"><a href="#cb7-100"></a>comparison_data <span class="op">=</span> comparison.compare_models()</span>
<span id="cb7-101"><a href="#cb7-101"></a></span>
<span id="cb7-102"><a href="#cb7-102"></a><span class="bu">print</span>(<span class="st">"Vision Models Comparison:"</span>)</span>
<span id="cb7-103"><a href="#cb7-103"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">80</span>)</span>
<span id="cb7-104"><a href="#cb7-104"></a><span class="cf">for</span> data <span class="kw">in</span> comparison_data:</span>
<span id="cb7-105"><a href="#cb7-105"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>data[<span class="st">'Model'</span>]<span class="sc">:15}</span><span class="ss"> | </span><span class="sc">{</span>data[<span class="st">'Year'</span>]<span class="sc">}</span><span class="ss"> | </span><span class="sc">{</span>data[<span class="st">'Type'</span>]<span class="sc">:15}</span><span class="ss"> | "</span></span>
<span id="cb7-106"><a href="#cb7-106"></a>          <span class="ss">f"</span><span class="sc">{</span>data[<span class="st">'Parameters (M)'</span>]<span class="sc">:&gt;8}</span><span class="ss"> M | </span><span class="sc">{</span>data[<span class="st">'Size (MB)'</span>]<span class="sc">:&gt;8}</span><span class="ss"> MB"</span>)</span>
<span id="cb7-107"><a href="#cb7-107"></a></span>
<span id="cb7-108"><a href="#cb7-108"></a>comparison.visualize_comparison(comparison_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="the-future-whats-next" class="level2" data-number="0.6">
<h2 data-number="0.6" class="anchored" data-anchor-id="the-future-whats-next"><span class="header-section-number">0.6</span> The Future: What’s Next?</h2>
<p>As we look ahead, several trends are shaping the future of computer vision:</p>
<section id="multimodal-foundation-models" class="level3" data-number="0.6.1">
<h3 data-number="0.6.1" class="anchored" data-anchor-id="multimodal-foundation-models"><span class="header-section-number">0.6.1</span> 1. <strong>Multimodal Foundation Models</strong></h3>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Future: Models that understand both vision and language</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="co"># Example: CLIP, GPT-4V, LLaVA</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="efficient-architectures" class="level3" data-number="0.6.2">
<h3 data-number="0.6.2" class="anchored" data-anchor-id="efficient-architectures"><span class="header-section-number">0.6.2</span> 2. <strong>Efficient Architectures</strong></h3>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Trend: Smaller, faster models for mobile devices</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="co"># Example: MobileViT, EfficientViT</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="self-supervised-learning" class="level3" data-number="0.6.3">
<h3 data-number="0.6.3" class="anchored" data-anchor-id="self-supervised-learning"><span class="header-section-number">0.6.3</span> 3. <strong>Self-Supervised Learning</strong></h3>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># Growing trend: Learning without labels</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="co"># Example: MAE, SimCLR, DINOv2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="your-challenge-build-a-modern-vision-pipeline" class="level2" data-number="0.7">
<h2 data-number="0.7" class="anchored" data-anchor-id="your-challenge-build-a-modern-vision-pipeline"><span class="header-section-number">0.7</span> Your Challenge: Build a Modern Vision Pipeline</h2>
<p>Now it’s your turn to build a complete modern vision system:</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="kw">class</span> ModernVisionPipeline:</span>
<span id="cb11-2"><a href="#cb11-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb11-3"><a href="#cb11-3"></a>        <span class="co"># Load multiple models for different tasks</span></span>
<span id="cb11-4"><a href="#cb11-4"></a>        <span class="va">self</span>.classification_model <span class="op">=</span> models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-5"><a href="#cb11-5"></a>        <span class="va">self</span>.feature_extractor <span class="op">=</span> <span class="va">None</span>  <span class="co"># DINOv2 model</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>        <span class="va">self</span>.similarity_engine <span class="op">=</span> DINOv2SimilarityEngine()</span>
<span id="cb11-7"><a href="#cb11-7"></a>    </span>
<span id="cb11-8"><a href="#cb11-8"></a>    <span class="kw">def</span> classify_image(<span class="va">self</span>, image):</span>
<span id="cb11-9"><a href="#cb11-9"></a>        <span class="co">"""Classify image using ResNet"""</span></span>
<span id="cb11-10"><a href="#cb11-10"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb11-11"><a href="#cb11-11"></a>        <span class="cf">pass</span></span>
<span id="cb11-12"><a href="#cb11-12"></a>    </span>
<span id="cb11-13"><a href="#cb11-13"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, image):</span>
<span id="cb11-14"><a href="#cb11-14"></a>        <span class="co">"""Extract features using DINOv2"""</span></span>
<span id="cb11-15"><a href="#cb11-15"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb11-16"><a href="#cb11-16"></a>        <span class="cf">pass</span></span>
<span id="cb11-17"><a href="#cb11-17"></a>    </span>
<span id="cb11-18"><a href="#cb11-18"></a>    <span class="kw">def</span> find_similar_images(<span class="va">self</span>, query_image, database):</span>
<span id="cb11-19"><a href="#cb11-19"></a>        <span class="co">"""Find similar images using DINOv2 features"""</span></span>
<span id="cb11-20"><a href="#cb11-20"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb11-21"><a href="#cb11-21"></a>        <span class="cf">pass</span></span>
<span id="cb11-22"><a href="#cb11-22"></a>    </span>
<span id="cb11-23"><a href="#cb11-23"></a>    <span class="kw">def</span> analyze_image(<span class="va">self</span>, image):</span>
<span id="cb11-24"><a href="#cb11-24"></a>        <span class="co">"""Complete image analysis pipeline"""</span></span>
<span id="cb11-25"><a href="#cb11-25"></a>        results <span class="op">=</span> {</span>
<span id="cb11-26"><a href="#cb11-26"></a>            <span class="st">'classification'</span>: <span class="va">self</span>.classify_image(image),</span>
<span id="cb11-27"><a href="#cb11-27"></a>            <span class="st">'features'</span>: <span class="va">self</span>.extract_features(image),</span>
<span id="cb11-28"><a href="#cb11-28"></a>            <span class="st">'similar_images'</span>: <span class="va">self</span>.find_similar_images(image, <span class="va">self</span>.image_database)</span>
<span id="cb11-29"><a href="#cb11-29"></a>        }</span>
<span id="cb11-30"><a href="#cb11-30"></a>        <span class="cf">return</span> results</span>
<span id="cb11-31"><a href="#cb11-31"></a></span>
<span id="cb11-32"><a href="#cb11-32"></a><span class="co"># Build your pipeline!</span></span>
<span id="cb11-33"><a href="#cb11-33"></a>pipeline <span class="op">=</span> ModernVisionPipeline()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="whats-coming-next" class="level2" data-number="0.8">
<h2 data-number="0.8" class="anchored" data-anchor-id="whats-coming-next"><span class="header-section-number">0.8</span> What’s Coming Next?</h2>
<p>In our next post, <a href="../08-first-cv-project/"><strong>“Your First CV Project: Putting It All Together”</strong></a>, we’ll:</p>
<ul>
<li><strong>Build a complete computer vision application</strong></li>
<li><strong>Combine classical and modern techniques</strong></li>
<li><strong>Deploy your model for real-world use</strong></li>
<li><strong>Create an interactive demo</strong></li>
</ul>
<p>You’ve just learned about the most advanced vision models ever created—next, we’ll put everything together into a real project!</p>
</section>
<section id="key-takeaways" class="level2" data-number="0.9">
<h2 data-number="0.9" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">0.9</span> Key Takeaways</h2>
<ul>
<li><strong>CNNs dominated</strong> computer vision for a decade</li>
<li><strong>Vision Transformers</strong> brought attention mechanisms to vision</li>
<li><strong>Foundation models</strong> like DINOv2 learn universal representations</li>
<li><strong>Self-supervised learning</strong> eliminates the need for labels</li>
<li><strong>Modern pipelines</strong> combine multiple approaches</li>
<li><strong>The field evolves rapidly</strong>—stay curious and keep learning!</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hands-On Lab
</div>
</div>
<div class="callout-body-container callout-body">
<p>Ready to experiment with cutting-edge vision models? Try the complete interactive notebook: <a href="https://colab.research.google.com/drive/1Modern_Vision_Models_123456"><strong>Modern Vision Models Lab</strong></a></p>
<p>Compare CNNs, Vision Transformers, and DINOv2 on your own images!</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Series Navigation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Previous</strong>: <a href="../06-why-deep-learning/">Why Deep Learning? When Classical Methods Hit the Wall</a></li>
<li><strong>Next</strong>: <a href="../08-first-cv-project/">Your First CV Project: Putting It All Together</a></li>
<li><strong>Series Home</strong>: <a href="../../../posts/series/computer-vision-foundations.html">Computer Vision Foundations</a></li>
</ul>
</div>
</div>
<hr>
<p><em>You’ve just explored the cutting edge of computer vision! From ResNet’s skip connections to DINOv2’s self-supervised learning—you now understand the models that power today’s AI applications. Next, we’ll build something amazing with all this knowledge!</em></p>


<!-- -->

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div>CC BY-NC-SA 4.0</div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{2025,
  author = {, Hasan},
  title = {Modern {Vision} {Models:} {CNNs,} {Vision} {Transformers,}
    and {DINOv2}},
  date = {2025-01-22},
  url = {https://hasangoni.quarto.pub/hasan-blog-post/posts/series/cv-foundations/07-modern-vision-models.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Hasan. 2025. <span>“Modern Vision Models: CNNs, Vision Transformers, and
DINOv2.”</span> January 22, 2025. <a href="https://hasangoni.quarto.pub/hasan-blog-post/posts/series/cv-foundations/07-modern-vision-models.html">https://hasangoni.quarto.pub/hasan-blog-post/posts/series/cv-foundations/07-modern-vision-models.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="yourusername/quarto_blog_hasan" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<script src="https://giscus.app/client.js" data-repo="HasanGoni/quarto_blog_hasan" data-repo-id="" data-category="General" data-category-id="" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="an">title:</span><span class="co"> "Modern Vision Models: CNNs, Vision Transformers, and DINOv2"</span></span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="an">author:</span><span class="co"> "Hasan"</span></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="an">date:</span><span class="co"> 2025-01-22</span></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="an">categories:</span><span class="co"> [computer-vision, transformers, foundation-models, dinov2]</span></span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="an">tags:</span><span class="co"> [cnn, vision-transformer, dinov2, self-supervised, huggingface]</span></span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="an">image:</span><span class="co"> "https://images.unsplash.com/photo-1620712943543-bcc4688e7485?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2065&amp;q=80"</span></span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="an">series:</span></span>
<span id="cb12-10"><a href="#cb12-10"></a><span class="co">  name: "Computer Vision Foundations"</span></span>
<span id="cb12-11"><a href="#cb12-11"></a><span class="co">  number: 7</span></span>
<span id="cb12-12"><a href="#cb12-12"></a><span class="an">format:</span></span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="co">  html: default</span></span>
<span id="cb12-14"><a href="#cb12-14"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb12-15"><a href="#cb12-15"></a><span class="co">---</span></span>
<span id="cb12-16"><a href="#cb12-16"></a></span>
<span id="cb12-17"><a href="#cb12-17"></a><span class="fu">## The Evolution of Vision: From AlexNet to DINOv2</span></span>
<span id="cb12-18"><a href="#cb12-18"></a></span>
<span id="cb12-19"><a href="#cb12-19"></a>Remember when we thought AlexNet was revolutionary in 2012? That was just the beginning! In the past decade, computer vision has evolved at breakneck speed:</span>
<span id="cb12-20"><a href="#cb12-20"></a></span>
<span id="cb12-21"><a href="#cb12-21"></a><span class="ss">- </span>**2012**: AlexNet - 8 layers, 60M parameters</span>
<span id="cb12-22"><a href="#cb12-22"></a><span class="ss">- </span>**2015**: ResNet - 152 layers, skip connections</span>
<span id="cb12-23"><a href="#cb12-23"></a><span class="ss">- </span>**2017**: Attention mechanisms emerge</span>
<span id="cb12-24"><a href="#cb12-24"></a><span class="ss">- </span>**2020**: Vision Transformers - "Attention is all you need" for vision</span>
<span id="cb12-25"><a href="#cb12-25"></a><span class="ss">- </span>**2023**: DINOv2 - Foundation models that understand everything</span>
<span id="cb12-26"><a href="#cb12-26"></a></span>
<span id="cb12-27"><a href="#cb12-27"></a>Today, we're going to explore this incredible journey and show you how to use the most powerful vision models ever created!</span>
<span id="cb12-28"><a href="#cb12-28"></a></span>
<span id="cb12-29"><a href="#cb12-29"></a><span class="fu">## The CNN Dynasty: ResNet, EfficientNet, and Beyond</span></span>
<span id="cb12-30"><a href="#cb12-30"></a></span>
<span id="cb12-31"><a href="#cb12-31"></a>Before transformers took over, CNNs ruled the vision world. Let's explore the key innovations:</span>
<span id="cb12-32"><a href="#cb12-32"></a></span>
<span id="cb12-33"><a href="#cb12-33"></a><span class="fu">### ResNet: The Skip Connection Revolution</span></span>
<span id="cb12-34"><a href="#cb12-34"></a></span>
<span id="cb12-37"><a href="#cb12-37"></a><span class="in">```{python}</span></span>
<span id="cb12-38"><a href="#cb12-38"></a><span class="co">#| eval: false</span></span>
<span id="cb12-39"><a href="#cb12-39"></a><span class="im">import</span> torch</span>
<span id="cb12-40"><a href="#cb12-40"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-41"><a href="#cb12-41"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb12-42"><a href="#cb12-42"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-43"><a href="#cb12-43"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-44"><a href="#cb12-44"></a></span>
<span id="cb12-45"><a href="#cb12-45"></a><span class="co"># Understanding ResNet's key innovation: skip connections</span></span>
<span id="cb12-46"><a href="#cb12-46"></a><span class="kw">class</span> ResidualBlock(nn.Module):</span>
<span id="cb12-47"><a href="#cb12-47"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, stride<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb12-48"><a href="#cb12-48"></a>        <span class="bu">super</span>(ResidualBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb12-49"><a href="#cb12-49"></a>        </span>
<span id="cb12-50"><a href="#cb12-50"></a>        <span class="co"># Main path</span></span>
<span id="cb12-51"><a href="#cb12-51"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_channels, out_channels, <span class="dv">3</span>, stride, <span class="dv">1</span>)</span>
<span id="cb12-52"><a href="#cb12-52"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb12-53"><a href="#cb12-53"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(out_channels, out_channels, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb12-54"><a href="#cb12-54"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb12-55"><a href="#cb12-55"></a>        </span>
<span id="cb12-56"><a href="#cb12-56"></a>        <span class="co"># Skip connection (the magic!)</span></span>
<span id="cb12-57"><a href="#cb12-57"></a>        <span class="va">self</span>.skip <span class="op">=</span> nn.Sequential()</span>
<span id="cb12-58"><a href="#cb12-58"></a>        <span class="cf">if</span> stride <span class="op">!=</span> <span class="dv">1</span> <span class="kw">or</span> in_channels <span class="op">!=</span> out_channels:</span>
<span id="cb12-59"><a href="#cb12-59"></a>            <span class="va">self</span>.skip <span class="op">=</span> nn.Sequential(</span>
<span id="cb12-60"><a href="#cb12-60"></a>                nn.Conv2d(in_channels, out_channels, <span class="dv">1</span>, stride),</span>
<span id="cb12-61"><a href="#cb12-61"></a>                nn.BatchNorm2d(out_channels)</span>
<span id="cb12-62"><a href="#cb12-62"></a>            )</span>
<span id="cb12-63"><a href="#cb12-63"></a>    </span>
<span id="cb12-64"><a href="#cb12-64"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-65"><a href="#cb12-65"></a>        <span class="co"># Main path</span></span>
<span id="cb12-66"><a href="#cb12-66"></a>        out <span class="op">=</span> torch.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x)))</span>
<span id="cb12-67"><a href="#cb12-67"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(<span class="va">self</span>.conv2(out))</span>
<span id="cb12-68"><a href="#cb12-68"></a>        </span>
<span id="cb12-69"><a href="#cb12-69"></a>        <span class="co"># Add skip connection (this is the key!)</span></span>
<span id="cb12-70"><a href="#cb12-70"></a>        out <span class="op">+=</span> <span class="va">self</span>.skip(x)</span>
<span id="cb12-71"><a href="#cb12-71"></a>        out <span class="op">=</span> torch.relu(out)</span>
<span id="cb12-72"><a href="#cb12-72"></a>        </span>
<span id="cb12-73"><a href="#cb12-73"></a>        <span class="cf">return</span> out</span>
<span id="cb12-74"><a href="#cb12-74"></a></span>
<span id="cb12-75"><a href="#cb12-75"></a><span class="co"># Create a simple ResNet-like model</span></span>
<span id="cb12-76"><a href="#cb12-76"></a><span class="kw">class</span> MiniResNet(nn.Module):</span>
<span id="cb12-77"><a href="#cb12-77"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb12-78"><a href="#cb12-78"></a>        <span class="bu">super</span>(MiniResNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb12-79"><a href="#cb12-79"></a>        </span>
<span id="cb12-80"><a href="#cb12-80"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb12-81"><a href="#cb12-81"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm2d(<span class="dv">64</span>)</span>
<span id="cb12-82"><a href="#cb12-82"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb12-83"><a href="#cb12-83"></a>        </span>
<span id="cb12-84"><a href="#cb12-84"></a>        <span class="co"># Stack residual blocks</span></span>
<span id="cb12-85"><a href="#cb12-85"></a>        <span class="va">self</span>.layer1 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb12-86"><a href="#cb12-86"></a>        <span class="va">self</span>.layer2 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb12-87"><a href="#cb12-87"></a>        <span class="va">self</span>.layer3 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb12-88"><a href="#cb12-88"></a>        </span>
<span id="cb12-89"><a href="#cb12-89"></a>        <span class="va">self</span>.avgpool <span class="op">=</span> nn.AdaptiveAvgPool2d((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb12-90"><a href="#cb12-90"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">256</span>, num_classes)</span>
<span id="cb12-91"><a href="#cb12-91"></a>    </span>
<span id="cb12-92"><a href="#cb12-92"></a>    <span class="kw">def</span> _make_layer(<span class="va">self</span>, in_channels, out_channels, num_blocks, stride):</span>
<span id="cb12-93"><a href="#cb12-93"></a>        layers <span class="op">=</span> []</span>
<span id="cb12-94"><a href="#cb12-94"></a>        layers.append(ResidualBlock(in_channels, out_channels, stride))</span>
<span id="cb12-95"><a href="#cb12-95"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_blocks):</span>
<span id="cb12-96"><a href="#cb12-96"></a>            layers.append(ResidualBlock(out_channels, out_channels))</span>
<span id="cb12-97"><a href="#cb12-97"></a>        <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb12-98"><a href="#cb12-98"></a>    </span>
<span id="cb12-99"><a href="#cb12-99"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-100"><a href="#cb12-100"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(torch.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x))))</span>
<span id="cb12-101"><a href="#cb12-101"></a>        x <span class="op">=</span> <span class="va">self</span>.layer1(x)</span>
<span id="cb12-102"><a href="#cb12-102"></a>        x <span class="op">=</span> <span class="va">self</span>.layer2(x)</span>
<span id="cb12-103"><a href="#cb12-103"></a>        x <span class="op">=</span> <span class="va">self</span>.layer3(x)</span>
<span id="cb12-104"><a href="#cb12-104"></a>        x <span class="op">=</span> <span class="va">self</span>.avgpool(x)</span>
<span id="cb12-105"><a href="#cb12-105"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-106"><a href="#cb12-106"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb12-107"><a href="#cb12-107"></a>        <span class="cf">return</span> x</span>
<span id="cb12-108"><a href="#cb12-108"></a></span>
<span id="cb12-109"><a href="#cb12-109"></a><span class="co"># Compare with official ResNet</span></span>
<span id="cb12-110"><a href="#cb12-110"></a>mini_resnet <span class="op">=</span> MiniResNet(num_classes<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb12-111"><a href="#cb12-111"></a>official_resnet <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-112"><a href="#cb12-112"></a></span>
<span id="cb12-113"><a href="#cb12-113"></a><span class="bu">print</span>(<span class="st">"Mini ResNet:"</span>)</span>
<span id="cb12-114"><a href="#cb12-114"></a><span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> mini_resnet.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb12-115"><a href="#cb12-115"></a></span>
<span id="cb12-116"><a href="#cb12-116"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Official ResNet-18:"</span>)</span>
<span id="cb12-117"><a href="#cb12-117"></a><span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> official_resnet.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb12-118"><a href="#cb12-118"></a><span class="in">```</span></span>
<span id="cb12-119"><a href="#cb12-119"></a></span>
<span id="cb12-120"><a href="#cb12-120"></a>**🎯 Try it yourself!** <span class="co">[</span><span class="ot">Open in Colab</span><span class="co">](https://colab.research.google.com/github/hasanpasha/quarto_blog_hasan/blob/main/notebooks/cv-foundations-07-modern-vision-models.ipynb)</span></span>
<span id="cb12-121"><a href="#cb12-121"></a></span>
<span id="cb12-122"><a href="#cb12-122"></a><span class="fu">### EfficientNet: Scaling Done Right</span></span>
<span id="cb12-123"><a href="#cb12-123"></a></span>
<span id="cb12-126"><a href="#cb12-126"></a><span class="in">```{python}</span></span>
<span id="cb12-127"><a href="#cb12-127"></a><span class="co">#| eval: false</span></span>
<span id="cb12-128"><a href="#cb12-128"></a><span class="co"># EfficientNet's key insight: compound scaling</span></span>
<span id="cb12-129"><a href="#cb12-129"></a><span class="im">from</span> torchvision.models <span class="im">import</span> efficientnet_b0, efficientnet_b7</span>
<span id="cb12-130"><a href="#cb12-130"></a></span>
<span id="cb12-131"><a href="#cb12-131"></a><span class="co"># Load different EfficientNet variants</span></span>
<span id="cb12-132"><a href="#cb12-132"></a>efficient_b0 <span class="op">=</span> efficientnet_b0(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-133"><a href="#cb12-133"></a>efficient_b7 <span class="op">=</span> efficientnet_b7(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-134"><a href="#cb12-134"></a></span>
<span id="cb12-135"><a href="#cb12-135"></a><span class="kw">def</span> model_info(model, name):</span>
<span id="cb12-136"><a href="#cb12-136"></a>    total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb12-137"><a href="#cb12-137"></a>    <span class="cf">return</span> {</span>
<span id="cb12-138"><a href="#cb12-138"></a>        <span class="st">'name'</span>: name,</span>
<span id="cb12-139"><a href="#cb12-139"></a>        <span class="st">'parameters'</span>: total_params,</span>
<span id="cb12-140"><a href="#cb12-140"></a>        <span class="st">'size_mb'</span>: total_params <span class="op">*</span> <span class="dv">4</span> <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)  <span class="co"># Rough estimate</span></span>
<span id="cb12-141"><a href="#cb12-141"></a>    }</span>
<span id="cb12-142"><a href="#cb12-142"></a></span>
<span id="cb12-143"><a href="#cb12-143"></a>models_comparison <span class="op">=</span> [</span>
<span id="cb12-144"><a href="#cb12-144"></a>    model_info(efficient_b0, <span class="st">'EfficientNet-B0'</span>),</span>
<span id="cb12-145"><a href="#cb12-145"></a>    model_info(efficient_b7, <span class="st">'EfficientNet-B7'</span>),</span>
<span id="cb12-146"><a href="#cb12-146"></a>    model_info(official_resnet, <span class="st">'ResNet-18'</span>)</span>
<span id="cb12-147"><a href="#cb12-147"></a>]</span>
<span id="cb12-148"><a href="#cb12-148"></a></span>
<span id="cb12-149"><a href="#cb12-149"></a><span class="bu">print</span>(<span class="st">"Model Comparison:"</span>)</span>
<span id="cb12-150"><a href="#cb12-150"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb12-151"><a href="#cb12-151"></a><span class="cf">for</span> info <span class="kw">in</span> models_comparison:</span>
<span id="cb12-152"><a href="#cb12-152"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>info[<span class="st">'name'</span>]<span class="sc">:20}</span><span class="ss"> | </span><span class="sc">{</span>info[<span class="st">'parameters'</span>]<span class="sc">:</span><span class="op">&gt;</span><span class="dv">10</span><span class="sc">,}</span><span class="ss"> params | </span><span class="sc">{</span>info[<span class="st">'size_mb'</span>]<span class="sc">:&gt;6.1f}</span><span class="ss"> MB"</span>)</span>
<span id="cb12-153"><a href="#cb12-153"></a><span class="in">```</span></span>
<span id="cb12-154"><a href="#cb12-154"></a></span>
<span id="cb12-155"><a href="#cb12-155"></a><span class="fu">## The Transformer Revolution: Vision Meets Attention</span></span>
<span id="cb12-156"><a href="#cb12-156"></a></span>
<span id="cb12-157"><a href="#cb12-157"></a>In 2020, everything changed when researchers asked: "What if we applied transformers to vision?"</span>
<span id="cb12-158"><a href="#cb12-158"></a></span>
<span id="cb12-159"><a href="#cb12-159"></a><span class="fu">### Understanding Vision Transformers</span></span>
<span id="cb12-160"><a href="#cb12-160"></a></span>
<span id="cb12-163"><a href="#cb12-163"></a><span class="in">```{python}</span></span>
<span id="cb12-164"><a href="#cb12-164"></a><span class="co">#| eval: false</span></span>
<span id="cb12-165"><a href="#cb12-165"></a><span class="kw">class</span> PatchEmbedding(nn.Module):</span>
<span id="cb12-166"><a href="#cb12-166"></a>    <span class="co">"""Convert image to sequence of patches"""</span></span>
<span id="cb12-167"><a href="#cb12-167"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_size<span class="op">=</span><span class="dv">224</span>, patch_size<span class="op">=</span><span class="dv">16</span>, in_channels<span class="op">=</span><span class="dv">3</span>, embed_dim<span class="op">=</span><span class="dv">768</span>):</span>
<span id="cb12-168"><a href="#cb12-168"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-169"><a href="#cb12-169"></a>        <span class="va">self</span>.img_size <span class="op">=</span> img_size</span>
<span id="cb12-170"><a href="#cb12-170"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb12-171"><a href="#cb12-171"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> (img_size <span class="op">//</span> patch_size) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb12-172"><a href="#cb12-172"></a>        </span>
<span id="cb12-173"><a href="#cb12-173"></a>        <span class="co"># Patch embedding using convolution</span></span>
<span id="cb12-174"><a href="#cb12-174"></a>        <span class="va">self</span>.projection <span class="op">=</span> nn.Conv2d(</span>
<span id="cb12-175"><a href="#cb12-175"></a>            in_channels, embed_dim, </span>
<span id="cb12-176"><a href="#cb12-176"></a>            kernel_size<span class="op">=</span>patch_size, stride<span class="op">=</span>patch_size</span>
<span id="cb12-177"><a href="#cb12-177"></a>        )</span>
<span id="cb12-178"><a href="#cb12-178"></a>    </span>
<span id="cb12-179"><a href="#cb12-179"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-180"><a href="#cb12-180"></a>        <span class="co"># x shape: (batch_size, channels, height, width)</span></span>
<span id="cb12-181"><a href="#cb12-181"></a>        x <span class="op">=</span> <span class="va">self</span>.projection(x)  <span class="co"># (batch_size, embed_dim, num_patches_h, num_patches_w)</span></span>
<span id="cb12-182"><a href="#cb12-182"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>)        <span class="co"># (batch_size, embed_dim, num_patches)</span></span>
<span id="cb12-183"><a href="#cb12-183"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)   <span class="co"># (batch_size, num_patches, embed_dim)</span></span>
<span id="cb12-184"><a href="#cb12-184"></a>        <span class="cf">return</span> x</span>
<span id="cb12-185"><a href="#cb12-185"></a></span>
<span id="cb12-186"><a href="#cb12-186"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb12-187"><a href="#cb12-187"></a>    <span class="co">"""Multi-head self-attention mechanism"""</span></span>
<span id="cb12-188"><a href="#cb12-188"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb12-189"><a href="#cb12-189"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-190"><a href="#cb12-190"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb12-191"><a href="#cb12-191"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb12-192"><a href="#cb12-192"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> embed_dim <span class="op">//</span> num_heads</span>
<span id="cb12-193"><a href="#cb12-193"></a>        </span>
<span id="cb12-194"><a href="#cb12-194"></a>        <span class="va">self</span>.qkv <span class="op">=</span> nn.Linear(embed_dim, embed_dim <span class="op">*</span> <span class="dv">3</span>)</span>
<span id="cb12-195"><a href="#cb12-195"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb12-196"><a href="#cb12-196"></a>    </span>
<span id="cb12-197"><a href="#cb12-197"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-198"><a href="#cb12-198"></a>        batch_size, seq_len, embed_dim <span class="op">=</span> x.shape</span>
<span id="cb12-199"><a href="#cb12-199"></a>        </span>
<span id="cb12-200"><a href="#cb12-200"></a>        <span class="co"># Generate Q, K, V</span></span>
<span id="cb12-201"><a href="#cb12-201"></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv(x).reshape(batch_size, seq_len, <span class="dv">3</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb12-202"><a href="#cb12-202"></a>        qkv <span class="op">=</span> qkv.permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>)  <span class="co"># (3, batch_size, num_heads, seq_len, head_dim)</span></span>
<span id="cb12-203"><a href="#cb12-203"></a>        q, k, v <span class="op">=</span> qkv[<span class="dv">0</span>], qkv[<span class="dv">1</span>], qkv[<span class="dv">2</span>]</span>
<span id="cb12-204"><a href="#cb12-204"></a>        </span>
<span id="cb12-205"><a href="#cb12-205"></a>        <span class="co"># Compute attention</span></span>
<span id="cb12-206"><a href="#cb12-206"></a>        attn <span class="op">=</span> (q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> (<span class="va">self</span>.head_dim <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb12-207"><a href="#cb12-207"></a>        attn <span class="op">=</span> torch.softmax(attn, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb12-208"><a href="#cb12-208"></a>        </span>
<span id="cb12-209"><a href="#cb12-209"></a>        <span class="co"># Apply attention to values</span></span>
<span id="cb12-210"><a href="#cb12-210"></a>        out <span class="op">=</span> (attn <span class="op">@</span> v).transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, seq_len, embed_dim)</span>
<span id="cb12-211"><a href="#cb12-211"></a>        out <span class="op">=</span> <span class="va">self</span>.proj(out)</span>
<span id="cb12-212"><a href="#cb12-212"></a>        </span>
<span id="cb12-213"><a href="#cb12-213"></a>        <span class="cf">return</span> out, attn</span>
<span id="cb12-214"><a href="#cb12-214"></a></span>
<span id="cb12-215"><a href="#cb12-215"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb12-216"><a href="#cb12-216"></a>    <span class="co">"""Single transformer encoder block"""</span></span>
<span id="cb12-217"><a href="#cb12-217"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">12</span>, mlp_ratio<span class="op">=</span><span class="fl">4.0</span>):</span>
<span id="cb12-218"><a href="#cb12-218"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-219"><a href="#cb12-219"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb12-220"><a href="#cb12-220"></a>        <span class="va">self</span>.attn <span class="op">=</span> MultiHeadAttention(embed_dim, num_heads)</span>
<span id="cb12-221"><a href="#cb12-221"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb12-222"><a href="#cb12-222"></a>        </span>
<span id="cb12-223"><a href="#cb12-223"></a>        <span class="co"># MLP</span></span>
<span id="cb12-224"><a href="#cb12-224"></a>        mlp_hidden_dim <span class="op">=</span> <span class="bu">int</span>(embed_dim <span class="op">*</span> mlp_ratio)</span>
<span id="cb12-225"><a href="#cb12-225"></a>        <span class="va">self</span>.mlp <span class="op">=</span> nn.Sequential(</span>
<span id="cb12-226"><a href="#cb12-226"></a>            nn.Linear(embed_dim, mlp_hidden_dim),</span>
<span id="cb12-227"><a href="#cb12-227"></a>            nn.GELU(),</span>
<span id="cb12-228"><a href="#cb12-228"></a>            nn.Linear(mlp_hidden_dim, embed_dim)</span>
<span id="cb12-229"><a href="#cb12-229"></a>        )</span>
<span id="cb12-230"><a href="#cb12-230"></a>    </span>
<span id="cb12-231"><a href="#cb12-231"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-232"><a href="#cb12-232"></a>        <span class="co"># Self-attention with residual connection</span></span>
<span id="cb12-233"><a href="#cb12-233"></a>        attn_out, attn_weights <span class="op">=</span> <span class="va">self</span>.attn(<span class="va">self</span>.norm1(x))</span>
<span id="cb12-234"><a href="#cb12-234"></a>        x <span class="op">=</span> x <span class="op">+</span> attn_out</span>
<span id="cb12-235"><a href="#cb12-235"></a>        </span>
<span id="cb12-236"><a href="#cb12-236"></a>        <span class="co"># MLP with residual connection</span></span>
<span id="cb12-237"><a href="#cb12-237"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.norm2(x))</span>
<span id="cb12-238"><a href="#cb12-238"></a>        </span>
<span id="cb12-239"><a href="#cb12-239"></a>        <span class="cf">return</span> x, attn_weights</span>
<span id="cb12-240"><a href="#cb12-240"></a></span>
<span id="cb12-241"><a href="#cb12-241"></a><span class="kw">class</span> SimpleViT(nn.Module):</span>
<span id="cb12-242"><a href="#cb12-242"></a>    <span class="co">"""Simplified Vision Transformer"""</span></span>
<span id="cb12-243"><a href="#cb12-243"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_size<span class="op">=</span><span class="dv">224</span>, patch_size<span class="op">=</span><span class="dv">16</span>, num_classes<span class="op">=</span><span class="dv">1000</span>, </span>
<span id="cb12-244"><a href="#cb12-244"></a>                 embed_dim<span class="op">=</span><span class="dv">768</span>, depth<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb12-245"><a href="#cb12-245"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-246"><a href="#cb12-246"></a>        </span>
<span id="cb12-247"><a href="#cb12-247"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb12-248"><a href="#cb12-248"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> PatchEmbedding(img_size, patch_size, <span class="dv">3</span>, embed_dim)</span>
<span id="cb12-249"><a href="#cb12-249"></a>        num_patches <span class="op">=</span> <span class="va">self</span>.patch_embed.num_patches</span>
<span id="cb12-250"><a href="#cb12-250"></a>        </span>
<span id="cb12-251"><a href="#cb12-251"></a>        <span class="co"># Class token and position embedding</span></span>
<span id="cb12-252"><a href="#cb12-252"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, embed_dim))</span>
<span id="cb12-253"><a href="#cb12-253"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, num_patches <span class="op">+</span> <span class="dv">1</span>, embed_dim))</span>
<span id="cb12-254"><a href="#cb12-254"></a>        </span>
<span id="cb12-255"><a href="#cb12-255"></a>        <span class="co"># Transformer blocks</span></span>
<span id="cb12-256"><a href="#cb12-256"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.ModuleList([</span>
<span id="cb12-257"><a href="#cb12-257"></a>            TransformerBlock(embed_dim, num_heads) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth)</span>
<span id="cb12-258"><a href="#cb12-258"></a>        ])</span>
<span id="cb12-259"><a href="#cb12-259"></a>        </span>
<span id="cb12-260"><a href="#cb12-260"></a>        <span class="co"># Classification head</span></span>
<span id="cb12-261"><a href="#cb12-261"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb12-262"><a href="#cb12-262"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(embed_dim, num_classes)</span>
<span id="cb12-263"><a href="#cb12-263"></a>    </span>
<span id="cb12-264"><a href="#cb12-264"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-265"><a href="#cb12-265"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb12-266"><a href="#cb12-266"></a>        </span>
<span id="cb12-267"><a href="#cb12-267"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb12-268"><a href="#cb12-268"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)  <span class="co"># (batch_size, num_patches, embed_dim)</span></span>
<span id="cb12-269"><a href="#cb12-269"></a>        </span>
<span id="cb12-270"><a href="#cb12-270"></a>        <span class="co"># Add class token</span></span>
<span id="cb12-271"><a href="#cb12-271"></a>        cls_tokens <span class="op">=</span> <span class="va">self</span>.cls_token.expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-272"><a href="#cb12-272"></a>        x <span class="op">=</span> torch.cat((cls_tokens, x), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-273"><a href="#cb12-273"></a>        </span>
<span id="cb12-274"><a href="#cb12-274"></a>        <span class="co"># Add position embedding</span></span>
<span id="cb12-275"><a href="#cb12-275"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed</span>
<span id="cb12-276"><a href="#cb12-276"></a>        </span>
<span id="cb12-277"><a href="#cb12-277"></a>        <span class="co"># Apply transformer blocks</span></span>
<span id="cb12-278"><a href="#cb12-278"></a>        attention_maps <span class="op">=</span> []</span>
<span id="cb12-279"><a href="#cb12-279"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.blocks:</span>
<span id="cb12-280"><a href="#cb12-280"></a>            x, attn <span class="op">=</span> block(x)</span>
<span id="cb12-281"><a href="#cb12-281"></a>            attention_maps.append(attn)</span>
<span id="cb12-282"><a href="#cb12-282"></a>        </span>
<span id="cb12-283"><a href="#cb12-283"></a>        <span class="co"># Classification</span></span>
<span id="cb12-284"><a href="#cb12-284"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb12-285"><a href="#cb12-285"></a>        cls_token_final <span class="op">=</span> x[:, <span class="dv">0</span>]  <span class="co"># Use class token for classification</span></span>
<span id="cb12-286"><a href="#cb12-286"></a>        out <span class="op">=</span> <span class="va">self</span>.head(cls_token_final)</span>
<span id="cb12-287"><a href="#cb12-287"></a>        </span>
<span id="cb12-288"><a href="#cb12-288"></a>        <span class="cf">return</span> out, attention_maps</span>
<span id="cb12-289"><a href="#cb12-289"></a></span>
<span id="cb12-290"><a href="#cb12-290"></a><span class="co"># Create a simple ViT</span></span>
<span id="cb12-291"><a href="#cb12-291"></a>simple_vit <span class="op">=</span> SimpleViT(depth<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">8</span>)  <span class="co"># Smaller for demo</span></span>
<span id="cb12-292"><a href="#cb12-292"></a>vit_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> simple_vit.parameters())</span>
<span id="cb12-293"><a href="#cb12-293"></a></span>
<span id="cb12-294"><a href="#cb12-294"></a><span class="bu">print</span>(<span class="ss">f"Simple ViT parameters: </span><span class="sc">{</span>vit_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb12-295"><a href="#cb12-295"></a></span>
<span id="cb12-296"><a href="#cb12-296"></a><span class="co"># Test with dummy input</span></span>
<span id="cb12-297"><a href="#cb12-297"></a>dummy_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb12-298"><a href="#cb12-298"></a>output, attention_maps <span class="op">=</span> simple_vit(dummy_input)</span>
<span id="cb12-299"><a href="#cb12-299"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-300"><a href="#cb12-300"></a><span class="bu">print</span>(<span class="ss">f"Number of attention maps: </span><span class="sc">{</span><span class="bu">len</span>(attention_maps)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-301"><a href="#cb12-301"></a><span class="in">```</span></span>
<span id="cb12-302"><a href="#cb12-302"></a></span>
<span id="cb12-303"><a href="#cb12-303"></a><span class="fu">### Visualizing Attention: What Does the Model Look At?</span></span>
<span id="cb12-304"><a href="#cb12-304"></a></span>
<span id="cb12-307"><a href="#cb12-307"></a><span class="in">```{python}</span></span>
<span id="cb12-308"><a href="#cb12-308"></a><span class="co">#| eval: false</span></span>
<span id="cb12-309"><a href="#cb12-309"></a><span class="kw">def</span> visualize_attention(image, attention_maps, patch_size<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb12-310"><a href="#cb12-310"></a>    <span class="co">"""Visualize what the vision transformer is looking at"""</span></span>
<span id="cb12-311"><a href="#cb12-311"></a>    </span>
<span id="cb12-312"><a href="#cb12-312"></a>    <span class="co"># Use attention from the last layer, first head</span></span>
<span id="cb12-313"><a href="#cb12-313"></a>    attn <span class="op">=</span> attention_maps[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>, <span class="dv">0</span>]  <span class="co"># (seq_len, seq_len)</span></span>
<span id="cb12-314"><a href="#cb12-314"></a>    </span>
<span id="cb12-315"><a href="#cb12-315"></a>    <span class="co"># Get attention from class token to all patches</span></span>
<span id="cb12-316"><a href="#cb12-316"></a>    cls_attn <span class="op">=</span> attn[<span class="dv">0</span>, <span class="dv">1</span>:]  <span class="co"># Exclude class token to class token attention</span></span>
<span id="cb12-317"><a href="#cb12-317"></a>    </span>
<span id="cb12-318"><a href="#cb12-318"></a>    <span class="co"># Reshape to spatial dimensions</span></span>
<span id="cb12-319"><a href="#cb12-319"></a>    num_patches_per_side <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(cls_attn) <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb12-320"><a href="#cb12-320"></a>    attn_map <span class="op">=</span> cls_attn.reshape(num_patches_per_side, num_patches_per_side)</span>
<span id="cb12-321"><a href="#cb12-321"></a>    </span>
<span id="cb12-322"><a href="#cb12-322"></a>    <span class="co"># Resize to image size</span></span>
<span id="cb12-323"><a href="#cb12-323"></a>    attn_map <span class="op">=</span> torch.nn.functional.interpolate(</span>
<span id="cb12-324"><a href="#cb12-324"></a>        attn_map.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb12-325"><a href="#cb12-325"></a>        size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb12-326"><a href="#cb12-326"></a>        mode<span class="op">=</span><span class="st">'bilinear'</span></span>
<span id="cb12-327"><a href="#cb12-327"></a>    ).squeeze()</span>
<span id="cb12-328"><a href="#cb12-328"></a>    </span>
<span id="cb12-329"><a href="#cb12-329"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb12-330"><a href="#cb12-330"></a>    </span>
<span id="cb12-331"><a href="#cb12-331"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb12-332"><a href="#cb12-332"></a>    plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb12-333"><a href="#cb12-333"></a>    plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb12-334"><a href="#cb12-334"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb12-335"><a href="#cb12-335"></a>    </span>
<span id="cb12-336"><a href="#cb12-336"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb12-337"><a href="#cb12-337"></a>    plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb12-338"><a href="#cb12-338"></a>    plt.imshow(attn_map.detach().numpy(), alpha<span class="op">=</span><span class="fl">0.6</span>, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb12-339"><a href="#cb12-339"></a>    plt.title(<span class="st">"Attention Map"</span>)</span>
<span id="cb12-340"><a href="#cb12-340"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb12-341"><a href="#cb12-341"></a>    </span>
<span id="cb12-342"><a href="#cb12-342"></a>    plt.tight_layout()</span>
<span id="cb12-343"><a href="#cb12-343"></a>    plt.show()</span>
<span id="cb12-344"><a href="#cb12-344"></a></span>
<span id="cb12-345"><a href="#cb12-345"></a><span class="co"># Visualize attention (you would use a real image)</span></span>
<span id="cb12-346"><a href="#cb12-346"></a>dummy_image <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb12-347"><a href="#cb12-347"></a>visualize_attention(dummy_image, attention_maps)</span>
<span id="cb12-348"><a href="#cb12-348"></a><span class="in">```</span></span>
<span id="cb12-349"><a href="#cb12-349"></a></span>
<span id="cb12-350"><a href="#cb12-350"></a><span class="fu">## Foundation Models: The DINOv2 Revolution</span></span>
<span id="cb12-351"><a href="#cb12-351"></a></span>
<span id="cb12-352"><a href="#cb12-352"></a>Now we reach the cutting edge: **Foundation Models**. DINOv2 (Distillation with No Labels v2) represents a paradigm shift:</span>
<span id="cb12-353"><a href="#cb12-353"></a></span>
<span id="cb12-354"><a href="#cb12-354"></a><span class="ss">- </span>**Self-supervised learning**: No labels needed!</span>
<span id="cb12-355"><a href="#cb12-355"></a><span class="ss">- </span>**Universal features**: Works for any vision task</span>
<span id="cb12-356"><a href="#cb12-356"></a><span class="ss">- </span>**Incredible performance**: Often beats supervised methods</span>
<span id="cb12-357"><a href="#cb12-357"></a></span>
<span id="cb12-358"><a href="#cb12-358"></a><span class="fu">### Using DINOv2 with HuggingFace</span></span>
<span id="cb12-359"><a href="#cb12-359"></a></span>
<span id="cb12-362"><a href="#cb12-362"></a><span class="in">```{python}</span></span>
<span id="cb12-363"><a href="#cb12-363"></a><span class="co">#| eval: false</span></span>
<span id="cb12-364"><a href="#cb12-364"></a><span class="co"># Install required packages</span></span>
<span id="cb12-365"><a href="#cb12-365"></a><span class="co"># !pip install transformers torch torchvision</span></span>
<span id="cb12-366"><a href="#cb12-366"></a></span>
<span id="cb12-367"><a href="#cb12-367"></a><span class="im">from</span> transformers <span class="im">import</span> AutoImageProcessor, AutoModel</span>
<span id="cb12-368"><a href="#cb12-368"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb12-369"><a href="#cb12-369"></a><span class="im">import</span> requests</span>
<span id="cb12-370"><a href="#cb12-370"></a></span>
<span id="cb12-371"><a href="#cb12-371"></a><span class="co"># Load DINOv2 model and processor</span></span>
<span id="cb12-372"><a href="#cb12-372"></a>model_name <span class="op">=</span> <span class="st">"facebook/dinov2-base"</span></span>
<span id="cb12-373"><a href="#cb12-373"></a>processor <span class="op">=</span> AutoImageProcessor.from_pretrained(model_name)</span>
<span id="cb12-374"><a href="#cb12-374"></a>model <span class="op">=</span> AutoModel.from_pretrained(model_name)</span>
<span id="cb12-375"><a href="#cb12-375"></a></span>
<span id="cb12-376"><a href="#cb12-376"></a><span class="bu">print</span>(<span class="ss">f"Loaded DINOv2 model: </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-377"><a href="#cb12-377"></a><span class="bu">print</span>(<span class="ss">f"Model parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb12-378"><a href="#cb12-378"></a></span>
<span id="cb12-379"><a href="#cb12-379"></a><span class="kw">def</span> extract_dinov2_features(image_path_or_url):</span>
<span id="cb12-380"><a href="#cb12-380"></a>    <span class="co">"""Extract features using DINOv2"""</span></span>
<span id="cb12-381"><a href="#cb12-381"></a>    </span>
<span id="cb12-382"><a href="#cb12-382"></a>    <span class="co"># Load image</span></span>
<span id="cb12-383"><a href="#cb12-383"></a>    <span class="cf">if</span> image_path_or_url.startswith(<span class="st">'http'</span>):</span>
<span id="cb12-384"><a href="#cb12-384"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(image_path_or_url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb12-385"><a href="#cb12-385"></a>    <span class="cf">else</span>:</span>
<span id="cb12-386"><a href="#cb12-386"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(image_path_or_url)</span>
<span id="cb12-387"><a href="#cb12-387"></a>    </span>
<span id="cb12-388"><a href="#cb12-388"></a>    <span class="co"># Process image</span></span>
<span id="cb12-389"><a href="#cb12-389"></a>    inputs <span class="op">=</span> processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb12-390"><a href="#cb12-390"></a>    </span>
<span id="cb12-391"><a href="#cb12-391"></a>    <span class="co"># Extract features</span></span>
<span id="cb12-392"><a href="#cb12-392"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-393"><a href="#cb12-393"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb12-394"><a href="#cb12-394"></a>        features <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb12-395"><a href="#cb12-395"></a>        </span>
<span id="cb12-396"><a href="#cb12-396"></a>        <span class="co"># Get CLS token (global image representation)</span></span>
<span id="cb12-397"><a href="#cb12-397"></a>        cls_features <span class="op">=</span> features[:, <span class="dv">0</span>]  <span class="co"># Shape: (1, 768)</span></span>
<span id="cb12-398"><a href="#cb12-398"></a>        </span>
<span id="cb12-399"><a href="#cb12-399"></a>        <span class="co"># Get patch features (local representations)</span></span>
<span id="cb12-400"><a href="#cb12-400"></a>        patch_features <span class="op">=</span> features[:, <span class="dv">1</span>:]  <span class="co"># Shape: (1, num_patches, 768)</span></span>
<span id="cb12-401"><a href="#cb12-401"></a>    </span>
<span id="cb12-402"><a href="#cb12-402"></a>    <span class="cf">return</span> {</span>
<span id="cb12-403"><a href="#cb12-403"></a>        <span class="st">'cls_features'</span>: cls_features,</span>
<span id="cb12-404"><a href="#cb12-404"></a>        <span class="st">'patch_features'</span>: patch_features,</span>
<span id="cb12-405"><a href="#cb12-405"></a>        <span class="st">'image'</span>: image</span>
<span id="cb12-406"><a href="#cb12-406"></a>    }</span>
<span id="cb12-407"><a href="#cb12-407"></a></span>
<span id="cb12-408"><a href="#cb12-408"></a><span class="co"># Example usage</span></span>
<span id="cb12-409"><a href="#cb12-409"></a><span class="kw">def</span> demo_dinov2_features():</span>
<span id="cb12-410"><a href="#cb12-410"></a>    <span class="co">"""Demonstrate DINOv2 feature extraction"""</span></span>
<span id="cb12-411"><a href="#cb12-411"></a>    </span>
<span id="cb12-412"><a href="#cb12-412"></a>    <span class="co"># Create dummy image for demo (you would use real images)</span></span>
<span id="cb12-413"><a href="#cb12-413"></a>    dummy_image <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb12-414"><a href="#cb12-414"></a>    </span>
<span id="cb12-415"><a href="#cb12-415"></a>    <span class="co"># Save temporarily</span></span>
<span id="cb12-416"><a href="#cb12-416"></a>    dummy_image.save(<span class="st">'temp_image.jpg'</span>)</span>
<span id="cb12-417"><a href="#cb12-417"></a>    </span>
<span id="cb12-418"><a href="#cb12-418"></a>    <span class="co"># Extract features</span></span>
<span id="cb12-419"><a href="#cb12-419"></a>    result <span class="op">=</span> extract_dinov2_features(<span class="st">'temp_image.jpg'</span>)</span>
<span id="cb12-420"><a href="#cb12-420"></a>    </span>
<span id="cb12-421"><a href="#cb12-421"></a>    <span class="bu">print</span>(<span class="st">"DINOv2 Feature Extraction Results:"</span>)</span>
<span id="cb12-422"><a href="#cb12-422"></a>    <span class="bu">print</span>(<span class="ss">f"Global features shape: </span><span class="sc">{</span>result[<span class="st">'cls_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-423"><a href="#cb12-423"></a>    <span class="bu">print</span>(<span class="ss">f"Patch features shape: </span><span class="sc">{</span>result[<span class="st">'patch_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-424"><a href="#cb12-424"></a>    </span>
<span id="cb12-425"><a href="#cb12-425"></a>    <span class="co"># Visualize features</span></span>
<span id="cb12-426"><a href="#cb12-426"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb12-427"><a href="#cb12-427"></a>    </span>
<span id="cb12-428"><a href="#cb12-428"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb12-429"><a href="#cb12-429"></a>    plt.imshow(result[<span class="st">'image'</span>])</span>
<span id="cb12-430"><a href="#cb12-430"></a>    plt.title(<span class="st">"Input Image"</span>)</span>
<span id="cb12-431"><a href="#cb12-431"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb12-432"><a href="#cb12-432"></a>    </span>
<span id="cb12-433"><a href="#cb12-433"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb12-434"><a href="#cb12-434"></a>    plt.plot(result[<span class="st">'cls_features'</span>].squeeze().numpy())</span>
<span id="cb12-435"><a href="#cb12-435"></a>    plt.title(<span class="st">"Global Features (768 dimensions)"</span>)</span>
<span id="cb12-436"><a href="#cb12-436"></a>    plt.xlabel(<span class="st">"Dimension"</span>)</span>
<span id="cb12-437"><a href="#cb12-437"></a>    plt.ylabel(<span class="st">"Value"</span>)</span>
<span id="cb12-438"><a href="#cb12-438"></a>    </span>
<span id="cb12-439"><a href="#cb12-439"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb12-440"><a href="#cb12-440"></a>    <span class="co"># Visualize patch features as heatmap</span></span>
<span id="cb12-441"><a href="#cb12-441"></a>    patch_norms <span class="op">=</span> torch.norm(result[<span class="st">'patch_features'</span>].squeeze(), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-442"><a href="#cb12-442"></a>    patch_size <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(patch_norms) <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb12-443"><a href="#cb12-443"></a>    patch_map <span class="op">=</span> patch_norms.reshape(patch_size, patch_size)</span>
<span id="cb12-444"><a href="#cb12-444"></a>    plt.imshow(patch_map.numpy(), cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb12-445"><a href="#cb12-445"></a>    plt.title(<span class="st">"Patch Feature Magnitudes"</span>)</span>
<span id="cb12-446"><a href="#cb12-446"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb12-447"><a href="#cb12-447"></a>    </span>
<span id="cb12-448"><a href="#cb12-448"></a>    plt.tight_layout()</span>
<span id="cb12-449"><a href="#cb12-449"></a>    plt.show()</span>
<span id="cb12-450"><a href="#cb12-450"></a>    </span>
<span id="cb12-451"><a href="#cb12-451"></a>    <span class="cf">return</span> result</span>
<span id="cb12-452"><a href="#cb12-452"></a></span>
<span id="cb12-453"><a href="#cb12-453"></a>demo_result <span class="op">=</span> demo_dinov2_features()</span>
<span id="cb12-454"><a href="#cb12-454"></a><span class="in">```</span></span>
<span id="cb12-455"><a href="#cb12-455"></a></span>
<span id="cb12-456"><a href="#cb12-456"></a><span class="fu">### Building a DINOv2-Powered Image Similarity Engine</span></span>
<span id="cb12-457"><a href="#cb12-457"></a></span>
<span id="cb12-460"><a href="#cb12-460"></a><span class="in">```{python}</span></span>
<span id="cb12-461"><a href="#cb12-461"></a><span class="co">#| eval: false</span></span>
<span id="cb12-462"><a href="#cb12-462"></a><span class="kw">class</span> DINOv2SimilarityEngine:</span>
<span id="cb12-463"><a href="#cb12-463"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb12-464"><a href="#cb12-464"></a>        <span class="va">self</span>.processor <span class="op">=</span> AutoImageProcessor.from_pretrained(<span class="st">"facebook/dinov2-base"</span>)</span>
<span id="cb12-465"><a href="#cb12-465"></a>        <span class="va">self</span>.model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">"facebook/dinov2-base"</span>)</span>
<span id="cb12-466"><a href="#cb12-466"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb12-467"><a href="#cb12-467"></a>        <span class="va">self</span>.image_database <span class="op">=</span> {}</span>
<span id="cb12-468"><a href="#cb12-468"></a>    </span>
<span id="cb12-469"><a href="#cb12-469"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, image):</span>
<span id="cb12-470"><a href="#cb12-470"></a>        <span class="co">"""Extract DINOv2 features from an image"""</span></span>
<span id="cb12-471"><a href="#cb12-471"></a>        inputs <span class="op">=</span> <span class="va">self</span>.processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb12-472"><a href="#cb12-472"></a>        </span>
<span id="cb12-473"><a href="#cb12-473"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-474"><a href="#cb12-474"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(<span class="op">**</span>inputs)</span>
<span id="cb12-475"><a href="#cb12-475"></a>            <span class="co"># Use CLS token as global image representation</span></span>
<span id="cb12-476"><a href="#cb12-476"></a>            features <span class="op">=</span> outputs.last_hidden_state[:, <span class="dv">0</span>]</span>
<span id="cb12-477"><a href="#cb12-477"></a>        </span>
<span id="cb12-478"><a href="#cb12-478"></a>        <span class="cf">return</span> features</span>
<span id="cb12-479"><a href="#cb12-479"></a>    </span>
<span id="cb12-480"><a href="#cb12-480"></a>    <span class="kw">def</span> add_image(<span class="va">self</span>, image_id, image):</span>
<span id="cb12-481"><a href="#cb12-481"></a>        <span class="co">"""Add an image to the database"""</span></span>
<span id="cb12-482"><a href="#cb12-482"></a>        features <span class="op">=</span> <span class="va">self</span>.extract_features(image)</span>
<span id="cb12-483"><a href="#cb12-483"></a>        <span class="va">self</span>.image_database[image_id] <span class="op">=</span> {</span>
<span id="cb12-484"><a href="#cb12-484"></a>            <span class="st">'features'</span>: features,</span>
<span id="cb12-485"><a href="#cb12-485"></a>            <span class="st">'image'</span>: image</span>
<span id="cb12-486"><a href="#cb12-486"></a>        }</span>
<span id="cb12-487"><a href="#cb12-487"></a>        <span class="bu">print</span>(<span class="ss">f"Added image '</span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ss">' to database"</span>)</span>
<span id="cb12-488"><a href="#cb12-488"></a>    </span>
<span id="cb12-489"><a href="#cb12-489"></a>    <span class="kw">def</span> find_similar_images(<span class="va">self</span>, query_image, top_k<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb12-490"><a href="#cb12-490"></a>        <span class="co">"""Find most similar images in the database"""</span></span>
<span id="cb12-491"><a href="#cb12-491"></a>        query_features <span class="op">=</span> <span class="va">self</span>.extract_features(query_image)</span>
<span id="cb12-492"><a href="#cb12-492"></a>        </span>
<span id="cb12-493"><a href="#cb12-493"></a>        similarities <span class="op">=</span> {}</span>
<span id="cb12-494"><a href="#cb12-494"></a>        </span>
<span id="cb12-495"><a href="#cb12-495"></a>        <span class="cf">for</span> image_id, data <span class="kw">in</span> <span class="va">self</span>.image_database.items():</span>
<span id="cb12-496"><a href="#cb12-496"></a>            <span class="co"># Compute cosine similarity</span></span>
<span id="cb12-497"><a href="#cb12-497"></a>            similarity <span class="op">=</span> torch.cosine_similarity(</span>
<span id="cb12-498"><a href="#cb12-498"></a>                query_features, data[<span class="st">'features'</span>], dim<span class="op">=</span><span class="dv">1</span></span>
<span id="cb12-499"><a href="#cb12-499"></a>            ).item()</span>
<span id="cb12-500"><a href="#cb12-500"></a>            similarities[image_id] <span class="op">=</span> similarity</span>
<span id="cb12-501"><a href="#cb12-501"></a>        </span>
<span id="cb12-502"><a href="#cb12-502"></a>        <span class="co"># Sort by similarity</span></span>
<span id="cb12-503"><a href="#cb12-503"></a>        sorted_similarities <span class="op">=</span> <span class="bu">sorted</span>(</span>
<span id="cb12-504"><a href="#cb12-504"></a>            similarities.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-505"><a href="#cb12-505"></a>        )</span>
<span id="cb12-506"><a href="#cb12-506"></a>        </span>
<span id="cb12-507"><a href="#cb12-507"></a>        <span class="cf">return</span> sorted_similarities[:top_k]</span>
<span id="cb12-508"><a href="#cb12-508"></a>    </span>
<span id="cb12-509"><a href="#cb12-509"></a>    <span class="kw">def</span> visualize_results(<span class="va">self</span>, query_image, similar_images):</span>
<span id="cb12-510"><a href="#cb12-510"></a>        <span class="co">"""Visualize similarity search results"""</span></span>
<span id="cb12-511"><a href="#cb12-511"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">8</span>))</span>
<span id="cb12-512"><a href="#cb12-512"></a>        </span>
<span id="cb12-513"><a href="#cb12-513"></a>        <span class="co"># Query image</span></span>
<span id="cb12-514"><a href="#cb12-514"></a>        plt.subplot(<span class="dv">2</span>, <span class="bu">len</span>(similar_images) <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb12-515"><a href="#cb12-515"></a>        plt.imshow(query_image)</span>
<span id="cb12-516"><a href="#cb12-516"></a>        plt.title(<span class="st">"Query Image"</span>)</span>
<span id="cb12-517"><a href="#cb12-517"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb12-518"><a href="#cb12-518"></a>        </span>
<span id="cb12-519"><a href="#cb12-519"></a>        <span class="co"># Similar images</span></span>
<span id="cb12-520"><a href="#cb12-520"></a>        <span class="cf">for</span> i, (image_id, similarity) <span class="kw">in</span> <span class="bu">enumerate</span>(similar_images):</span>
<span id="cb12-521"><a href="#cb12-521"></a>            plt.subplot(<span class="dv">2</span>, <span class="bu">len</span>(similar_images) <span class="op">+</span> <span class="dv">1</span>, i <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb12-522"><a href="#cb12-522"></a>            plt.imshow(<span class="va">self</span>.image_database[image_id][<span class="st">'image'</span>])</span>
<span id="cb12-523"><a href="#cb12-523"></a>            plt.title(<span class="ss">f"</span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ch">\n</span><span class="ss">Similarity: </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-524"><a href="#cb12-524"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb12-525"><a href="#cb12-525"></a>        </span>
<span id="cb12-526"><a href="#cb12-526"></a>        plt.tight_layout()</span>
<span id="cb12-527"><a href="#cb12-527"></a>        plt.show()</span>
<span id="cb12-528"><a href="#cb12-528"></a></span>
<span id="cb12-529"><a href="#cb12-529"></a><span class="co"># Create similarity engine</span></span>
<span id="cb12-530"><a href="#cb12-530"></a>similarity_engine <span class="op">=</span> DINOv2SimilarityEngine()</span>
<span id="cb12-531"><a href="#cb12-531"></a></span>
<span id="cb12-532"><a href="#cb12-532"></a><span class="co"># Demo with dummy images (you would use real images)</span></span>
<span id="cb12-533"><a href="#cb12-533"></a><span class="kw">def</span> demo_similarity_engine():</span>
<span id="cb12-534"><a href="#cb12-534"></a>    <span class="co">"""Demonstrate the similarity engine"""</span></span>
<span id="cb12-535"><a href="#cb12-535"></a>    </span>
<span id="cb12-536"><a href="#cb12-536"></a>    <span class="co"># Create some dummy images with different colors</span></span>
<span id="cb12-537"><a href="#cb12-537"></a>    colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'yellow'</span>, <span class="st">'purple'</span>]</span>
<span id="cb12-538"><a href="#cb12-538"></a>    </span>
<span id="cb12-539"><a href="#cb12-539"></a>    <span class="cf">for</span> color <span class="kw">in</span> colors:</span>
<span id="cb12-540"><a href="#cb12-540"></a>        dummy_img <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span>color)</span>
<span id="cb12-541"><a href="#cb12-541"></a>        similarity_engine.add_image(<span class="ss">f"</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss">_image"</span>, dummy_img)</span>
<span id="cb12-542"><a href="#cb12-542"></a>    </span>
<span id="cb12-543"><a href="#cb12-543"></a>    <span class="co"># Query with a red image</span></span>
<span id="cb12-544"><a href="#cb12-544"></a>    query_img <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb12-545"><a href="#cb12-545"></a>    </span>
<span id="cb12-546"><a href="#cb12-546"></a>    <span class="co"># Find similar images</span></span>
<span id="cb12-547"><a href="#cb12-547"></a>    similar <span class="op">=</span> similarity_engine.find_similar_images(query_img, top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb12-548"><a href="#cb12-548"></a>    </span>
<span id="cb12-549"><a href="#cb12-549"></a>    <span class="bu">print</span>(<span class="st">"Most similar images:"</span>)</span>
<span id="cb12-550"><a href="#cb12-550"></a>    <span class="cf">for</span> image_id, similarity <span class="kw">in</span> similar:</span>
<span id="cb12-551"><a href="#cb12-551"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-552"><a href="#cb12-552"></a>    </span>
<span id="cb12-553"><a href="#cb12-553"></a>    <span class="co"># Visualize results</span></span>
<span id="cb12-554"><a href="#cb12-554"></a>    similarity_engine.visualize_results(query_img, similar)</span>
<span id="cb12-555"><a href="#cb12-555"></a></span>
<span id="cb12-556"><a href="#cb12-556"></a>demo_similarity_engine()</span>
<span id="cb12-557"><a href="#cb12-557"></a><span class="in">```</span></span>
<span id="cb12-558"><a href="#cb12-558"></a></span>
<span id="cb12-559"><a href="#cb12-559"></a><span class="fu">## Comparing All Approaches: The Ultimate Showdown</span></span>
<span id="cb12-560"><a href="#cb12-560"></a></span>
<span id="cb12-561"><a href="#cb12-561"></a>Let's compare all the approaches we've learned:</span>
<span id="cb12-562"><a href="#cb12-562"></a></span>
<span id="cb12-565"><a href="#cb12-565"></a><span class="in">```{python}</span></span>
<span id="cb12-566"><a href="#cb12-566"></a><span class="co">#| eval: false</span></span>
<span id="cb12-567"><a href="#cb12-567"></a><span class="kw">class</span> VisionModelComparison:</span>
<span id="cb12-568"><a href="#cb12-568"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb12-569"><a href="#cb12-569"></a>        <span class="va">self</span>.models <span class="op">=</span> {</span>
<span id="cb12-570"><a href="#cb12-570"></a>            <span class="st">'ResNet-18'</span>: models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-571"><a href="#cb12-571"></a>            <span class="st">'ResNet-50'</span>: models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-572"><a href="#cb12-572"></a>            <span class="st">'EfficientNet-B0'</span>: efficientnet_b0(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-573"><a href="#cb12-573"></a>            <span class="st">'ViT-Base'</span>: <span class="va">None</span>,  <span class="co"># Would load from transformers</span></span>
<span id="cb12-574"><a href="#cb12-574"></a>            <span class="st">'DINOv2-Base'</span>: <span class="va">None</span>  <span class="co"># Already loaded above</span></span>
<span id="cb12-575"><a href="#cb12-575"></a>        }</span>
<span id="cb12-576"><a href="#cb12-576"></a>    </span>
<span id="cb12-577"><a href="#cb12-577"></a>    <span class="kw">def</span> compare_models(<span class="va">self</span>):</span>
<span id="cb12-578"><a href="#cb12-578"></a>        <span class="co">"""Compare different vision models"""</span></span>
<span id="cb12-579"><a href="#cb12-579"></a>        comparison_data <span class="op">=</span> []</span>
<span id="cb12-580"><a href="#cb12-580"></a>        </span>
<span id="cb12-581"><a href="#cb12-581"></a>        <span class="cf">for</span> name, model <span class="kw">in</span> <span class="va">self</span>.models.items():</span>
<span id="cb12-582"><a href="#cb12-582"></a>            <span class="cf">if</span> model <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-583"><a href="#cb12-583"></a>                params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb12-584"><a href="#cb12-584"></a>                size_mb <span class="op">=</span> params <span class="op">*</span> <span class="dv">4</span> <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)</span>
<span id="cb12-585"><a href="#cb12-585"></a>                </span>
<span id="cb12-586"><a href="#cb12-586"></a>                comparison_data.append({</span>
<span id="cb12-587"><a href="#cb12-587"></a>                    <span class="st">'Model'</span>: name,</span>
<span id="cb12-588"><a href="#cb12-588"></a>                    <span class="st">'Parameters (M)'</span>: <span class="ss">f"</span><span class="sc">{</span>params <span class="op">/</span> <span class="fl">1e6</span><span class="sc">:.1f}</span><span class="ss">"</span>,</span>
<span id="cb12-589"><a href="#cb12-589"></a>                    <span class="st">'Size (MB)'</span>: <span class="ss">f"</span><span class="sc">{</span>size_mb<span class="sc">:.1f}</span><span class="ss">"</span>,</span>
<span id="cb12-590"><a href="#cb12-590"></a>                    <span class="st">'Year'</span>: <span class="va">self</span>.get_year(name),</span>
<span id="cb12-591"><a href="#cb12-591"></a>                    <span class="st">'Type'</span>: <span class="va">self</span>.get_type(name)</span>
<span id="cb12-592"><a href="#cb12-592"></a>                })</span>
<span id="cb12-593"><a href="#cb12-593"></a>        </span>
<span id="cb12-594"><a href="#cb12-594"></a>        <span class="cf">return</span> comparison_data</span>
<span id="cb12-595"><a href="#cb12-595"></a>    </span>
<span id="cb12-596"><a href="#cb12-596"></a>    <span class="kw">def</span> get_year(<span class="va">self</span>, name):</span>
<span id="cb12-597"><a href="#cb12-597"></a>        year_map <span class="op">=</span> {</span>
<span id="cb12-598"><a href="#cb12-598"></a>            <span class="st">'ResNet-18'</span>: <span class="dv">2015</span>,</span>
<span id="cb12-599"><a href="#cb12-599"></a>            <span class="st">'ResNet-50'</span>: <span class="dv">2015</span>,</span>
<span id="cb12-600"><a href="#cb12-600"></a>            <span class="st">'EfficientNet-B0'</span>: <span class="dv">2019</span>,</span>
<span id="cb12-601"><a href="#cb12-601"></a>            <span class="st">'ViT-Base'</span>: <span class="dv">2020</span>,</span>
<span id="cb12-602"><a href="#cb12-602"></a>            <span class="st">'DINOv2-Base'</span>: <span class="dv">2023</span></span>
<span id="cb12-603"><a href="#cb12-603"></a>        }</span>
<span id="cb12-604"><a href="#cb12-604"></a>        <span class="cf">return</span> year_map.get(name, <span class="st">'Unknown'</span>)</span>
<span id="cb12-605"><a href="#cb12-605"></a>    </span>
<span id="cb12-606"><a href="#cb12-606"></a>    <span class="kw">def</span> get_type(<span class="va">self</span>, name):</span>
<span id="cb12-607"><a href="#cb12-607"></a>        <span class="cf">if</span> <span class="st">'ResNet'</span> <span class="kw">in</span> name <span class="kw">or</span> <span class="st">'EfficientNet'</span> <span class="kw">in</span> name:</span>
<span id="cb12-608"><a href="#cb12-608"></a>            <span class="cf">return</span> <span class="st">'CNN'</span></span>
<span id="cb12-609"><a href="#cb12-609"></a>        <span class="cf">elif</span> <span class="st">'ViT'</span> <span class="kw">in</span> name:</span>
<span id="cb12-610"><a href="#cb12-610"></a>            <span class="cf">return</span> <span class="st">'Transformer'</span></span>
<span id="cb12-611"><a href="#cb12-611"></a>        <span class="cf">elif</span> <span class="st">'DINOv2'</span> <span class="kw">in</span> name:</span>
<span id="cb12-612"><a href="#cb12-612"></a>            <span class="cf">return</span> <span class="st">'Foundation Model'</span></span>
<span id="cb12-613"><a href="#cb12-613"></a>        <span class="cf">return</span> <span class="st">'Unknown'</span></span>
<span id="cb12-614"><a href="#cb12-614"></a>    </span>
<span id="cb12-615"><a href="#cb12-615"></a>    <span class="kw">def</span> visualize_comparison(<span class="va">self</span>, data):</span>
<span id="cb12-616"><a href="#cb12-616"></a>        <span class="co">"""Visualize model comparison"""</span></span>
<span id="cb12-617"><a href="#cb12-617"></a>        <span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-618"><a href="#cb12-618"></a>        </span>
<span id="cb12-619"><a href="#cb12-619"></a>        df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb12-620"><a href="#cb12-620"></a>        </span>
<span id="cb12-621"><a href="#cb12-621"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb12-622"><a href="#cb12-622"></a>        </span>
<span id="cb12-623"><a href="#cb12-623"></a>        <span class="co"># Parameters vs Year</span></span>
<span id="cb12-624"><a href="#cb12-624"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb12-625"><a href="#cb12-625"></a>        <span class="cf">for</span> model_type <span class="kw">in</span> df[<span class="st">'Type'</span>].unique():</span>
<span id="cb12-626"><a href="#cb12-626"></a>            subset <span class="op">=</span> df[df[<span class="st">'Type'</span>] <span class="op">==</span> model_type]</span>
<span id="cb12-627"><a href="#cb12-627"></a>            plt.scatter(subset[<span class="st">'Year'</span>], subset[<span class="st">'Parameters (M)'</span>].astype(<span class="bu">float</span>), </span>
<span id="cb12-628"><a href="#cb12-628"></a>                       label<span class="op">=</span>model_type, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb12-629"><a href="#cb12-629"></a>        </span>
<span id="cb12-630"><a href="#cb12-630"></a>        plt.xlabel(<span class="st">'Year'</span>)</span>
<span id="cb12-631"><a href="#cb12-631"></a>        plt.ylabel(<span class="st">'Parameters (Millions)'</span>)</span>
<span id="cb12-632"><a href="#cb12-632"></a>        plt.title(<span class="st">'Model Size Evolution'</span>)</span>
<span id="cb12-633"><a href="#cb12-633"></a>        plt.legend()</span>
<span id="cb12-634"><a href="#cb12-634"></a>        plt.grid(<span class="va">True</span>)</span>
<span id="cb12-635"><a href="#cb12-635"></a>        </span>
<span id="cb12-636"><a href="#cb12-636"></a>        <span class="co"># Model types distribution</span></span>
<span id="cb12-637"><a href="#cb12-637"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb12-638"><a href="#cb12-638"></a>        type_counts <span class="op">=</span> df[<span class="st">'Type'</span>].value_counts()</span>
<span id="cb12-639"><a href="#cb12-639"></a>        plt.pie(type_counts.values, labels<span class="op">=</span>type_counts.index, autopct<span class="op">=</span><span class="st">'</span><span class="sc">%1.1f%%</span><span class="st">'</span>)</span>
<span id="cb12-640"><a href="#cb12-640"></a>        plt.title(<span class="st">'Model Types Distribution'</span>)</span>
<span id="cb12-641"><a href="#cb12-641"></a>        </span>
<span id="cb12-642"><a href="#cb12-642"></a>        <span class="co"># Size comparison</span></span>
<span id="cb12-643"><a href="#cb12-643"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb12-644"><a href="#cb12-644"></a>        plt.bar(df[<span class="st">'Model'</span>], df[<span class="st">'Size (MB)'</span>].astype(<span class="bu">float</span>))</span>
<span id="cb12-645"><a href="#cb12-645"></a>        plt.xticks(rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb12-646"><a href="#cb12-646"></a>        plt.ylabel(<span class="st">'Size (MB)'</span>)</span>
<span id="cb12-647"><a href="#cb12-647"></a>        plt.title(<span class="st">'Model Size Comparison'</span>)</span>
<span id="cb12-648"><a href="#cb12-648"></a>        </span>
<span id="cb12-649"><a href="#cb12-649"></a>        <span class="co"># Timeline</span></span>
<span id="cb12-650"><a href="#cb12-650"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb12-651"><a href="#cb12-651"></a>        timeline_data <span class="op">=</span> df.sort_values(<span class="st">'Year'</span>)</span>
<span id="cb12-652"><a href="#cb12-652"></a>        plt.plot(timeline_data[<span class="st">'Year'</span>], <span class="bu">range</span>(<span class="bu">len</span>(timeline_data)), <span class="st">'o-'</span>)</span>
<span id="cb12-653"><a href="#cb12-653"></a>        <span class="cf">for</span> i, (idx, row) <span class="kw">in</span> <span class="bu">enumerate</span>(timeline_data.iterrows()):</span>
<span id="cb12-654"><a href="#cb12-654"></a>            plt.annotate(row[<span class="st">'Model'</span>], (row[<span class="st">'Year'</span>], i), </span>
<span id="cb12-655"><a href="#cb12-655"></a>                        xytext<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">0</span>), textcoords<span class="op">=</span><span class="st">'offset points'</span>)</span>
<span id="cb12-656"><a href="#cb12-656"></a>        plt.xlabel(<span class="st">'Year'</span>)</span>
<span id="cb12-657"><a href="#cb12-657"></a>        plt.ylabel(<span class="st">'Model Index'</span>)</span>
<span id="cb12-658"><a href="#cb12-658"></a>        plt.title(<span class="st">'Vision Models Timeline'</span>)</span>
<span id="cb12-659"><a href="#cb12-659"></a>        plt.grid(<span class="va">True</span>)</span>
<span id="cb12-660"><a href="#cb12-660"></a>        </span>
<span id="cb12-661"><a href="#cb12-661"></a>        plt.tight_layout()</span>
<span id="cb12-662"><a href="#cb12-662"></a>        plt.show()</span>
<span id="cb12-663"><a href="#cb12-663"></a></span>
<span id="cb12-664"><a href="#cb12-664"></a><span class="co"># Run comparison</span></span>
<span id="cb12-665"><a href="#cb12-665"></a>comparison <span class="op">=</span> VisionModelComparison()</span>
<span id="cb12-666"><a href="#cb12-666"></a>comparison_data <span class="op">=</span> comparison.compare_models()</span>
<span id="cb12-667"><a href="#cb12-667"></a></span>
<span id="cb12-668"><a href="#cb12-668"></a><span class="bu">print</span>(<span class="st">"Vision Models Comparison:"</span>)</span>
<span id="cb12-669"><a href="#cb12-669"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">80</span>)</span>
<span id="cb12-670"><a href="#cb12-670"></a><span class="cf">for</span> data <span class="kw">in</span> comparison_data:</span>
<span id="cb12-671"><a href="#cb12-671"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>data[<span class="st">'Model'</span>]<span class="sc">:15}</span><span class="ss"> | </span><span class="sc">{</span>data[<span class="st">'Year'</span>]<span class="sc">}</span><span class="ss"> | </span><span class="sc">{</span>data[<span class="st">'Type'</span>]<span class="sc">:15}</span><span class="ss"> | "</span></span>
<span id="cb12-672"><a href="#cb12-672"></a>          <span class="ss">f"</span><span class="sc">{</span>data[<span class="st">'Parameters (M)'</span>]<span class="sc">:&gt;8}</span><span class="ss"> M | </span><span class="sc">{</span>data[<span class="st">'Size (MB)'</span>]<span class="sc">:&gt;8}</span><span class="ss"> MB"</span>)</span>
<span id="cb12-673"><a href="#cb12-673"></a></span>
<span id="cb12-674"><a href="#cb12-674"></a>comparison.visualize_comparison(comparison_data)</span>
<span id="cb12-675"><a href="#cb12-675"></a><span class="in">```</span></span>
<span id="cb12-676"><a href="#cb12-676"></a></span>
<span id="cb12-677"><a href="#cb12-677"></a><span class="fu">## The Future: What's Next?</span></span>
<span id="cb12-678"><a href="#cb12-678"></a></span>
<span id="cb12-679"><a href="#cb12-679"></a>As we look ahead, several trends are shaping the future of computer vision:</span>
<span id="cb12-680"><a href="#cb12-680"></a></span>
<span id="cb12-681"><a href="#cb12-681"></a><span class="fu">### 1. **Multimodal Foundation Models**</span></span>
<span id="cb12-682"><a href="#cb12-682"></a><span class="in">```python</span></span>
<span id="cb12-683"><a href="#cb12-683"></a><span class="co"># Future: Models that understand both vision and language</span></span>
<span id="cb12-684"><a href="#cb12-684"></a><span class="co"># Example: CLIP, GPT-4V, LLaVA</span></span>
<span id="cb12-685"><a href="#cb12-685"></a><span class="in">```</span></span>
<span id="cb12-686"><a href="#cb12-686"></a></span>
<span id="cb12-687"><a href="#cb12-687"></a><span class="fu">### 2. **Efficient Architectures**</span></span>
<span id="cb12-688"><a href="#cb12-688"></a><span class="in">```python</span></span>
<span id="cb12-689"><a href="#cb12-689"></a><span class="co"># Trend: Smaller, faster models for mobile devices</span></span>
<span id="cb12-690"><a href="#cb12-690"></a><span class="co"># Example: MobileViT, EfficientViT</span></span>
<span id="cb12-691"><a href="#cb12-691"></a><span class="in">```</span></span>
<span id="cb12-692"><a href="#cb12-692"></a></span>
<span id="cb12-693"><a href="#cb12-693"></a><span class="fu">### 3. **Self-Supervised Learning**</span></span>
<span id="cb12-694"><a href="#cb12-694"></a><span class="in">```python</span></span>
<span id="cb12-695"><a href="#cb12-695"></a><span class="co"># Growing trend: Learning without labels</span></span>
<span id="cb12-696"><a href="#cb12-696"></a><span class="co"># Example: MAE, SimCLR, DINOv2</span></span>
<span id="cb12-697"><a href="#cb12-697"></a><span class="in">```</span></span>
<span id="cb12-698"><a href="#cb12-698"></a></span>
<span id="cb12-699"><a href="#cb12-699"></a><span class="fu">## Your Challenge: Build a Modern Vision Pipeline</span></span>
<span id="cb12-700"><a href="#cb12-700"></a></span>
<span id="cb12-701"><a href="#cb12-701"></a>Now it's your turn to build a complete modern vision system:</span>
<span id="cb12-702"><a href="#cb12-702"></a></span>
<span id="cb12-705"><a href="#cb12-705"></a><span class="in">```{python}</span></span>
<span id="cb12-706"><a href="#cb12-706"></a><span class="co">#| eval: false</span></span>
<span id="cb12-707"><a href="#cb12-707"></a><span class="kw">class</span> ModernVisionPipeline:</span>
<span id="cb12-708"><a href="#cb12-708"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb12-709"><a href="#cb12-709"></a>        <span class="co"># Load multiple models for different tasks</span></span>
<span id="cb12-710"><a href="#cb12-710"></a>        <span class="va">self</span>.classification_model <span class="op">=</span> models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-711"><a href="#cb12-711"></a>        <span class="va">self</span>.feature_extractor <span class="op">=</span> <span class="va">None</span>  <span class="co"># DINOv2 model</span></span>
<span id="cb12-712"><a href="#cb12-712"></a>        <span class="va">self</span>.similarity_engine <span class="op">=</span> DINOv2SimilarityEngine()</span>
<span id="cb12-713"><a href="#cb12-713"></a>    </span>
<span id="cb12-714"><a href="#cb12-714"></a>    <span class="kw">def</span> classify_image(<span class="va">self</span>, image):</span>
<span id="cb12-715"><a href="#cb12-715"></a>        <span class="co">"""Classify image using ResNet"""</span></span>
<span id="cb12-716"><a href="#cb12-716"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb12-717"><a href="#cb12-717"></a>        <span class="cf">pass</span></span>
<span id="cb12-718"><a href="#cb12-718"></a>    </span>
<span id="cb12-719"><a href="#cb12-719"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, image):</span>
<span id="cb12-720"><a href="#cb12-720"></a>        <span class="co">"""Extract features using DINOv2"""</span></span>
<span id="cb12-721"><a href="#cb12-721"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb12-722"><a href="#cb12-722"></a>        <span class="cf">pass</span></span>
<span id="cb12-723"><a href="#cb12-723"></a>    </span>
<span id="cb12-724"><a href="#cb12-724"></a>    <span class="kw">def</span> find_similar_images(<span class="va">self</span>, query_image, database):</span>
<span id="cb12-725"><a href="#cb12-725"></a>        <span class="co">"""Find similar images using DINOv2 features"""</span></span>
<span id="cb12-726"><a href="#cb12-726"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb12-727"><a href="#cb12-727"></a>        <span class="cf">pass</span></span>
<span id="cb12-728"><a href="#cb12-728"></a>    </span>
<span id="cb12-729"><a href="#cb12-729"></a>    <span class="kw">def</span> analyze_image(<span class="va">self</span>, image):</span>
<span id="cb12-730"><a href="#cb12-730"></a>        <span class="co">"""Complete image analysis pipeline"""</span></span>
<span id="cb12-731"><a href="#cb12-731"></a>        results <span class="op">=</span> {</span>
<span id="cb12-732"><a href="#cb12-732"></a>            <span class="st">'classification'</span>: <span class="va">self</span>.classify_image(image),</span>
<span id="cb12-733"><a href="#cb12-733"></a>            <span class="st">'features'</span>: <span class="va">self</span>.extract_features(image),</span>
<span id="cb12-734"><a href="#cb12-734"></a>            <span class="st">'similar_images'</span>: <span class="va">self</span>.find_similar_images(image, <span class="va">self</span>.image_database)</span>
<span id="cb12-735"><a href="#cb12-735"></a>        }</span>
<span id="cb12-736"><a href="#cb12-736"></a>        <span class="cf">return</span> results</span>
<span id="cb12-737"><a href="#cb12-737"></a></span>
<span id="cb12-738"><a href="#cb12-738"></a><span class="co"># Build your pipeline!</span></span>
<span id="cb12-739"><a href="#cb12-739"></a>pipeline <span class="op">=</span> ModernVisionPipeline()</span>
<span id="cb12-740"><a href="#cb12-740"></a><span class="in">```</span></span>
<span id="cb12-741"><a href="#cb12-741"></a></span>
<span id="cb12-742"><a href="#cb12-742"></a><span class="fu">## What's Coming Next?</span></span>
<span id="cb12-743"><a href="#cb12-743"></a></span>
<span id="cb12-744"><a href="#cb12-744"></a>In our next post, <span class="co">[</span><span class="ot">**"Your First CV Project: Putting It All Together"**</span><span class="co">](../08-first-cv-project/)</span>, we'll:</span>
<span id="cb12-745"><a href="#cb12-745"></a></span>
<span id="cb12-746"><a href="#cb12-746"></a><span class="ss">- </span>**Build a complete computer vision application**</span>
<span id="cb12-747"><a href="#cb12-747"></a><span class="ss">- </span>**Combine classical and modern techniques**</span>
<span id="cb12-748"><a href="#cb12-748"></a><span class="ss">- </span>**Deploy your model for real-world use**</span>
<span id="cb12-749"><a href="#cb12-749"></a><span class="ss">- </span>**Create an interactive demo**</span>
<span id="cb12-750"><a href="#cb12-750"></a></span>
<span id="cb12-751"><a href="#cb12-751"></a>You've just learned about the most advanced vision models ever created—next, we'll put everything together into a real project!</span>
<span id="cb12-752"><a href="#cb12-752"></a></span>
<span id="cb12-753"><a href="#cb12-753"></a><span class="fu">## Key Takeaways</span></span>
<span id="cb12-754"><a href="#cb12-754"></a></span>
<span id="cb12-755"><a href="#cb12-755"></a><span class="ss">- </span>**CNNs dominated** computer vision for a decade</span>
<span id="cb12-756"><a href="#cb12-756"></a><span class="ss">- </span>**Vision Transformers** brought attention mechanisms to vision</span>
<span id="cb12-757"><a href="#cb12-757"></a><span class="ss">- </span>**Foundation models** like DINOv2 learn universal representations</span>
<span id="cb12-758"><a href="#cb12-758"></a><span class="ss">- </span>**Self-supervised learning** eliminates the need for labels</span>
<span id="cb12-759"><a href="#cb12-759"></a><span class="ss">- </span>**Modern pipelines** combine multiple approaches</span>
<span id="cb12-760"><a href="#cb12-760"></a><span class="ss">- </span>**The field evolves rapidly**—stay curious and keep learning!</span>
<span id="cb12-761"><a href="#cb12-761"></a></span>
<span id="cb12-762"><a href="#cb12-762"></a>:::{.callout-tip}</span>
<span id="cb12-763"><a href="#cb12-763"></a><span class="fu">## Hands-On Lab</span></span>
<span id="cb12-764"><a href="#cb12-764"></a>Ready to experiment with cutting-edge vision models? Try the complete interactive notebook: <span class="co">[</span><span class="ot">**Modern Vision Models Lab**</span><span class="co">](https://colab.research.google.com/drive/1Modern_Vision_Models_123456)</span></span>
<span id="cb12-765"><a href="#cb12-765"></a></span>
<span id="cb12-766"><a href="#cb12-766"></a>Compare CNNs, Vision Transformers, and DINOv2 on your own images!</span>
<span id="cb12-767"><a href="#cb12-767"></a>:::</span>
<span id="cb12-768"><a href="#cb12-768"></a></span>
<span id="cb12-769"><a href="#cb12-769"></a>:::{.callout-note}</span>
<span id="cb12-770"><a href="#cb12-770"></a><span class="fu">## Series Navigation</span></span>
<span id="cb12-771"><a href="#cb12-771"></a><span class="ss">- </span>**Previous**: <span class="co">[</span><span class="ot">Why Deep Learning? When Classical Methods Hit the Wall</span><span class="co">](../06-why-deep-learning/)</span></span>
<span id="cb12-772"><a href="#cb12-772"></a><span class="ss">- </span>**Next**: <span class="co">[</span><span class="ot">Your First CV Project: Putting It All Together</span><span class="co">](../08-first-cv-project/)</span></span>
<span id="cb12-773"><a href="#cb12-773"></a><span class="ss">- </span>**Series Home**: <span class="co">[</span><span class="ot">Computer Vision Foundations</span><span class="co">](../computer-vision-foundations.qmd)</span></span>
<span id="cb12-774"><a href="#cb12-774"></a>:::</span>
<span id="cb12-775"><a href="#cb12-775"></a></span>
<span id="cb12-776"><a href="#cb12-776"></a>---</span>
<span id="cb12-777"><a href="#cb12-777"></a></span>
<span id="cb12-778"><a href="#cb12-778"></a>*You've just explored the cutting edge of computer vision! From ResNet's skip connections to DINOv2's self-supervised learning—you now understand the models that power today's AI applications. Next, we'll build something amazing with all this knowledge!* </span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



<script src="../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>