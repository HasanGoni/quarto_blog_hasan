<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hasan">
<meta name="dcterms.date" content="2025-01-22">

<title>Hasan’s Data Science &amp; AI Blog - Modern Vision Models: CNNs, Vision Transformers, and DINOv2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Hasan’s Data Science &amp; AI Blog - Modern Vision Models: CNNs, Vision Transformers, and DINOv2">
<meta property="og:description" content="">
<meta property="og:image" content="https://images.unsplash.com/photo-1620712943543-bcc4688e7485?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2065&amp;q=80">
<meta property="og:site-name" content="Hasan's Data Science &amp; AI Blog">
<meta name="twitter:title" content="Hasan’s Data Science &amp; AI Blog - Modern Vision Models: CNNs, Vision Transformers, and DINOv2">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://images.unsplash.com/photo-1620712943543-bcc4688e7485?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2065&amp;q=80">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Hasan’s Data Science &amp; AI Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-series" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Series</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-series">    
        <li>
    <a class="dropdown-item" href="../../../posts/series/command-line-mastery.html" rel="" target="">
 <span class="dropdown-text">Command Line Mastery for HPC</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/data-science-steps.html" rel="" target="">
 <span class="dropdown-text">Data Science Steps</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/computer-vision-foundations.html" rel="" target="">
 <span class="dropdown-text">Computer Vision Foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/vim-mastery.html" rel="" target="">
 <span class="dropdown-text">Vim Mastery</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/vlm.html" rel="" target="">
 <span class="dropdown-text">VLM Series</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/series/anomaly-detection/index.html" rel="" target="">
 <span class="dropdown-text">Anomaly Detection</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../categories.html" rel="" target="">
 <span class="menu-text">Categories</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../tags.html" rel="" target="">
 <span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/HasanGoni" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hasangoni" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/series/computer-vision-foundations.html">Computer Vision Foundations</a></li><li class="breadcrumb-item"><a href="../../../posts/series/cv-foundations/08-modern-vision-models.html">8. Modern Vision Models</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Modern Vision Models: CNNs, Vision Transformers, and DINOv2</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">computer-vision</div>
                <div class="quarto-category">transformers</div>
                <div class="quarto-category">foundation-models</div>
                <div class="quarto-category">dinov2</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Hasan </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 22, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">June 28, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Command Line Mastery for HPC</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Command Line Mastery for HPC Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/01-digital-compass.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. Digital Compass</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/02-file-detective.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. File Detective</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/03-file-manipulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. File Manipulation Magic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/04-text-processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Text Processing Power</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/05-permission-wizardry.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Permission Wizardry</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/06-ssh-remote-access.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. SSH and Remote Access</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/07-file-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. File Transfer Mastery</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/08-environment-mastery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. Environment Mastery</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/09-hpc-job-scheduling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. HPC Job Scheduling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/10-parallel-processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. Parallel Processing Power</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/11-bash-scripting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Bash Scripting Essentials</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/command-line-mastery/12-automation-excellence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">12. Automation Excellence</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Data Science</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/data-science-steps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part01/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 01</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part02/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 02</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part03/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 03</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part04/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 04</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part05/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 05</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/data-science-steps-to-follow-part06/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Steps to Follow - 06</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Computer Vision Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/computer-vision-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computer Vision Foundations Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/01-why-computer-vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. Why Computer Vision?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/02-images-as-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Images as Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/03-opencv-essentials.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. OpenCV Essentials</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/04-finding-patterns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Finding Patterns</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/05-image-segmentation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Image Segmentation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/06-feature-magic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Feature Magic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/07-why-deep-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Why Deep Learning?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/08-modern-vision-models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">8. Modern Vision Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/09-first-cv-project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. First CV Project</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/cv-foundations/10-where-to-go-next.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. Where to Go Next</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Anomaly Detection</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/anomaly-detection/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/anomaly-detection/finding-the-oddballs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Finding the Oddballs</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">VLM Series</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/series/vlm-qwen3-14b/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Qwen3-14B</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-evolution-of-vision-from-alexnet-to-dinov2" id="toc-the-evolution-of-vision-from-alexnet-to-dinov2" class="nav-link active" data-scroll-target="#the-evolution-of-vision-from-alexnet-to-dinov2"><span class="header-section-number">0.1</span> The Evolution of Vision: From AlexNet to DINOv2</a></li>
  <li><a href="#the-cnn-dynasty-resnet-efficientnet-and-beyond" id="toc-the-cnn-dynasty-resnet-efficientnet-and-beyond" class="nav-link" data-scroll-target="#the-cnn-dynasty-resnet-efficientnet-and-beyond"><span class="header-section-number">0.2</span> The CNN Dynasty: ResNet, EfficientNet, and Beyond</a>
  <ul class="collapse">
  <li><a href="#resnet-the-skip-connection-revolution" id="toc-resnet-the-skip-connection-revolution" class="nav-link" data-scroll-target="#resnet-the-skip-connection-revolution"><span class="header-section-number">0.2.1</span> ResNet: The Skip Connection Revolution</a></li>
  <li><a href="#efficientnet-scaling-done-right" id="toc-efficientnet-scaling-done-right" class="nav-link" data-scroll-target="#efficientnet-scaling-done-right"><span class="header-section-number">0.2.2</span> EfficientNet: Scaling Done Right</a></li>
  </ul></li>
  <li><a href="#the-transformer-revolution-vision-meets-attention" id="toc-the-transformer-revolution-vision-meets-attention" class="nav-link" data-scroll-target="#the-transformer-revolution-vision-meets-attention"><span class="header-section-number">0.3</span> The Transformer Revolution: Vision Meets Attention</a>
  <ul class="collapse">
  <li><a href="#understanding-vision-transformers" id="toc-understanding-vision-transformers" class="nav-link" data-scroll-target="#understanding-vision-transformers"><span class="header-section-number">0.3.1</span> Understanding Vision Transformers</a></li>
  <li><a href="#visualizing-attention-what-does-the-model-look-at" id="toc-visualizing-attention-what-does-the-model-look-at" class="nav-link" data-scroll-target="#visualizing-attention-what-does-the-model-look-at"><span class="header-section-number">0.3.2</span> Visualizing Attention: What Does the Model Look At?</a></li>
  </ul></li>
  <li><a href="#foundation-models-the-dinov2-revolution" id="toc-foundation-models-the-dinov2-revolution" class="nav-link" data-scroll-target="#foundation-models-the-dinov2-revolution"><span class="header-section-number">0.4</span> Foundation Models: The DINOv2 Revolution</a>
  <ul class="collapse">
  <li><a href="#using-dinov2-with-huggingface" id="toc-using-dinov2-with-huggingface" class="nav-link" data-scroll-target="#using-dinov2-with-huggingface"><span class="header-section-number">0.4.1</span> Using DINOv2 with HuggingFace</a></li>
  <li><a href="#building-a-dinov2-powered-image-similarity-engine" id="toc-building-a-dinov2-powered-image-similarity-engine" class="nav-link" data-scroll-target="#building-a-dinov2-powered-image-similarity-engine"><span class="header-section-number">0.4.2</span> Building a DINOv2-Powered Image Similarity Engine</a></li>
  </ul></li>
  <li><a href="#comparing-all-approaches-the-ultimate-showdown" id="toc-comparing-all-approaches-the-ultimate-showdown" class="nav-link" data-scroll-target="#comparing-all-approaches-the-ultimate-showdown"><span class="header-section-number">0.5</span> Comparing All Approaches: The Ultimate Showdown</a></li>
  <li><a href="#the-future-whats-next" id="toc-the-future-whats-next" class="nav-link" data-scroll-target="#the-future-whats-next"><span class="header-section-number">0.6</span> The Future: What’s Next?</a>
  <ul class="collapse">
  <li><a href="#multimodal-foundation-models" id="toc-multimodal-foundation-models" class="nav-link" data-scroll-target="#multimodal-foundation-models"><span class="header-section-number">0.6.1</span> 1. <strong>Multimodal Foundation Models</strong></a></li>
  <li><a href="#efficient-architectures" id="toc-efficient-architectures" class="nav-link" data-scroll-target="#efficient-architectures"><span class="header-section-number">0.6.2</span> 2. <strong>Efficient Architectures</strong></a></li>
  <li><a href="#self-supervised-learning" id="toc-self-supervised-learning" class="nav-link" data-scroll-target="#self-supervised-learning"><span class="header-section-number">0.6.3</span> 3. <strong>Self-Supervised Learning</strong></a></li>
  </ul></li>
  <li><a href="#your-challenge-build-a-modern-vision-pipeline" id="toc-your-challenge-build-a-modern-vision-pipeline" class="nav-link" data-scroll-target="#your-challenge-build-a-modern-vision-pipeline"><span class="header-section-number">0.7</span> Your Challenge: Build a Modern Vision Pipeline</a></li>
  <li><a href="#whats-coming-next" id="toc-whats-coming-next" class="nav-link" data-scroll-target="#whats-coming-next"><span class="header-section-number">0.8</span> What’s Coming Next?</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">0.9</span> Key Takeaways</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="the-evolution-of-vision-from-alexnet-to-dinov2" class="level2" data-number="0.1">
<h2 data-number="0.1" class="anchored" data-anchor-id="the-evolution-of-vision-from-alexnet-to-dinov2"><span class="header-section-number">0.1</span> The Evolution of Vision: From AlexNet to DINOv2</h2>
<p>Remember when we thought AlexNet was revolutionary in 2012? That was just the beginning! In the past decade, computer vision has evolved at breakneck speed:</p>
<ul>
<li><strong>2012</strong>: AlexNet - 8 layers, 60M parameters</li>
<li><strong>2015</strong>: ResNet - 152 layers, skip connections</li>
<li><strong>2017</strong>: Attention mechanisms emerge</li>
<li><strong>2020</strong>: Vision Transformers - “Attention is all you need” for vision</li>
<li><strong>2023</strong>: DINOv2 - Foundation models that understand everything</li>
</ul>
<p>Today, we’re going to explore this incredible journey and show you how to use the most powerful vision models ever created!</p>
</section>
<section id="the-cnn-dynasty-resnet-efficientnet-and-beyond" class="level2" data-number="0.2">
<h2 data-number="0.2" class="anchored" data-anchor-id="the-cnn-dynasty-resnet-efficientnet-and-beyond"><span class="header-section-number">0.2</span> The CNN Dynasty: ResNet, EfficientNet, and Beyond</h2>
<p>Before transformers took over, CNNs ruled the vision world. Let’s explore the key innovations:</p>
<section id="resnet-the-skip-connection-revolution" class="level3" data-number="0.2.1">
<h3 data-number="0.2.1" class="anchored" data-anchor-id="resnet-the-skip-connection-revolution"><span class="header-section-number">0.2.1</span> ResNet: The Skip Connection Revolution</h3>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># Understanding ResNet's key innovation: skip connections</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="kw">class</span> ResidualBlock(nn.Module):</span>
<span id="cb1-9"><a href="#cb1-9"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, stride<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb1-10"><a href="#cb1-10"></a>        <span class="bu">super</span>(ResidualBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-11"><a href="#cb1-11"></a>        </span>
<span id="cb1-12"><a href="#cb1-12"></a>        <span class="co"># Main path</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_channels, out_channels, <span class="dv">3</span>, stride, <span class="dv">1</span>)</span>
<span id="cb1-14"><a href="#cb1-14"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb1-15"><a href="#cb1-15"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(out_channels, out_channels, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-16"><a href="#cb1-16"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb1-17"><a href="#cb1-17"></a>        </span>
<span id="cb1-18"><a href="#cb1-18"></a>        <span class="co"># Skip connection (the magic!)</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>        <span class="va">self</span>.skip <span class="op">=</span> nn.Sequential()</span>
<span id="cb1-20"><a href="#cb1-20"></a>        <span class="cf">if</span> stride <span class="op">!=</span> <span class="dv">1</span> <span class="kw">or</span> in_channels <span class="op">!=</span> out_channels:</span>
<span id="cb1-21"><a href="#cb1-21"></a>            <span class="va">self</span>.skip <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-22"><a href="#cb1-22"></a>                nn.Conv2d(in_channels, out_channels, <span class="dv">1</span>, stride),</span>
<span id="cb1-23"><a href="#cb1-23"></a>                nn.BatchNorm2d(out_channels)</span>
<span id="cb1-24"><a href="#cb1-24"></a>            )</span>
<span id="cb1-25"><a href="#cb1-25"></a>    </span>
<span id="cb1-26"><a href="#cb1-26"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-27"><a href="#cb1-27"></a>        <span class="co"># Main path</span></span>
<span id="cb1-28"><a href="#cb1-28"></a>        out <span class="op">=</span> torch.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x)))</span>
<span id="cb1-29"><a href="#cb1-29"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(<span class="va">self</span>.conv2(out))</span>
<span id="cb1-30"><a href="#cb1-30"></a>        </span>
<span id="cb1-31"><a href="#cb1-31"></a>        <span class="co"># Add skip connection (this is the key!)</span></span>
<span id="cb1-32"><a href="#cb1-32"></a>        out <span class="op">+=</span> <span class="va">self</span>.skip(x)</span>
<span id="cb1-33"><a href="#cb1-33"></a>        out <span class="op">=</span> torch.relu(out)</span>
<span id="cb1-34"><a href="#cb1-34"></a>        </span>
<span id="cb1-35"><a href="#cb1-35"></a>        <span class="cf">return</span> out</span>
<span id="cb1-36"><a href="#cb1-36"></a></span>
<span id="cb1-37"><a href="#cb1-37"></a><span class="co"># Create a simple ResNet-like model</span></span>
<span id="cb1-38"><a href="#cb1-38"></a><span class="kw">class</span> MiniResNet(nn.Module):</span>
<span id="cb1-39"><a href="#cb1-39"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb1-40"><a href="#cb1-40"></a>        <span class="bu">super</span>(MiniResNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-41"><a href="#cb1-41"></a>        </span>
<span id="cb1-42"><a href="#cb1-42"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb1-43"><a href="#cb1-43"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm2d(<span class="dv">64</span>)</span>
<span id="cb1-44"><a href="#cb1-44"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-45"><a href="#cb1-45"></a>        </span>
<span id="cb1-46"><a href="#cb1-46"></a>        <span class="co"># Stack residual blocks</span></span>
<span id="cb1-47"><a href="#cb1-47"></a>        <span class="va">self</span>.layer1 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-48"><a href="#cb1-48"></a>        <span class="va">self</span>.layer2 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-49"><a href="#cb1-49"></a>        <span class="va">self</span>.layer3 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-50"><a href="#cb1-50"></a>        </span>
<span id="cb1-51"><a href="#cb1-51"></a>        <span class="va">self</span>.avgpool <span class="op">=</span> nn.AdaptiveAvgPool2d((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb1-52"><a href="#cb1-52"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">256</span>, num_classes)</span>
<span id="cb1-53"><a href="#cb1-53"></a>    </span>
<span id="cb1-54"><a href="#cb1-54"></a>    <span class="kw">def</span> _make_layer(<span class="va">self</span>, in_channels, out_channels, num_blocks, stride):</span>
<span id="cb1-55"><a href="#cb1-55"></a>        layers <span class="op">=</span> []</span>
<span id="cb1-56"><a href="#cb1-56"></a>        layers.append(ResidualBlock(in_channels, out_channels, stride))</span>
<span id="cb1-57"><a href="#cb1-57"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_blocks):</span>
<span id="cb1-58"><a href="#cb1-58"></a>            layers.append(ResidualBlock(out_channels, out_channels))</span>
<span id="cb1-59"><a href="#cb1-59"></a>        <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb1-60"><a href="#cb1-60"></a>    </span>
<span id="cb1-61"><a href="#cb1-61"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-62"><a href="#cb1-62"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(torch.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x))))</span>
<span id="cb1-63"><a href="#cb1-63"></a>        x <span class="op">=</span> <span class="va">self</span>.layer1(x)</span>
<span id="cb1-64"><a href="#cb1-64"></a>        x <span class="op">=</span> <span class="va">self</span>.layer2(x)</span>
<span id="cb1-65"><a href="#cb1-65"></a>        x <span class="op">=</span> <span class="va">self</span>.layer3(x)</span>
<span id="cb1-66"><a href="#cb1-66"></a>        x <span class="op">=</span> <span class="va">self</span>.avgpool(x)</span>
<span id="cb1-67"><a href="#cb1-67"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-68"><a href="#cb1-68"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb1-69"><a href="#cb1-69"></a>        <span class="cf">return</span> x</span>
<span id="cb1-70"><a href="#cb1-70"></a></span>
<span id="cb1-71"><a href="#cb1-71"></a><span class="co"># Compare with official ResNet</span></span>
<span id="cb1-72"><a href="#cb1-72"></a>mini_resnet <span class="op">=</span> MiniResNet(num_classes<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb1-73"><a href="#cb1-73"></a>official_resnet <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-74"><a href="#cb1-74"></a></span>
<span id="cb1-75"><a href="#cb1-75"></a><span class="bu">print</span>(<span class="st">"Mini ResNet:"</span>)</span>
<span id="cb1-76"><a href="#cb1-76"></a><span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> mini_resnet.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb1-77"><a href="#cb1-77"></a></span>
<span id="cb1-78"><a href="#cb1-78"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Official ResNet-18:"</span>)</span>
<span id="cb1-79"><a href="#cb1-79"></a><span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> official_resnet.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Mini ResNet:
Parameters: 3,042,024

Official ResNet-18:
Parameters: 11,689,512</code></pre>
</div>
</div>
<p><strong>🎯 Try it yourself!</strong> <a href="https://colab.research.google.com/github/hasanpasha/quarto_blog_hasan/blob/main/notebooks/cv-foundations-07-modern-vision-models.ipynb">Open in Colab</a></p>
</section>
<section id="efficientnet-scaling-done-right" class="level3" data-number="0.2.2">
<h3 data-number="0.2.2" class="anchored" data-anchor-id="efficientnet-scaling-done-right"><span class="header-section-number">0.2.2</span> EfficientNet: Scaling Done Right</h3>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># EfficientNet's key insight: compound scaling</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">from</span> torchvision.models <span class="im">import</span> efficientnet_b0, efficientnet_b7</span>
<span id="cb3-3"><a href="#cb3-3"></a></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co"># Load different EfficientNet variants</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>efficient_b0 <span class="op">=</span> efficientnet_b0(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6"></a>efficient_b7 <span class="op">=</span> efficientnet_b7(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="kw">def</span> model_info(model, name):</span>
<span id="cb3-9"><a href="#cb3-9"></a>    total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb3-10"><a href="#cb3-10"></a>    <span class="cf">return</span> {</span>
<span id="cb3-11"><a href="#cb3-11"></a>        <span class="st">'name'</span>: name,</span>
<span id="cb3-12"><a href="#cb3-12"></a>        <span class="st">'parameters'</span>: total_params,</span>
<span id="cb3-13"><a href="#cb3-13"></a>        <span class="st">'size_mb'</span>: total_params <span class="op">*</span> <span class="dv">4</span> <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)  <span class="co"># Rough estimate</span></span>
<span id="cb3-14"><a href="#cb3-14"></a>    }</span>
<span id="cb3-15"><a href="#cb3-15"></a></span>
<span id="cb3-16"><a href="#cb3-16"></a>models_comparison <span class="op">=</span> [</span>
<span id="cb3-17"><a href="#cb3-17"></a>    model_info(efficient_b0, <span class="st">'EfficientNet-B0'</span>),</span>
<span id="cb3-18"><a href="#cb3-18"></a>    model_info(efficient_b7, <span class="st">'EfficientNet-B7'</span>),</span>
<span id="cb3-19"><a href="#cb3-19"></a>    model_info(official_resnet, <span class="st">'ResNet-18'</span>)</span>
<span id="cb3-20"><a href="#cb3-20"></a>]</span>
<span id="cb3-21"><a href="#cb3-21"></a></span>
<span id="cb3-22"><a href="#cb3-22"></a><span class="bu">print</span>(<span class="st">"Model Comparison:"</span>)</span>
<span id="cb3-23"><a href="#cb3-23"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb3-24"><a href="#cb3-24"></a><span class="cf">for</span> info <span class="kw">in</span> models_comparison:</span>
<span id="cb3-25"><a href="#cb3-25"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>info[<span class="st">'name'</span>]<span class="sc">:20}</span><span class="ss"> | </span><span class="sc">{</span>info[<span class="st">'parameters'</span>]<span class="sc">:</span><span class="op">&gt;</span><span class="dv">10</span><span class="sc">,}</span><span class="ss"> params | </span><span class="sc">{</span>info[<span class="st">'size_mb'</span>]<span class="sc">:&gt;6.1f}</span><span class="ss"> MB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model Comparison:
--------------------------------------------------
EfficientNet-B0      |  5,288,548 params |   20.2 MB
EfficientNet-B7      | 66,347,960 params |  253.1 MB
ResNet-18            | 11,689,512 params |   44.6 MB</code></pre>
</div>
</div>
</section>
</section>
<section id="the-transformer-revolution-vision-meets-attention" class="level2" data-number="0.3">
<h2 data-number="0.3" class="anchored" data-anchor-id="the-transformer-revolution-vision-meets-attention"><span class="header-section-number">0.3</span> The Transformer Revolution: Vision Meets Attention</h2>
<p>In 2020, everything changed when researchers asked: “What if we applied transformers to vision?”</p>
<section id="understanding-vision-transformers" class="level3" data-number="0.3.1">
<h3 data-number="0.3.1" class="anchored" data-anchor-id="understanding-vision-transformers"><span class="header-section-number">0.3.1</span> Understanding Vision Transformers</h3>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">class</span> PatchEmbedding(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="co">"""Convert image to sequence of patches"""</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_size<span class="op">=</span><span class="dv">224</span>, patch_size<span class="op">=</span><span class="dv">16</span>, in_channels<span class="op">=</span><span class="dv">3</span>, embed_dim<span class="op">=</span><span class="dv">768</span>):</span>
<span id="cb5-4"><a href="#cb5-4"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-5"><a href="#cb5-5"></a>        <span class="va">self</span>.img_size <span class="op">=</span> img_size</span>
<span id="cb5-6"><a href="#cb5-6"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb5-7"><a href="#cb5-7"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> (img_size <span class="op">//</span> patch_size) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>        </span>
<span id="cb5-9"><a href="#cb5-9"></a>        <span class="co"># Patch embedding using convolution</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>        <span class="va">self</span>.projection <span class="op">=</span> nn.Conv2d(</span>
<span id="cb5-11"><a href="#cb5-11"></a>            in_channels, embed_dim, </span>
<span id="cb5-12"><a href="#cb5-12"></a>            kernel_size<span class="op">=</span>patch_size, stride<span class="op">=</span>patch_size</span>
<span id="cb5-13"><a href="#cb5-13"></a>        )</span>
<span id="cb5-14"><a href="#cb5-14"></a>    </span>
<span id="cb5-15"><a href="#cb5-15"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-16"><a href="#cb5-16"></a>        <span class="co"># x shape: (batch_size, channels, height, width)</span></span>
<span id="cb5-17"><a href="#cb5-17"></a>        x <span class="op">=</span> <span class="va">self</span>.projection(x)  <span class="co"># (batch_size, embed_dim, num_patches_h, num_patches_w)</span></span>
<span id="cb5-18"><a href="#cb5-18"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>)        <span class="co"># (batch_size, embed_dim, num_patches)</span></span>
<span id="cb5-19"><a href="#cb5-19"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)   <span class="co"># (batch_size, num_patches, embed_dim)</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>        <span class="cf">return</span> x</span>
<span id="cb5-21"><a href="#cb5-21"></a></span>
<span id="cb5-22"><a href="#cb5-22"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb5-23"><a href="#cb5-23"></a>    <span class="co">"""Multi-head self-attention mechanism"""</span></span>
<span id="cb5-24"><a href="#cb5-24"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb5-25"><a href="#cb5-25"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-26"><a href="#cb5-26"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb5-27"><a href="#cb5-27"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb5-28"><a href="#cb5-28"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> embed_dim <span class="op">//</span> num_heads</span>
<span id="cb5-29"><a href="#cb5-29"></a>        </span>
<span id="cb5-30"><a href="#cb5-30"></a>        <span class="va">self</span>.qkv <span class="op">=</span> nn.Linear(embed_dim, embed_dim <span class="op">*</span> <span class="dv">3</span>)</span>
<span id="cb5-31"><a href="#cb5-31"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb5-32"><a href="#cb5-32"></a>    </span>
<span id="cb5-33"><a href="#cb5-33"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-34"><a href="#cb5-34"></a>        batch_size, seq_len, embed_dim <span class="op">=</span> x.shape</span>
<span id="cb5-35"><a href="#cb5-35"></a>        </span>
<span id="cb5-36"><a href="#cb5-36"></a>        <span class="co"># Generate Q, K, V</span></span>
<span id="cb5-37"><a href="#cb5-37"></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv(x).reshape(batch_size, seq_len, <span class="dv">3</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb5-38"><a href="#cb5-38"></a>        qkv <span class="op">=</span> qkv.permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>)  <span class="co"># (3, batch_size, num_heads, seq_len, head_dim)</span></span>
<span id="cb5-39"><a href="#cb5-39"></a>        q, k, v <span class="op">=</span> qkv[<span class="dv">0</span>], qkv[<span class="dv">1</span>], qkv[<span class="dv">2</span>]</span>
<span id="cb5-40"><a href="#cb5-40"></a>        </span>
<span id="cb5-41"><a href="#cb5-41"></a>        <span class="co"># Compute attention</span></span>
<span id="cb5-42"><a href="#cb5-42"></a>        attn <span class="op">=</span> (q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> (<span class="va">self</span>.head_dim <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb5-43"><a href="#cb5-43"></a>        attn <span class="op">=</span> torch.softmax(attn, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-44"><a href="#cb5-44"></a>        </span>
<span id="cb5-45"><a href="#cb5-45"></a>        <span class="co"># Apply attention to values</span></span>
<span id="cb5-46"><a href="#cb5-46"></a>        out <span class="op">=</span> (attn <span class="op">@</span> v).transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, seq_len, embed_dim)</span>
<span id="cb5-47"><a href="#cb5-47"></a>        out <span class="op">=</span> <span class="va">self</span>.proj(out)</span>
<span id="cb5-48"><a href="#cb5-48"></a>        </span>
<span id="cb5-49"><a href="#cb5-49"></a>        <span class="cf">return</span> out, attn</span>
<span id="cb5-50"><a href="#cb5-50"></a></span>
<span id="cb5-51"><a href="#cb5-51"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb5-52"><a href="#cb5-52"></a>    <span class="co">"""Single transformer encoder block"""</span></span>
<span id="cb5-53"><a href="#cb5-53"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">12</span>, mlp_ratio<span class="op">=</span><span class="fl">4.0</span>):</span>
<span id="cb5-54"><a href="#cb5-54"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-55"><a href="#cb5-55"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb5-56"><a href="#cb5-56"></a>        <span class="va">self</span>.attn <span class="op">=</span> MultiHeadAttention(embed_dim, num_heads)</span>
<span id="cb5-57"><a href="#cb5-57"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb5-58"><a href="#cb5-58"></a>        </span>
<span id="cb5-59"><a href="#cb5-59"></a>        <span class="co"># MLP</span></span>
<span id="cb5-60"><a href="#cb5-60"></a>        mlp_hidden_dim <span class="op">=</span> <span class="bu">int</span>(embed_dim <span class="op">*</span> mlp_ratio)</span>
<span id="cb5-61"><a href="#cb5-61"></a>        <span class="va">self</span>.mlp <span class="op">=</span> nn.Sequential(</span>
<span id="cb5-62"><a href="#cb5-62"></a>            nn.Linear(embed_dim, mlp_hidden_dim),</span>
<span id="cb5-63"><a href="#cb5-63"></a>            nn.GELU(),</span>
<span id="cb5-64"><a href="#cb5-64"></a>            nn.Linear(mlp_hidden_dim, embed_dim)</span>
<span id="cb5-65"><a href="#cb5-65"></a>        )</span>
<span id="cb5-66"><a href="#cb5-66"></a>    </span>
<span id="cb5-67"><a href="#cb5-67"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-68"><a href="#cb5-68"></a>        <span class="co"># Self-attention with residual connection</span></span>
<span id="cb5-69"><a href="#cb5-69"></a>        attn_out, attn_weights <span class="op">=</span> <span class="va">self</span>.attn(<span class="va">self</span>.norm1(x))</span>
<span id="cb5-70"><a href="#cb5-70"></a>        x <span class="op">=</span> x <span class="op">+</span> attn_out</span>
<span id="cb5-71"><a href="#cb5-71"></a>        </span>
<span id="cb5-72"><a href="#cb5-72"></a>        <span class="co"># MLP with residual connection</span></span>
<span id="cb5-73"><a href="#cb5-73"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.norm2(x))</span>
<span id="cb5-74"><a href="#cb5-74"></a>        </span>
<span id="cb5-75"><a href="#cb5-75"></a>        <span class="cf">return</span> x, attn_weights</span>
<span id="cb5-76"><a href="#cb5-76"></a></span>
<span id="cb5-77"><a href="#cb5-77"></a><span class="kw">class</span> SimpleViT(nn.Module):</span>
<span id="cb5-78"><a href="#cb5-78"></a>    <span class="co">"""Simplified Vision Transformer"""</span></span>
<span id="cb5-79"><a href="#cb5-79"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_size<span class="op">=</span><span class="dv">224</span>, patch_size<span class="op">=</span><span class="dv">16</span>, num_classes<span class="op">=</span><span class="dv">1000</span>, </span>
<span id="cb5-80"><a href="#cb5-80"></a>                 embed_dim<span class="op">=</span><span class="dv">768</span>, depth<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb5-81"><a href="#cb5-81"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-82"><a href="#cb5-82"></a>        </span>
<span id="cb5-83"><a href="#cb5-83"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb5-84"><a href="#cb5-84"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> PatchEmbedding(img_size, patch_size, <span class="dv">3</span>, embed_dim)</span>
<span id="cb5-85"><a href="#cb5-85"></a>        num_patches <span class="op">=</span> <span class="va">self</span>.patch_embed.num_patches</span>
<span id="cb5-86"><a href="#cb5-86"></a>        </span>
<span id="cb5-87"><a href="#cb5-87"></a>        <span class="co"># Class token and position embedding</span></span>
<span id="cb5-88"><a href="#cb5-88"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, embed_dim))</span>
<span id="cb5-89"><a href="#cb5-89"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, num_patches <span class="op">+</span> <span class="dv">1</span>, embed_dim))</span>
<span id="cb5-90"><a href="#cb5-90"></a>        </span>
<span id="cb5-91"><a href="#cb5-91"></a>        <span class="co"># Transformer blocks</span></span>
<span id="cb5-92"><a href="#cb5-92"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.ModuleList([</span>
<span id="cb5-93"><a href="#cb5-93"></a>            TransformerBlock(embed_dim, num_heads) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth)</span>
<span id="cb5-94"><a href="#cb5-94"></a>        ])</span>
<span id="cb5-95"><a href="#cb5-95"></a>        </span>
<span id="cb5-96"><a href="#cb5-96"></a>        <span class="co"># Classification head</span></span>
<span id="cb5-97"><a href="#cb5-97"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb5-98"><a href="#cb5-98"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(embed_dim, num_classes)</span>
<span id="cb5-99"><a href="#cb5-99"></a>    </span>
<span id="cb5-100"><a href="#cb5-100"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-101"><a href="#cb5-101"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb5-102"><a href="#cb5-102"></a>        </span>
<span id="cb5-103"><a href="#cb5-103"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb5-104"><a href="#cb5-104"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)  <span class="co"># (batch_size, num_patches, embed_dim)</span></span>
<span id="cb5-105"><a href="#cb5-105"></a>        </span>
<span id="cb5-106"><a href="#cb5-106"></a>        <span class="co"># Add class token</span></span>
<span id="cb5-107"><a href="#cb5-107"></a>        cls_tokens <span class="op">=</span> <span class="va">self</span>.cls_token.expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-108"><a href="#cb5-108"></a>        x <span class="op">=</span> torch.cat((cls_tokens, x), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-109"><a href="#cb5-109"></a>        </span>
<span id="cb5-110"><a href="#cb5-110"></a>        <span class="co"># Add position embedding</span></span>
<span id="cb5-111"><a href="#cb5-111"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed</span>
<span id="cb5-112"><a href="#cb5-112"></a>        </span>
<span id="cb5-113"><a href="#cb5-113"></a>        <span class="co"># Apply transformer blocks</span></span>
<span id="cb5-114"><a href="#cb5-114"></a>        attention_maps <span class="op">=</span> []</span>
<span id="cb5-115"><a href="#cb5-115"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.blocks:</span>
<span id="cb5-116"><a href="#cb5-116"></a>            x, attn <span class="op">=</span> block(x)</span>
<span id="cb5-117"><a href="#cb5-117"></a>            attention_maps.append(attn)</span>
<span id="cb5-118"><a href="#cb5-118"></a>        </span>
<span id="cb5-119"><a href="#cb5-119"></a>        <span class="co"># Classification</span></span>
<span id="cb5-120"><a href="#cb5-120"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb5-121"><a href="#cb5-121"></a>        cls_token_final <span class="op">=</span> x[:, <span class="dv">0</span>]  <span class="co"># Use class token for classification</span></span>
<span id="cb5-122"><a href="#cb5-122"></a>        out <span class="op">=</span> <span class="va">self</span>.head(cls_token_final)</span>
<span id="cb5-123"><a href="#cb5-123"></a>        </span>
<span id="cb5-124"><a href="#cb5-124"></a>        <span class="cf">return</span> out, attention_maps</span>
<span id="cb5-125"><a href="#cb5-125"></a></span>
<span id="cb5-126"><a href="#cb5-126"></a><span class="co"># Create a simple ViT</span></span>
<span id="cb5-127"><a href="#cb5-127"></a>simple_vit <span class="op">=</span> SimpleViT(depth<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">8</span>)  <span class="co"># Smaller for demo</span></span>
<span id="cb5-128"><a href="#cb5-128"></a>vit_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> simple_vit.parameters())</span>
<span id="cb5-129"><a href="#cb5-129"></a></span>
<span id="cb5-130"><a href="#cb5-130"></a><span class="bu">print</span>(<span class="ss">f"Simple ViT parameters: </span><span class="sc">{</span>vit_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb5-131"><a href="#cb5-131"></a></span>
<span id="cb5-132"><a href="#cb5-132"></a><span class="co"># Test with dummy input</span></span>
<span id="cb5-133"><a href="#cb5-133"></a>dummy_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb5-134"><a href="#cb5-134"></a>output, attention_maps <span class="op">=</span> simple_vit(dummy_input)</span>
<span id="cb5-135"><a href="#cb5-135"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-136"><a href="#cb5-136"></a><span class="bu">print</span>(<span class="ss">f"Number of attention maps: </span><span class="sc">{</span><span class="bu">len</span>(attention_maps)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Simple ViT parameters: 44,040,424
Output shape: torch.Size([1, 1000])
Number of attention maps: 6</code></pre>
</div>
</div>
</section>
<section id="visualizing-attention-what-does-the-model-look-at" class="level3" data-number="0.3.2">
<h3 data-number="0.3.2" class="anchored" data-anchor-id="visualizing-attention-what-does-the-model-look-at"><span class="header-section-number">0.3.2</span> Visualizing Attention: What Does the Model Look At?</h3>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">def</span> visualize_attention(image, attention_maps, patch_size<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="co">"""Visualize what the vision transformer is looking at"""</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>    </span>
<span id="cb7-4"><a href="#cb7-4"></a>    <span class="co"># Use attention from the last layer, first head</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>    attn <span class="op">=</span> attention_maps[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>, <span class="dv">0</span>]  <span class="co"># (seq_len, seq_len)</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>    </span>
<span id="cb7-7"><a href="#cb7-7"></a>    <span class="co"># Get attention from class token to all patches</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>    cls_attn <span class="op">=</span> attn[<span class="dv">0</span>, <span class="dv">1</span>:]  <span class="co"># Exclude class token to class token attention</span></span>
<span id="cb7-9"><a href="#cb7-9"></a>    </span>
<span id="cb7-10"><a href="#cb7-10"></a>    <span class="co"># Reshape to spatial dimensions</span></span>
<span id="cb7-11"><a href="#cb7-11"></a>    num_patches_per_side <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(cls_attn) <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb7-12"><a href="#cb7-12"></a>    attn_map <span class="op">=</span> cls_attn.reshape(num_patches_per_side, num_patches_per_side)</span>
<span id="cb7-13"><a href="#cb7-13"></a>    </span>
<span id="cb7-14"><a href="#cb7-14"></a>    <span class="co"># Resize to image size</span></span>
<span id="cb7-15"><a href="#cb7-15"></a>    attn_map <span class="op">=</span> torch.nn.functional.interpolate(</span>
<span id="cb7-16"><a href="#cb7-16"></a>        attn_map.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb7-17"><a href="#cb7-17"></a>        size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb7-18"><a href="#cb7-18"></a>        mode<span class="op">=</span><span class="st">'bilinear'</span></span>
<span id="cb7-19"><a href="#cb7-19"></a>    ).squeeze()</span>
<span id="cb7-20"><a href="#cb7-20"></a>    </span>
<span id="cb7-21"><a href="#cb7-21"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb7-22"><a href="#cb7-22"></a>    </span>
<span id="cb7-23"><a href="#cb7-23"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb7-24"><a href="#cb7-24"></a>    plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb7-25"><a href="#cb7-25"></a>    plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb7-26"><a href="#cb7-26"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb7-27"><a href="#cb7-27"></a>    </span>
<span id="cb7-28"><a href="#cb7-28"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb7-29"><a href="#cb7-29"></a>    plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb7-30"><a href="#cb7-30"></a>    plt.imshow(attn_map.detach().numpy(), alpha<span class="op">=</span><span class="fl">0.6</span>, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb7-31"><a href="#cb7-31"></a>    plt.title(<span class="st">"Attention Map"</span>)</span>
<span id="cb7-32"><a href="#cb7-32"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb7-33"><a href="#cb7-33"></a>    </span>
<span id="cb7-34"><a href="#cb7-34"></a>    plt.tight_layout()</span>
<span id="cb7-35"><a href="#cb7-35"></a>    plt.show()</span>
<span id="cb7-36"><a href="#cb7-36"></a></span>
<span id="cb7-37"><a href="#cb7-37"></a><span class="co"># Visualize attention (you would use a real image)</span></span>
<span id="cb7-38"><a href="#cb7-38"></a>dummy_image <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb7-39"><a href="#cb7-39"></a>visualize_attention(dummy_image, attention_maps)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="08-modern-vision-models_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="foundation-models-the-dinov2-revolution" class="level2" data-number="0.4">
<h2 data-number="0.4" class="anchored" data-anchor-id="foundation-models-the-dinov2-revolution"><span class="header-section-number">0.4</span> Foundation Models: The DINOv2 Revolution</h2>
<p>Now we reach the cutting edge: <strong>Foundation Models</strong>. DINOv2 (Distillation with No Labels v2) represents a paradigm shift:</p>
<ul>
<li><strong>Self-supervised learning</strong>: No labels needed!</li>
<li><strong>Universal features</strong>: Works for any vision task</li>
<li><strong>Incredible performance</strong>: Often beats supervised methods</li>
</ul>
<section id="using-dinov2-with-huggingface" class="level3" data-number="0.4.1">
<h3 data-number="0.4.1" class="anchored" data-anchor-id="using-dinov2-with-huggingface"><span class="header-section-number">0.4.1</span> Using DINOv2 with HuggingFace</h3>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Install required packages</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="co"># !pip install transformers torch torchvision</span></span>
<span id="cb8-3"><a href="#cb8-3"></a></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="im">from</span> transformers <span class="im">import</span> AutoImageProcessor, AutoModel</span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="im">import</span> requests</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="co"># Load DINOv2 model and processor</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>model_name <span class="op">=</span> <span class="st">"facebook/dinov2-base"</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>processor <span class="op">=</span> AutoImageProcessor.from_pretrained(model_name)</span>
<span id="cb8-11"><a href="#cb8-11"></a>model <span class="op">=</span> AutoModel.from_pretrained(model_name)</span>
<span id="cb8-12"><a href="#cb8-12"></a></span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="bu">print</span>(<span class="ss">f"Loaded DINOv2 model: </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-14"><a href="#cb8-14"></a><span class="bu">print</span>(<span class="ss">f"Model parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb8-15"><a href="#cb8-15"></a></span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="kw">def</span> extract_dinov2_features(image_path_or_url):</span>
<span id="cb8-17"><a href="#cb8-17"></a>    <span class="co">"""Extract features using DINOv2"""</span></span>
<span id="cb8-18"><a href="#cb8-18"></a>    </span>
<span id="cb8-19"><a href="#cb8-19"></a>    <span class="co"># Load image</span></span>
<span id="cb8-20"><a href="#cb8-20"></a>    <span class="cf">if</span> image_path_or_url.startswith(<span class="st">'http'</span>):</span>
<span id="cb8-21"><a href="#cb8-21"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(image_path_or_url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb8-22"><a href="#cb8-22"></a>    <span class="cf">else</span>:</span>
<span id="cb8-23"><a href="#cb8-23"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(image_path_or_url)</span>
<span id="cb8-24"><a href="#cb8-24"></a>    </span>
<span id="cb8-25"><a href="#cb8-25"></a>    <span class="co"># Process image</span></span>
<span id="cb8-26"><a href="#cb8-26"></a>    inputs <span class="op">=</span> processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb8-27"><a href="#cb8-27"></a>    </span>
<span id="cb8-28"><a href="#cb8-28"></a>    <span class="co"># Extract features</span></span>
<span id="cb8-29"><a href="#cb8-29"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-30"><a href="#cb8-30"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb8-31"><a href="#cb8-31"></a>        features <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb8-32"><a href="#cb8-32"></a>        </span>
<span id="cb8-33"><a href="#cb8-33"></a>        <span class="co"># Get CLS token (global image representation)</span></span>
<span id="cb8-34"><a href="#cb8-34"></a>        cls_features <span class="op">=</span> features[:, <span class="dv">0</span>]  <span class="co"># Shape: (1, 768)</span></span>
<span id="cb8-35"><a href="#cb8-35"></a>        </span>
<span id="cb8-36"><a href="#cb8-36"></a>        <span class="co"># Get patch features (local representations)</span></span>
<span id="cb8-37"><a href="#cb8-37"></a>        patch_features <span class="op">=</span> features[:, <span class="dv">1</span>:]  <span class="co"># Shape: (1, num_patches, 768)</span></span>
<span id="cb8-38"><a href="#cb8-38"></a>    </span>
<span id="cb8-39"><a href="#cb8-39"></a>    <span class="cf">return</span> {</span>
<span id="cb8-40"><a href="#cb8-40"></a>        <span class="st">'cls_features'</span>: cls_features,</span>
<span id="cb8-41"><a href="#cb8-41"></a>        <span class="st">'patch_features'</span>: patch_features,</span>
<span id="cb8-42"><a href="#cb8-42"></a>        <span class="st">'image'</span>: image</span>
<span id="cb8-43"><a href="#cb8-43"></a>    }</span>
<span id="cb8-44"><a href="#cb8-44"></a></span>
<span id="cb8-45"><a href="#cb8-45"></a><span class="co"># Example usage</span></span>
<span id="cb8-46"><a href="#cb8-46"></a><span class="kw">def</span> demo_dinov2_features():</span>
<span id="cb8-47"><a href="#cb8-47"></a>    <span class="co">"""Demonstrate DINOv2 feature extraction"""</span></span>
<span id="cb8-48"><a href="#cb8-48"></a>    </span>
<span id="cb8-49"><a href="#cb8-49"></a>    <span class="co"># Create dummy image for demo (you would use real images)</span></span>
<span id="cb8-50"><a href="#cb8-50"></a>    dummy_image <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb8-51"><a href="#cb8-51"></a>    </span>
<span id="cb8-52"><a href="#cb8-52"></a>    <span class="co"># Save temporarily</span></span>
<span id="cb8-53"><a href="#cb8-53"></a>    dummy_image.save(<span class="st">'temp_image.jpg'</span>)</span>
<span id="cb8-54"><a href="#cb8-54"></a>    </span>
<span id="cb8-55"><a href="#cb8-55"></a>    <span class="co"># Extract features</span></span>
<span id="cb8-56"><a href="#cb8-56"></a>    result <span class="op">=</span> extract_dinov2_features(<span class="st">'temp_image.jpg'</span>)</span>
<span id="cb8-57"><a href="#cb8-57"></a>    </span>
<span id="cb8-58"><a href="#cb8-58"></a>    <span class="bu">print</span>(<span class="st">"DINOv2 Feature Extraction Results:"</span>)</span>
<span id="cb8-59"><a href="#cb8-59"></a>    <span class="bu">print</span>(<span class="ss">f"Global features shape: </span><span class="sc">{</span>result[<span class="st">'cls_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-60"><a href="#cb8-60"></a>    <span class="bu">print</span>(<span class="ss">f"Patch features shape: </span><span class="sc">{</span>result[<span class="st">'patch_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-61"><a href="#cb8-61"></a>    </span>
<span id="cb8-62"><a href="#cb8-62"></a>    <span class="co"># Visualize features</span></span>
<span id="cb8-63"><a href="#cb8-63"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb8-64"><a href="#cb8-64"></a>    </span>
<span id="cb8-65"><a href="#cb8-65"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb8-66"><a href="#cb8-66"></a>    plt.imshow(result[<span class="st">'image'</span>])</span>
<span id="cb8-67"><a href="#cb8-67"></a>    plt.title(<span class="st">"Input Image"</span>)</span>
<span id="cb8-68"><a href="#cb8-68"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb8-69"><a href="#cb8-69"></a>    </span>
<span id="cb8-70"><a href="#cb8-70"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb8-71"><a href="#cb8-71"></a>    plt.plot(result[<span class="st">'cls_features'</span>].squeeze().numpy())</span>
<span id="cb8-72"><a href="#cb8-72"></a>    plt.title(<span class="st">"Global Features (768 dimensions)"</span>)</span>
<span id="cb8-73"><a href="#cb8-73"></a>    plt.xlabel(<span class="st">"Dimension"</span>)</span>
<span id="cb8-74"><a href="#cb8-74"></a>    plt.ylabel(<span class="st">"Value"</span>)</span>
<span id="cb8-75"><a href="#cb8-75"></a>    </span>
<span id="cb8-76"><a href="#cb8-76"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb8-77"><a href="#cb8-77"></a>    <span class="co"># Visualize patch features as heatmap</span></span>
<span id="cb8-78"><a href="#cb8-78"></a>    patch_norms <span class="op">=</span> torch.norm(result[<span class="st">'patch_features'</span>].squeeze(), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-79"><a href="#cb8-79"></a>    patch_size <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(patch_norms) <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb8-80"><a href="#cb8-80"></a>    patch_map <span class="op">=</span> patch_norms.reshape(patch_size, patch_size)</span>
<span id="cb8-81"><a href="#cb8-81"></a>    plt.imshow(patch_map.numpy(), cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb8-82"><a href="#cb8-82"></a>    plt.title(<span class="st">"Patch Feature Magnitudes"</span>)</span>
<span id="cb8-83"><a href="#cb8-83"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb8-84"><a href="#cb8-84"></a>    </span>
<span id="cb8-85"><a href="#cb8-85"></a>    plt.tight_layout()</span>
<span id="cb8-86"><a href="#cb8-86"></a>    plt.show()</span>
<span id="cb8-87"><a href="#cb8-87"></a>    </span>
<span id="cb8-88"><a href="#cb8-88"></a>    <span class="cf">return</span> result</span>
<span id="cb8-89"><a href="#cb8-89"></a></span>
<span id="cb8-90"><a href="#cb8-90"></a>demo_result <span class="op">=</span> demo_dinov2_features()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Loaded DINOv2 model: facebook/dinov2-base
Model parameters: 86,580,480
DINOv2 Feature Extraction Results:
Global features shape: torch.Size([1, 768])
Patch features shape: torch.Size([1, 256, 768])</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="08-modern-vision-models_files/figure-html/cell-6-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="building-a-dinov2-powered-image-similarity-engine" class="level3" data-number="0.4.2">
<h3 data-number="0.4.2" class="anchored" data-anchor-id="building-a-dinov2-powered-image-similarity-engine"><span class="header-section-number">0.4.2</span> Building a DINOv2-Powered Image Similarity Engine</h3>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">class</span> DINOv2SimilarityEngine:</span>
<span id="cb10-2"><a href="#cb10-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb10-3"><a href="#cb10-3"></a>        <span class="va">self</span>.processor <span class="op">=</span> AutoImageProcessor.from_pretrained(<span class="st">"facebook/dinov2-base"</span>)</span>
<span id="cb10-4"><a href="#cb10-4"></a>        <span class="va">self</span>.model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">"facebook/dinov2-base"</span>)</span>
<span id="cb10-5"><a href="#cb10-5"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb10-6"><a href="#cb10-6"></a>        <span class="va">self</span>.image_database <span class="op">=</span> {}</span>
<span id="cb10-7"><a href="#cb10-7"></a>    </span>
<span id="cb10-8"><a href="#cb10-8"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, image):</span>
<span id="cb10-9"><a href="#cb10-9"></a>        <span class="co">"""Extract DINOv2 features from an image"""</span></span>
<span id="cb10-10"><a href="#cb10-10"></a>        inputs <span class="op">=</span> <span class="va">self</span>.processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb10-11"><a href="#cb10-11"></a>        </span>
<span id="cb10-12"><a href="#cb10-12"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-13"><a href="#cb10-13"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(<span class="op">**</span>inputs)</span>
<span id="cb10-14"><a href="#cb10-14"></a>            <span class="co"># Use CLS token as global image representation</span></span>
<span id="cb10-15"><a href="#cb10-15"></a>            features <span class="op">=</span> outputs.last_hidden_state[:, <span class="dv">0</span>]</span>
<span id="cb10-16"><a href="#cb10-16"></a>        </span>
<span id="cb10-17"><a href="#cb10-17"></a>        <span class="cf">return</span> features</span>
<span id="cb10-18"><a href="#cb10-18"></a>    </span>
<span id="cb10-19"><a href="#cb10-19"></a>    <span class="kw">def</span> add_image(<span class="va">self</span>, image_id, image):</span>
<span id="cb10-20"><a href="#cb10-20"></a>        <span class="co">"""Add an image to the database"""</span></span>
<span id="cb10-21"><a href="#cb10-21"></a>        features <span class="op">=</span> <span class="va">self</span>.extract_features(image)</span>
<span id="cb10-22"><a href="#cb10-22"></a>        <span class="va">self</span>.image_database[image_id] <span class="op">=</span> {</span>
<span id="cb10-23"><a href="#cb10-23"></a>            <span class="st">'features'</span>: features,</span>
<span id="cb10-24"><a href="#cb10-24"></a>            <span class="st">'image'</span>: image</span>
<span id="cb10-25"><a href="#cb10-25"></a>        }</span>
<span id="cb10-26"><a href="#cb10-26"></a>        <span class="bu">print</span>(<span class="ss">f"Added image '</span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ss">' to database"</span>)</span>
<span id="cb10-27"><a href="#cb10-27"></a>    </span>
<span id="cb10-28"><a href="#cb10-28"></a>    <span class="kw">def</span> find_similar_images(<span class="va">self</span>, query_image, top_k<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb10-29"><a href="#cb10-29"></a>        <span class="co">"""Find most similar images in the database"""</span></span>
<span id="cb10-30"><a href="#cb10-30"></a>        query_features <span class="op">=</span> <span class="va">self</span>.extract_features(query_image)</span>
<span id="cb10-31"><a href="#cb10-31"></a>        </span>
<span id="cb10-32"><a href="#cb10-32"></a>        similarities <span class="op">=</span> {}</span>
<span id="cb10-33"><a href="#cb10-33"></a>        </span>
<span id="cb10-34"><a href="#cb10-34"></a>        <span class="cf">for</span> image_id, data <span class="kw">in</span> <span class="va">self</span>.image_database.items():</span>
<span id="cb10-35"><a href="#cb10-35"></a>            <span class="co"># Compute cosine similarity</span></span>
<span id="cb10-36"><a href="#cb10-36"></a>            similarity <span class="op">=</span> torch.cosine_similarity(</span>
<span id="cb10-37"><a href="#cb10-37"></a>                query_features, data[<span class="st">'features'</span>], dim<span class="op">=</span><span class="dv">1</span></span>
<span id="cb10-38"><a href="#cb10-38"></a>            ).item()</span>
<span id="cb10-39"><a href="#cb10-39"></a>            similarities[image_id] <span class="op">=</span> similarity</span>
<span id="cb10-40"><a href="#cb10-40"></a>        </span>
<span id="cb10-41"><a href="#cb10-41"></a>        <span class="co"># Sort by similarity</span></span>
<span id="cb10-42"><a href="#cb10-42"></a>        sorted_similarities <span class="op">=</span> <span class="bu">sorted</span>(</span>
<span id="cb10-43"><a href="#cb10-43"></a>            similarities.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span></span>
<span id="cb10-44"><a href="#cb10-44"></a>        )</span>
<span id="cb10-45"><a href="#cb10-45"></a>        </span>
<span id="cb10-46"><a href="#cb10-46"></a>        <span class="cf">return</span> sorted_similarities[:top_k]</span>
<span id="cb10-47"><a href="#cb10-47"></a>    </span>
<span id="cb10-48"><a href="#cb10-48"></a>    <span class="kw">def</span> visualize_results(<span class="va">self</span>, query_image, similar_images):</span>
<span id="cb10-49"><a href="#cb10-49"></a>        <span class="co">"""Visualize similarity search results"""</span></span>
<span id="cb10-50"><a href="#cb10-50"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">8</span>))</span>
<span id="cb10-51"><a href="#cb10-51"></a>        </span>
<span id="cb10-52"><a href="#cb10-52"></a>        <span class="co"># Query image</span></span>
<span id="cb10-53"><a href="#cb10-53"></a>        plt.subplot(<span class="dv">2</span>, <span class="bu">len</span>(similar_images) <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb10-54"><a href="#cb10-54"></a>        plt.imshow(query_image)</span>
<span id="cb10-55"><a href="#cb10-55"></a>        plt.title(<span class="st">"Query Image"</span>)</span>
<span id="cb10-56"><a href="#cb10-56"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb10-57"><a href="#cb10-57"></a>        </span>
<span id="cb10-58"><a href="#cb10-58"></a>        <span class="co"># Similar images</span></span>
<span id="cb10-59"><a href="#cb10-59"></a>        <span class="cf">for</span> i, (image_id, similarity) <span class="kw">in</span> <span class="bu">enumerate</span>(similar_images):</span>
<span id="cb10-60"><a href="#cb10-60"></a>            plt.subplot(<span class="dv">2</span>, <span class="bu">len</span>(similar_images) <span class="op">+</span> <span class="dv">1</span>, i <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb10-61"><a href="#cb10-61"></a>            plt.imshow(<span class="va">self</span>.image_database[image_id][<span class="st">'image'</span>])</span>
<span id="cb10-62"><a href="#cb10-62"></a>            plt.title(<span class="ss">f"</span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ch">\n</span><span class="ss">Similarity: </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-63"><a href="#cb10-63"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb10-64"><a href="#cb10-64"></a>        </span>
<span id="cb10-65"><a href="#cb10-65"></a>        plt.tight_layout()</span>
<span id="cb10-66"><a href="#cb10-66"></a>        plt.show()</span>
<span id="cb10-67"><a href="#cb10-67"></a></span>
<span id="cb10-68"><a href="#cb10-68"></a><span class="co"># Create similarity engine</span></span>
<span id="cb10-69"><a href="#cb10-69"></a>similarity_engine <span class="op">=</span> DINOv2SimilarityEngine()</span>
<span id="cb10-70"><a href="#cb10-70"></a></span>
<span id="cb10-71"><a href="#cb10-71"></a><span class="co"># Demo with dummy images (you would use real images)</span></span>
<span id="cb10-72"><a href="#cb10-72"></a><span class="kw">def</span> demo_similarity_engine():</span>
<span id="cb10-73"><a href="#cb10-73"></a>    <span class="co">"""Demonstrate the similarity engine"""</span></span>
<span id="cb10-74"><a href="#cb10-74"></a>    </span>
<span id="cb10-75"><a href="#cb10-75"></a>    <span class="co"># Create some dummy images with different colors</span></span>
<span id="cb10-76"><a href="#cb10-76"></a>    colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'yellow'</span>, <span class="st">'purple'</span>]</span>
<span id="cb10-77"><a href="#cb10-77"></a>    </span>
<span id="cb10-78"><a href="#cb10-78"></a>    <span class="cf">for</span> color <span class="kw">in</span> colors:</span>
<span id="cb10-79"><a href="#cb10-79"></a>        dummy_img <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span>color)</span>
<span id="cb10-80"><a href="#cb10-80"></a>        similarity_engine.add_image(<span class="ss">f"</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss">_image"</span>, dummy_img)</span>
<span id="cb10-81"><a href="#cb10-81"></a>    </span>
<span id="cb10-82"><a href="#cb10-82"></a>    <span class="co"># Query with a red image</span></span>
<span id="cb10-83"><a href="#cb10-83"></a>    query_img <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb10-84"><a href="#cb10-84"></a>    </span>
<span id="cb10-85"><a href="#cb10-85"></a>    <span class="co"># Find similar images</span></span>
<span id="cb10-86"><a href="#cb10-86"></a>    similar <span class="op">=</span> similarity_engine.find_similar_images(query_img, top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb10-87"><a href="#cb10-87"></a>    </span>
<span id="cb10-88"><a href="#cb10-88"></a>    <span class="bu">print</span>(<span class="st">"Most similar images:"</span>)</span>
<span id="cb10-89"><a href="#cb10-89"></a>    <span class="cf">for</span> image_id, similarity <span class="kw">in</span> similar:</span>
<span id="cb10-90"><a href="#cb10-90"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-91"><a href="#cb10-91"></a>    </span>
<span id="cb10-92"><a href="#cb10-92"></a>    <span class="co"># Visualize results</span></span>
<span id="cb10-93"><a href="#cb10-93"></a>    similarity_engine.visualize_results(query_img, similar)</span>
<span id="cb10-94"><a href="#cb10-94"></a></span>
<span id="cb10-95"><a href="#cb10-95"></a>demo_similarity_engine()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Added image 'red_image' to database
Added image 'blue_image' to database
Added image 'green_image' to database
Added image 'yellow_image' to database
Added image 'purple_image' to database
Most similar images:
  red_image: 1.000
  yellow_image: 0.871
  green_image: 0.858</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="08-modern-vision-models_files/figure-html/cell-7-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="comparing-all-approaches-the-ultimate-showdown" class="level2" data-number="0.5">
<h2 data-number="0.5" class="anchored" data-anchor-id="comparing-all-approaches-the-ultimate-showdown"><span class="header-section-number">0.5</span> Comparing All Approaches: The Ultimate Showdown</h2>
<p>Let’s compare all the approaches we’ve learned:</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">class</span> VisionModelComparison:</span>
<span id="cb12-2"><a href="#cb12-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb12-3"><a href="#cb12-3"></a>        <span class="va">self</span>.models <span class="op">=</span> {</span>
<span id="cb12-4"><a href="#cb12-4"></a>            <span class="st">'ResNet-18'</span>: models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-5"><a href="#cb12-5"></a>            <span class="st">'ResNet-50'</span>: models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-6"><a href="#cb12-6"></a>            <span class="st">'EfficientNet-B0'</span>: efficientnet_b0(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-7"><a href="#cb12-7"></a>            <span class="st">'ViT-Base'</span>: <span class="va">None</span>,  <span class="co"># Would load from transformers</span></span>
<span id="cb12-8"><a href="#cb12-8"></a>            <span class="st">'DINOv2-Base'</span>: <span class="va">None</span>  <span class="co"># Already loaded above</span></span>
<span id="cb12-9"><a href="#cb12-9"></a>        }</span>
<span id="cb12-10"><a href="#cb12-10"></a>    </span>
<span id="cb12-11"><a href="#cb12-11"></a>    <span class="kw">def</span> compare_models(<span class="va">self</span>):</span>
<span id="cb12-12"><a href="#cb12-12"></a>        <span class="co">"""Compare different vision models"""</span></span>
<span id="cb12-13"><a href="#cb12-13"></a>        comparison_data <span class="op">=</span> []</span>
<span id="cb12-14"><a href="#cb12-14"></a>        </span>
<span id="cb12-15"><a href="#cb12-15"></a>        <span class="cf">for</span> name, model <span class="kw">in</span> <span class="va">self</span>.models.items():</span>
<span id="cb12-16"><a href="#cb12-16"></a>            <span class="cf">if</span> model <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-17"><a href="#cb12-17"></a>                params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb12-18"><a href="#cb12-18"></a>                size_mb <span class="op">=</span> params <span class="op">*</span> <span class="dv">4</span> <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)</span>
<span id="cb12-19"><a href="#cb12-19"></a>                </span>
<span id="cb12-20"><a href="#cb12-20"></a>                comparison_data.append({</span>
<span id="cb12-21"><a href="#cb12-21"></a>                    <span class="st">'Model'</span>: name,</span>
<span id="cb12-22"><a href="#cb12-22"></a>                    <span class="st">'Parameters (M)'</span>: <span class="ss">f"</span><span class="sc">{</span>params <span class="op">/</span> <span class="fl">1e6</span><span class="sc">:.1f}</span><span class="ss">"</span>,</span>
<span id="cb12-23"><a href="#cb12-23"></a>                    <span class="st">'Size (MB)'</span>: <span class="ss">f"</span><span class="sc">{</span>size_mb<span class="sc">:.1f}</span><span class="ss">"</span>,</span>
<span id="cb12-24"><a href="#cb12-24"></a>                    <span class="st">'Year'</span>: <span class="va">self</span>.get_year(name),</span>
<span id="cb12-25"><a href="#cb12-25"></a>                    <span class="st">'Type'</span>: <span class="va">self</span>.get_type(name)</span>
<span id="cb12-26"><a href="#cb12-26"></a>                })</span>
<span id="cb12-27"><a href="#cb12-27"></a>        </span>
<span id="cb12-28"><a href="#cb12-28"></a>        <span class="cf">return</span> comparison_data</span>
<span id="cb12-29"><a href="#cb12-29"></a>    </span>
<span id="cb12-30"><a href="#cb12-30"></a>    <span class="kw">def</span> get_year(<span class="va">self</span>, name):</span>
<span id="cb12-31"><a href="#cb12-31"></a>        year_map <span class="op">=</span> {</span>
<span id="cb12-32"><a href="#cb12-32"></a>            <span class="st">'ResNet-18'</span>: <span class="dv">2015</span>,</span>
<span id="cb12-33"><a href="#cb12-33"></a>            <span class="st">'ResNet-50'</span>: <span class="dv">2015</span>,</span>
<span id="cb12-34"><a href="#cb12-34"></a>            <span class="st">'EfficientNet-B0'</span>: <span class="dv">2019</span>,</span>
<span id="cb12-35"><a href="#cb12-35"></a>            <span class="st">'ViT-Base'</span>: <span class="dv">2020</span>,</span>
<span id="cb12-36"><a href="#cb12-36"></a>            <span class="st">'DINOv2-Base'</span>: <span class="dv">2023</span></span>
<span id="cb12-37"><a href="#cb12-37"></a>        }</span>
<span id="cb12-38"><a href="#cb12-38"></a>        <span class="cf">return</span> year_map.get(name, <span class="st">'Unknown'</span>)</span>
<span id="cb12-39"><a href="#cb12-39"></a>    </span>
<span id="cb12-40"><a href="#cb12-40"></a>    <span class="kw">def</span> get_type(<span class="va">self</span>, name):</span>
<span id="cb12-41"><a href="#cb12-41"></a>        <span class="cf">if</span> <span class="st">'ResNet'</span> <span class="kw">in</span> name <span class="kw">or</span> <span class="st">'EfficientNet'</span> <span class="kw">in</span> name:</span>
<span id="cb12-42"><a href="#cb12-42"></a>            <span class="cf">return</span> <span class="st">'CNN'</span></span>
<span id="cb12-43"><a href="#cb12-43"></a>        <span class="cf">elif</span> <span class="st">'ViT'</span> <span class="kw">in</span> name:</span>
<span id="cb12-44"><a href="#cb12-44"></a>            <span class="cf">return</span> <span class="st">'Transformer'</span></span>
<span id="cb12-45"><a href="#cb12-45"></a>        <span class="cf">elif</span> <span class="st">'DINOv2'</span> <span class="kw">in</span> name:</span>
<span id="cb12-46"><a href="#cb12-46"></a>            <span class="cf">return</span> <span class="st">'Foundation Model'</span></span>
<span id="cb12-47"><a href="#cb12-47"></a>        <span class="cf">return</span> <span class="st">'Unknown'</span></span>
<span id="cb12-48"><a href="#cb12-48"></a>    </span>
<span id="cb12-49"><a href="#cb12-49"></a>    <span class="kw">def</span> visualize_comparison(<span class="va">self</span>, data):</span>
<span id="cb12-50"><a href="#cb12-50"></a>        <span class="co">"""Visualize model comparison"""</span></span>
<span id="cb12-51"><a href="#cb12-51"></a>        <span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-52"><a href="#cb12-52"></a>        </span>
<span id="cb12-53"><a href="#cb12-53"></a>        df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb12-54"><a href="#cb12-54"></a>        </span>
<span id="cb12-55"><a href="#cb12-55"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb12-56"><a href="#cb12-56"></a>        </span>
<span id="cb12-57"><a href="#cb12-57"></a>        <span class="co"># Parameters vs Year</span></span>
<span id="cb12-58"><a href="#cb12-58"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb12-59"><a href="#cb12-59"></a>        <span class="cf">for</span> model_type <span class="kw">in</span> df[<span class="st">'Type'</span>].unique():</span>
<span id="cb12-60"><a href="#cb12-60"></a>            subset <span class="op">=</span> df[df[<span class="st">'Type'</span>] <span class="op">==</span> model_type]</span>
<span id="cb12-61"><a href="#cb12-61"></a>            plt.scatter(subset[<span class="st">'Year'</span>], subset[<span class="st">'Parameters (M)'</span>].astype(<span class="bu">float</span>), </span>
<span id="cb12-62"><a href="#cb12-62"></a>                       label<span class="op">=</span>model_type, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb12-63"><a href="#cb12-63"></a>        </span>
<span id="cb12-64"><a href="#cb12-64"></a>        plt.xlabel(<span class="st">'Year'</span>)</span>
<span id="cb12-65"><a href="#cb12-65"></a>        plt.ylabel(<span class="st">'Parameters (Millions)'</span>)</span>
<span id="cb12-66"><a href="#cb12-66"></a>        plt.title(<span class="st">'Model Size Evolution'</span>)</span>
<span id="cb12-67"><a href="#cb12-67"></a>        plt.legend()</span>
<span id="cb12-68"><a href="#cb12-68"></a>        plt.grid(<span class="va">True</span>)</span>
<span id="cb12-69"><a href="#cb12-69"></a>        </span>
<span id="cb12-70"><a href="#cb12-70"></a>        <span class="co"># Model types distribution</span></span>
<span id="cb12-71"><a href="#cb12-71"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb12-72"><a href="#cb12-72"></a>        type_counts <span class="op">=</span> df[<span class="st">'Type'</span>].value_counts()</span>
<span id="cb12-73"><a href="#cb12-73"></a>        plt.pie(type_counts.values, labels<span class="op">=</span>type_counts.index, autopct<span class="op">=</span><span class="st">'</span><span class="sc">%1.1f%%</span><span class="st">'</span>)</span>
<span id="cb12-74"><a href="#cb12-74"></a>        plt.title(<span class="st">'Model Types Distribution'</span>)</span>
<span id="cb12-75"><a href="#cb12-75"></a>        </span>
<span id="cb12-76"><a href="#cb12-76"></a>        <span class="co"># Size comparison</span></span>
<span id="cb12-77"><a href="#cb12-77"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb12-78"><a href="#cb12-78"></a>        plt.bar(df[<span class="st">'Model'</span>], df[<span class="st">'Size (MB)'</span>].astype(<span class="bu">float</span>))</span>
<span id="cb12-79"><a href="#cb12-79"></a>        plt.xticks(rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb12-80"><a href="#cb12-80"></a>        plt.ylabel(<span class="st">'Size (MB)'</span>)</span>
<span id="cb12-81"><a href="#cb12-81"></a>        plt.title(<span class="st">'Model Size Comparison'</span>)</span>
<span id="cb12-82"><a href="#cb12-82"></a>        </span>
<span id="cb12-83"><a href="#cb12-83"></a>        <span class="co"># Timeline</span></span>
<span id="cb12-84"><a href="#cb12-84"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb12-85"><a href="#cb12-85"></a>        timeline_data <span class="op">=</span> df.sort_values(<span class="st">'Year'</span>)</span>
<span id="cb12-86"><a href="#cb12-86"></a>        plt.plot(timeline_data[<span class="st">'Year'</span>], <span class="bu">range</span>(<span class="bu">len</span>(timeline_data)), <span class="st">'o-'</span>)</span>
<span id="cb12-87"><a href="#cb12-87"></a>        <span class="cf">for</span> i, (idx, row) <span class="kw">in</span> <span class="bu">enumerate</span>(timeline_data.iterrows()):</span>
<span id="cb12-88"><a href="#cb12-88"></a>            plt.annotate(row[<span class="st">'Model'</span>], (row[<span class="st">'Year'</span>], i), </span>
<span id="cb12-89"><a href="#cb12-89"></a>                        xytext<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">0</span>), textcoords<span class="op">=</span><span class="st">'offset points'</span>)</span>
<span id="cb12-90"><a href="#cb12-90"></a>        plt.xlabel(<span class="st">'Year'</span>)</span>
<span id="cb12-91"><a href="#cb12-91"></a>        plt.ylabel(<span class="st">'Model Index'</span>)</span>
<span id="cb12-92"><a href="#cb12-92"></a>        plt.title(<span class="st">'Vision Models Timeline'</span>)</span>
<span id="cb12-93"><a href="#cb12-93"></a>        plt.grid(<span class="va">True</span>)</span>
<span id="cb12-94"><a href="#cb12-94"></a>        </span>
<span id="cb12-95"><a href="#cb12-95"></a>        plt.tight_layout()</span>
<span id="cb12-96"><a href="#cb12-96"></a>        plt.show()</span>
<span id="cb12-97"><a href="#cb12-97"></a></span>
<span id="cb12-98"><a href="#cb12-98"></a><span class="co"># Run comparison</span></span>
<span id="cb12-99"><a href="#cb12-99"></a>comparison <span class="op">=</span> VisionModelComparison()</span>
<span id="cb12-100"><a href="#cb12-100"></a>comparison_data <span class="op">=</span> comparison.compare_models()</span>
<span id="cb12-101"><a href="#cb12-101"></a></span>
<span id="cb12-102"><a href="#cb12-102"></a><span class="bu">print</span>(<span class="st">"Vision Models Comparison:"</span>)</span>
<span id="cb12-103"><a href="#cb12-103"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">80</span>)</span>
<span id="cb12-104"><a href="#cb12-104"></a><span class="cf">for</span> data <span class="kw">in</span> comparison_data:</span>
<span id="cb12-105"><a href="#cb12-105"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>data[<span class="st">'Model'</span>]<span class="sc">:15}</span><span class="ss"> | </span><span class="sc">{</span>data[<span class="st">'Year'</span>]<span class="sc">}</span><span class="ss"> | </span><span class="sc">{</span>data[<span class="st">'Type'</span>]<span class="sc">:15}</span><span class="ss"> | "</span></span>
<span id="cb12-106"><a href="#cb12-106"></a>          <span class="ss">f"</span><span class="sc">{</span>data[<span class="st">'Parameters (M)'</span>]<span class="sc">:&gt;8}</span><span class="ss"> M | </span><span class="sc">{</span>data[<span class="st">'Size (MB)'</span>]<span class="sc">:&gt;8}</span><span class="ss"> MB"</span>)</span>
<span id="cb12-107"><a href="#cb12-107"></a></span>
<span id="cb12-108"><a href="#cb12-108"></a>comparison.visualize_comparison(comparison_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Vision Models Comparison:
--------------------------------------------------------------------------------
ResNet-18       | 2015 | CNN             |     11.7 M |     44.6 MB
ResNet-50       | 2015 | CNN             |     25.6 M |     97.5 MB
EfficientNet-B0 | 2019 | CNN             |      5.3 M |     20.2 MB</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="08-modern-vision-models_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="the-future-whats-next" class="level2" data-number="0.6">
<h2 data-number="0.6" class="anchored" data-anchor-id="the-future-whats-next"><span class="header-section-number">0.6</span> The Future: What’s Next?</h2>
<p>As we look ahead, several trends are shaping the future of computer vision:</p>
<section id="multimodal-foundation-models" class="level3" data-number="0.6.1">
<h3 data-number="0.6.1" class="anchored" data-anchor-id="multimodal-foundation-models"><span class="header-section-number">0.6.1</span> 1. <strong>Multimodal Foundation Models</strong></h3>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># Future: Models that understand both vision and language</span></span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="co"># Example: CLIP, GPT-4V, LLaVA</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="efficient-architectures" class="level3" data-number="0.6.2">
<h3 data-number="0.6.2" class="anchored" data-anchor-id="efficient-architectures"><span class="header-section-number">0.6.2</span> 2. <strong>Efficient Architectures</strong></h3>
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># Trend: Smaller, faster models for mobile devices</span></span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="co"># Example: MobileViT, EfficientViT</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="self-supervised-learning" class="level3" data-number="0.6.3">
<h3 data-number="0.6.3" class="anchored" data-anchor-id="self-supervised-learning"><span class="header-section-number">0.6.3</span> 3. <strong>Self-Supervised Learning</strong></h3>
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># Growing trend: Learning without labels</span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="co"># Example: MAE, SimCLR, DINOv2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="your-challenge-build-a-modern-vision-pipeline" class="level2" data-number="0.7">
<h2 data-number="0.7" class="anchored" data-anchor-id="your-challenge-build-a-modern-vision-pipeline"><span class="header-section-number">0.7</span> Your Challenge: Build a Modern Vision Pipeline</h2>
<p>Now it’s your turn to build a complete modern vision system:</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="kw">class</span> ModernVisionPipeline:</span>
<span id="cb17-2"><a href="#cb17-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb17-3"><a href="#cb17-3"></a>        <span class="co"># Load multiple models for different tasks</span></span>
<span id="cb17-4"><a href="#cb17-4"></a>        <span class="va">self</span>.classification_model <span class="op">=</span> models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-5"><a href="#cb17-5"></a>        <span class="va">self</span>.feature_extractor <span class="op">=</span> <span class="va">None</span>  <span class="co"># DINOv2 model</span></span>
<span id="cb17-6"><a href="#cb17-6"></a>        <span class="va">self</span>.similarity_engine <span class="op">=</span> DINOv2SimilarityEngine()</span>
<span id="cb17-7"><a href="#cb17-7"></a>    </span>
<span id="cb17-8"><a href="#cb17-8"></a>    <span class="kw">def</span> classify_image(<span class="va">self</span>, image):</span>
<span id="cb17-9"><a href="#cb17-9"></a>        <span class="co">"""Classify image using ResNet"""</span></span>
<span id="cb17-10"><a href="#cb17-10"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb17-11"><a href="#cb17-11"></a>        <span class="cf">pass</span></span>
<span id="cb17-12"><a href="#cb17-12"></a>    </span>
<span id="cb17-13"><a href="#cb17-13"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, image):</span>
<span id="cb17-14"><a href="#cb17-14"></a>        <span class="co">"""Extract features using DINOv2"""</span></span>
<span id="cb17-15"><a href="#cb17-15"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb17-16"><a href="#cb17-16"></a>        <span class="cf">pass</span></span>
<span id="cb17-17"><a href="#cb17-17"></a>    </span>
<span id="cb17-18"><a href="#cb17-18"></a>    <span class="kw">def</span> find_similar_images(<span class="va">self</span>, query_image, database):</span>
<span id="cb17-19"><a href="#cb17-19"></a>        <span class="co">"""Find similar images using DINOv2 features"""</span></span>
<span id="cb17-20"><a href="#cb17-20"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb17-21"><a href="#cb17-21"></a>        <span class="cf">pass</span></span>
<span id="cb17-22"><a href="#cb17-22"></a>    </span>
<span id="cb17-23"><a href="#cb17-23"></a>    <span class="kw">def</span> analyze_image(<span class="va">self</span>, image):</span>
<span id="cb17-24"><a href="#cb17-24"></a>        <span class="co">"""Complete image analysis pipeline"""</span></span>
<span id="cb17-25"><a href="#cb17-25"></a>        results <span class="op">=</span> {</span>
<span id="cb17-26"><a href="#cb17-26"></a>            <span class="st">'classification'</span>: <span class="va">self</span>.classify_image(image),</span>
<span id="cb17-27"><a href="#cb17-27"></a>            <span class="st">'features'</span>: <span class="va">self</span>.extract_features(image),</span>
<span id="cb17-28"><a href="#cb17-28"></a>            <span class="st">'similar_images'</span>: <span class="va">self</span>.find_similar_images(image, <span class="va">self</span>.image_database)</span>
<span id="cb17-29"><a href="#cb17-29"></a>        }</span>
<span id="cb17-30"><a href="#cb17-30"></a>        <span class="cf">return</span> results</span>
<span id="cb17-31"><a href="#cb17-31"></a></span>
<span id="cb17-32"><a href="#cb17-32"></a><span class="co"># Build your pipeline!</span></span>
<span id="cb17-33"><a href="#cb17-33"></a>pipeline <span class="op">=</span> ModernVisionPipeline()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="whats-coming-next" class="level2" data-number="0.8">
<h2 data-number="0.8" class="anchored" data-anchor-id="whats-coming-next"><span class="header-section-number">0.8</span> What’s Coming Next?</h2>
<p>In our next post, <a href="../08-first-cv-project/"><strong>“Your First CV Project: Putting It All Together”</strong></a>, we’ll:</p>
<ul>
<li><strong>Build a complete computer vision application</strong></li>
<li><strong>Combine classical and modern techniques</strong></li>
<li><strong>Deploy your model for real-world use</strong></li>
<li><strong>Create an interactive demo</strong></li>
</ul>
<p>You’ve just learned about the most advanced vision models ever created—next, we’ll put everything together into a real project!</p>
</section>
<section id="key-takeaways" class="level2" data-number="0.9">
<h2 data-number="0.9" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">0.9</span> Key Takeaways</h2>
<ul>
<li><strong>CNNs dominated</strong> computer vision for a decade</li>
<li><strong>Vision Transformers</strong> brought attention mechanisms to vision</li>
<li><strong>Foundation models</strong> like DINOv2 learn universal representations</li>
<li><strong>Self-supervised learning</strong> eliminates the need for labels</li>
<li><strong>Modern pipelines</strong> combine multiple approaches</li>
<li><strong>The field evolves rapidly</strong>—stay curious and keep learning!</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hands-On Lab
</div>
</div>
<div class="callout-body-container callout-body">
<p>Ready to experiment with cutting-edge vision models? Try the complete interactive notebook: <a href="https://colab.research.google.com/drive/1Modern_Vision_Models_123456"><strong>Modern Vision Models Lab</strong></a></p>
<p>Compare CNNs, Vision Transformers, and DINOv2 on your own images!</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Series Navigation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Previous</strong>: <a href="../../../posts/series/cv-foundations/07-why-deep-learning.html">Why Deep Learning? When Classical Methods Hit the Wall</a></li>
<li><strong>Next</strong>: <a href="../../../posts/series/cv-foundations/09-first-cv-project.html">Your First CV Project: Putting It All Together</a></li>
<li><strong>Series Home</strong>: <a href="../../../posts/series/computer-vision-foundations.html">Computer Vision Foundations</a></li>
</ul>
</div>
</div>
<hr>
<p><em>You’ve just explored the cutting edge of computer vision! From ResNet’s skip connections to DINOv2’s self-supervised learning—you now understand the models that power today’s AI applications. Next, we’ll build something amazing with all this knowledge!</em></p>


<!-- -->

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div>CC BY-NC-SA 4.0</div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{2025,
  author = {, Hasan},
  title = {Modern {Vision} {Models:} {CNNs,} {Vision} {Transformers,}
    and {DINOv2}},
  date = {2025-01-22},
  url = {https://hasangoni.quarto.pub/hasan-blog-post/posts/series/cv-foundations/08-modern-vision-models.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Hasan. 2025. <span>“Modern Vision Models: CNNs, Vision Transformers, and
DINOv2.”</span> January 22, 2025. <a href="https://hasangoni.quarto.pub/hasan-blog-post/posts/series/cv-foundations/08-modern-vision-models.html">https://hasangoni.quarto.pub/hasan-blog-post/posts/series/cv-foundations/08-modern-vision-models.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="yourusername/quarto_blog_hasan" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<script src="https://giscus.app/client.js" data-repo="HasanGoni/quarto_blog_hasan" data-repo-id="" data-category="General" data-category-id="" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb18" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb18-1"><a href="#cb18-1"></a><span class="co">---</span></span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="an">title:</span><span class="co"> "Modern Vision Models: CNNs, Vision Transformers, and DINOv2"</span></span>
<span id="cb18-3"><a href="#cb18-3"></a><span class="an">author:</span><span class="co"> "Hasan"</span></span>
<span id="cb18-4"><a href="#cb18-4"></a><span class="an">date:</span><span class="co"> 2025-01-22</span></span>
<span id="cb18-5"><a href="#cb18-5"></a><span class="an">categories:</span><span class="co"> [computer-vision, transformers, foundation-models, dinov2]</span></span>
<span id="cb18-6"><a href="#cb18-6"></a><span class="an">tags:</span><span class="co"> [cnn, vision-transformer, dinov2, self-supervised, huggingface]</span></span>
<span id="cb18-7"><a href="#cb18-7"></a><span class="an">image:</span><span class="co"> "https://images.unsplash.com/photo-1620712943543-bcc4688e7485?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2065&amp;q=80"</span></span>
<span id="cb18-8"><a href="#cb18-8"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb18-9"><a href="#cb18-9"></a><span class="an">series:</span></span>
<span id="cb18-10"><a href="#cb18-10"></a><span class="co">  name: "Computer Vision Foundations"</span></span>
<span id="cb18-11"><a href="#cb18-11"></a><span class="co">  number: 8</span></span>
<span id="cb18-12"><a href="#cb18-12"></a><span class="an">format:</span></span>
<span id="cb18-13"><a href="#cb18-13"></a><span class="co">  html: default</span></span>
<span id="cb18-14"><a href="#cb18-14"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb18-15"><a href="#cb18-15"></a><span class="co">---</span></span>
<span id="cb18-16"><a href="#cb18-16"></a></span>
<span id="cb18-17"><a href="#cb18-17"></a><span class="fu">## The Evolution of Vision: From AlexNet to DINOv2</span></span>
<span id="cb18-18"><a href="#cb18-18"></a></span>
<span id="cb18-19"><a href="#cb18-19"></a>Remember when we thought AlexNet was revolutionary in 2012? That was just the beginning! In the past decade, computer vision has evolved at breakneck speed:</span>
<span id="cb18-20"><a href="#cb18-20"></a></span>
<span id="cb18-21"><a href="#cb18-21"></a><span class="ss">- </span>**2012**: AlexNet - 8 layers, 60M parameters</span>
<span id="cb18-22"><a href="#cb18-22"></a><span class="ss">- </span>**2015**: ResNet - 152 layers, skip connections</span>
<span id="cb18-23"><a href="#cb18-23"></a><span class="ss">- </span>**2017**: Attention mechanisms emerge</span>
<span id="cb18-24"><a href="#cb18-24"></a><span class="ss">- </span>**2020**: Vision Transformers - "Attention is all you need" for vision</span>
<span id="cb18-25"><a href="#cb18-25"></a><span class="ss">- </span>**2023**: DINOv2 - Foundation models that understand everything</span>
<span id="cb18-26"><a href="#cb18-26"></a></span>
<span id="cb18-27"><a href="#cb18-27"></a>Today, we're going to explore this incredible journey and show you how to use the most powerful vision models ever created!</span>
<span id="cb18-28"><a href="#cb18-28"></a></span>
<span id="cb18-29"><a href="#cb18-29"></a><span class="fu">## The CNN Dynasty: ResNet, EfficientNet, and Beyond</span></span>
<span id="cb18-30"><a href="#cb18-30"></a></span>
<span id="cb18-31"><a href="#cb18-31"></a>Before transformers took over, CNNs ruled the vision world. Let's explore the key innovations:</span>
<span id="cb18-32"><a href="#cb18-32"></a></span>
<span id="cb18-33"><a href="#cb18-33"></a><span class="fu">### ResNet: The Skip Connection Revolution</span></span>
<span id="cb18-34"><a href="#cb18-34"></a></span>
<span id="cb18-37"><a href="#cb18-37"></a><span class="in">```{python}</span></span>
<span id="cb18-38"><a href="#cb18-38"></a><span class="co">#| eval: true</span></span>
<span id="cb18-39"><a href="#cb18-39"></a><span class="im">import</span> torch</span>
<span id="cb18-40"><a href="#cb18-40"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb18-41"><a href="#cb18-41"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb18-42"><a href="#cb18-42"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-43"><a href="#cb18-43"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-44"><a href="#cb18-44"></a></span>
<span id="cb18-45"><a href="#cb18-45"></a><span class="co"># Understanding ResNet's key innovation: skip connections</span></span>
<span id="cb18-46"><a href="#cb18-46"></a><span class="kw">class</span> ResidualBlock(nn.Module):</span>
<span id="cb18-47"><a href="#cb18-47"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, stride<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb18-48"><a href="#cb18-48"></a>        <span class="bu">super</span>(ResidualBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb18-49"><a href="#cb18-49"></a>        </span>
<span id="cb18-50"><a href="#cb18-50"></a>        <span class="co"># Main path</span></span>
<span id="cb18-51"><a href="#cb18-51"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_channels, out_channels, <span class="dv">3</span>, stride, <span class="dv">1</span>)</span>
<span id="cb18-52"><a href="#cb18-52"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb18-53"><a href="#cb18-53"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(out_channels, out_channels, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-54"><a href="#cb18-54"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> nn.BatchNorm2d(out_channels)</span>
<span id="cb18-55"><a href="#cb18-55"></a>        </span>
<span id="cb18-56"><a href="#cb18-56"></a>        <span class="co"># Skip connection (the magic!)</span></span>
<span id="cb18-57"><a href="#cb18-57"></a>        <span class="va">self</span>.skip <span class="op">=</span> nn.Sequential()</span>
<span id="cb18-58"><a href="#cb18-58"></a>        <span class="cf">if</span> stride <span class="op">!=</span> <span class="dv">1</span> <span class="kw">or</span> in_channels <span class="op">!=</span> out_channels:</span>
<span id="cb18-59"><a href="#cb18-59"></a>            <span class="va">self</span>.skip <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-60"><a href="#cb18-60"></a>                nn.Conv2d(in_channels, out_channels, <span class="dv">1</span>, stride),</span>
<span id="cb18-61"><a href="#cb18-61"></a>                nn.BatchNorm2d(out_channels)</span>
<span id="cb18-62"><a href="#cb18-62"></a>            )</span>
<span id="cb18-63"><a href="#cb18-63"></a>    </span>
<span id="cb18-64"><a href="#cb18-64"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-65"><a href="#cb18-65"></a>        <span class="co"># Main path</span></span>
<span id="cb18-66"><a href="#cb18-66"></a>        out <span class="op">=</span> torch.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x)))</span>
<span id="cb18-67"><a href="#cb18-67"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(<span class="va">self</span>.conv2(out))</span>
<span id="cb18-68"><a href="#cb18-68"></a>        </span>
<span id="cb18-69"><a href="#cb18-69"></a>        <span class="co"># Add skip connection (this is the key!)</span></span>
<span id="cb18-70"><a href="#cb18-70"></a>        out <span class="op">+=</span> <span class="va">self</span>.skip(x)</span>
<span id="cb18-71"><a href="#cb18-71"></a>        out <span class="op">=</span> torch.relu(out)</span>
<span id="cb18-72"><a href="#cb18-72"></a>        </span>
<span id="cb18-73"><a href="#cb18-73"></a>        <span class="cf">return</span> out</span>
<span id="cb18-74"><a href="#cb18-74"></a></span>
<span id="cb18-75"><a href="#cb18-75"></a><span class="co"># Create a simple ResNet-like model</span></span>
<span id="cb18-76"><a href="#cb18-76"></a><span class="kw">class</span> MiniResNet(nn.Module):</span>
<span id="cb18-77"><a href="#cb18-77"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb18-78"><a href="#cb18-78"></a>        <span class="bu">super</span>(MiniResNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb18-79"><a href="#cb18-79"></a>        </span>
<span id="cb18-80"><a href="#cb18-80"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb18-81"><a href="#cb18-81"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm2d(<span class="dv">64</span>)</span>
<span id="cb18-82"><a href="#cb18-82"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb18-83"><a href="#cb18-83"></a>        </span>
<span id="cb18-84"><a href="#cb18-84"></a>        <span class="co"># Stack residual blocks</span></span>
<span id="cb18-85"><a href="#cb18-85"></a>        <span class="va">self</span>.layer1 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb18-86"><a href="#cb18-86"></a>        <span class="va">self</span>.layer2 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-87"><a href="#cb18-87"></a>        <span class="va">self</span>.layer3 <span class="op">=</span> <span class="va">self</span>._make_layer(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-88"><a href="#cb18-88"></a>        </span>
<span id="cb18-89"><a href="#cb18-89"></a>        <span class="va">self</span>.avgpool <span class="op">=</span> nn.AdaptiveAvgPool2d((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb18-90"><a href="#cb18-90"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">256</span>, num_classes)</span>
<span id="cb18-91"><a href="#cb18-91"></a>    </span>
<span id="cb18-92"><a href="#cb18-92"></a>    <span class="kw">def</span> _make_layer(<span class="va">self</span>, in_channels, out_channels, num_blocks, stride):</span>
<span id="cb18-93"><a href="#cb18-93"></a>        layers <span class="op">=</span> []</span>
<span id="cb18-94"><a href="#cb18-94"></a>        layers.append(ResidualBlock(in_channels, out_channels, stride))</span>
<span id="cb18-95"><a href="#cb18-95"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_blocks):</span>
<span id="cb18-96"><a href="#cb18-96"></a>            layers.append(ResidualBlock(out_channels, out_channels))</span>
<span id="cb18-97"><a href="#cb18-97"></a>        <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb18-98"><a href="#cb18-98"></a>    </span>
<span id="cb18-99"><a href="#cb18-99"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-100"><a href="#cb18-100"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(torch.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x))))</span>
<span id="cb18-101"><a href="#cb18-101"></a>        x <span class="op">=</span> <span class="va">self</span>.layer1(x)</span>
<span id="cb18-102"><a href="#cb18-102"></a>        x <span class="op">=</span> <span class="va">self</span>.layer2(x)</span>
<span id="cb18-103"><a href="#cb18-103"></a>        x <span class="op">=</span> <span class="va">self</span>.layer3(x)</span>
<span id="cb18-104"><a href="#cb18-104"></a>        x <span class="op">=</span> <span class="va">self</span>.avgpool(x)</span>
<span id="cb18-105"><a href="#cb18-105"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb18-106"><a href="#cb18-106"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb18-107"><a href="#cb18-107"></a>        <span class="cf">return</span> x</span>
<span id="cb18-108"><a href="#cb18-108"></a></span>
<span id="cb18-109"><a href="#cb18-109"></a><span class="co"># Compare with official ResNet</span></span>
<span id="cb18-110"><a href="#cb18-110"></a>mini_resnet <span class="op">=</span> MiniResNet(num_classes<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb18-111"><a href="#cb18-111"></a>official_resnet <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-112"><a href="#cb18-112"></a></span>
<span id="cb18-113"><a href="#cb18-113"></a><span class="bu">print</span>(<span class="st">"Mini ResNet:"</span>)</span>
<span id="cb18-114"><a href="#cb18-114"></a><span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> mini_resnet.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb18-115"><a href="#cb18-115"></a></span>
<span id="cb18-116"><a href="#cb18-116"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Official ResNet-18:"</span>)</span>
<span id="cb18-117"><a href="#cb18-117"></a><span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> official_resnet.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb18-118"><a href="#cb18-118"></a><span class="in">```</span></span>
<span id="cb18-119"><a href="#cb18-119"></a></span>
<span id="cb18-120"><a href="#cb18-120"></a>**🎯 Try it yourself!** <span class="co">[</span><span class="ot">Open in Colab</span><span class="co">](https://colab.research.google.com/github/hasanpasha/quarto_blog_hasan/blob/main/notebooks/cv-foundations-07-modern-vision-models.ipynb)</span></span>
<span id="cb18-121"><a href="#cb18-121"></a></span>
<span id="cb18-122"><a href="#cb18-122"></a><span class="fu">### EfficientNet: Scaling Done Right</span></span>
<span id="cb18-123"><a href="#cb18-123"></a></span>
<span id="cb18-126"><a href="#cb18-126"></a><span class="in">```{python}</span></span>
<span id="cb18-127"><a href="#cb18-127"></a><span class="co">#| eval: true</span></span>
<span id="cb18-128"><a href="#cb18-128"></a><span class="co"># EfficientNet's key insight: compound scaling</span></span>
<span id="cb18-129"><a href="#cb18-129"></a><span class="im">from</span> torchvision.models <span class="im">import</span> efficientnet_b0, efficientnet_b7</span>
<span id="cb18-130"><a href="#cb18-130"></a></span>
<span id="cb18-131"><a href="#cb18-131"></a><span class="co"># Load different EfficientNet variants</span></span>
<span id="cb18-132"><a href="#cb18-132"></a>efficient_b0 <span class="op">=</span> efficientnet_b0(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-133"><a href="#cb18-133"></a>efficient_b7 <span class="op">=</span> efficientnet_b7(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-134"><a href="#cb18-134"></a></span>
<span id="cb18-135"><a href="#cb18-135"></a><span class="kw">def</span> model_info(model, name):</span>
<span id="cb18-136"><a href="#cb18-136"></a>    total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb18-137"><a href="#cb18-137"></a>    <span class="cf">return</span> {</span>
<span id="cb18-138"><a href="#cb18-138"></a>        <span class="st">'name'</span>: name,</span>
<span id="cb18-139"><a href="#cb18-139"></a>        <span class="st">'parameters'</span>: total_params,</span>
<span id="cb18-140"><a href="#cb18-140"></a>        <span class="st">'size_mb'</span>: total_params <span class="op">*</span> <span class="dv">4</span> <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)  <span class="co"># Rough estimate</span></span>
<span id="cb18-141"><a href="#cb18-141"></a>    }</span>
<span id="cb18-142"><a href="#cb18-142"></a></span>
<span id="cb18-143"><a href="#cb18-143"></a>models_comparison <span class="op">=</span> [</span>
<span id="cb18-144"><a href="#cb18-144"></a>    model_info(efficient_b0, <span class="st">'EfficientNet-B0'</span>),</span>
<span id="cb18-145"><a href="#cb18-145"></a>    model_info(efficient_b7, <span class="st">'EfficientNet-B7'</span>),</span>
<span id="cb18-146"><a href="#cb18-146"></a>    model_info(official_resnet, <span class="st">'ResNet-18'</span>)</span>
<span id="cb18-147"><a href="#cb18-147"></a>]</span>
<span id="cb18-148"><a href="#cb18-148"></a></span>
<span id="cb18-149"><a href="#cb18-149"></a><span class="bu">print</span>(<span class="st">"Model Comparison:"</span>)</span>
<span id="cb18-150"><a href="#cb18-150"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb18-151"><a href="#cb18-151"></a><span class="cf">for</span> info <span class="kw">in</span> models_comparison:</span>
<span id="cb18-152"><a href="#cb18-152"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>info[<span class="st">'name'</span>]<span class="sc">:20}</span><span class="ss"> | </span><span class="sc">{</span>info[<span class="st">'parameters'</span>]<span class="sc">:</span><span class="op">&gt;</span><span class="dv">10</span><span class="sc">,}</span><span class="ss"> params | </span><span class="sc">{</span>info[<span class="st">'size_mb'</span>]<span class="sc">:&gt;6.1f}</span><span class="ss"> MB"</span>)</span>
<span id="cb18-153"><a href="#cb18-153"></a><span class="in">```</span></span>
<span id="cb18-154"><a href="#cb18-154"></a></span>
<span id="cb18-155"><a href="#cb18-155"></a><span class="fu">## The Transformer Revolution: Vision Meets Attention</span></span>
<span id="cb18-156"><a href="#cb18-156"></a></span>
<span id="cb18-157"><a href="#cb18-157"></a>In 2020, everything changed when researchers asked: "What if we applied transformers to vision?"</span>
<span id="cb18-158"><a href="#cb18-158"></a></span>
<span id="cb18-159"><a href="#cb18-159"></a><span class="fu">### Understanding Vision Transformers</span></span>
<span id="cb18-160"><a href="#cb18-160"></a></span>
<span id="cb18-163"><a href="#cb18-163"></a><span class="in">```{python}</span></span>
<span id="cb18-164"><a href="#cb18-164"></a><span class="co">#| eval: true</span></span>
<span id="cb18-165"><a href="#cb18-165"></a><span class="kw">class</span> PatchEmbedding(nn.Module):</span>
<span id="cb18-166"><a href="#cb18-166"></a>    <span class="co">"""Convert image to sequence of patches"""</span></span>
<span id="cb18-167"><a href="#cb18-167"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_size<span class="op">=</span><span class="dv">224</span>, patch_size<span class="op">=</span><span class="dv">16</span>, in_channels<span class="op">=</span><span class="dv">3</span>, embed_dim<span class="op">=</span><span class="dv">768</span>):</span>
<span id="cb18-168"><a href="#cb18-168"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-169"><a href="#cb18-169"></a>        <span class="va">self</span>.img_size <span class="op">=</span> img_size</span>
<span id="cb18-170"><a href="#cb18-170"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb18-171"><a href="#cb18-171"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> (img_size <span class="op">//</span> patch_size) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb18-172"><a href="#cb18-172"></a>        </span>
<span id="cb18-173"><a href="#cb18-173"></a>        <span class="co"># Patch embedding using convolution</span></span>
<span id="cb18-174"><a href="#cb18-174"></a>        <span class="va">self</span>.projection <span class="op">=</span> nn.Conv2d(</span>
<span id="cb18-175"><a href="#cb18-175"></a>            in_channels, embed_dim, </span>
<span id="cb18-176"><a href="#cb18-176"></a>            kernel_size<span class="op">=</span>patch_size, stride<span class="op">=</span>patch_size</span>
<span id="cb18-177"><a href="#cb18-177"></a>        )</span>
<span id="cb18-178"><a href="#cb18-178"></a>    </span>
<span id="cb18-179"><a href="#cb18-179"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-180"><a href="#cb18-180"></a>        <span class="co"># x shape: (batch_size, channels, height, width)</span></span>
<span id="cb18-181"><a href="#cb18-181"></a>        x <span class="op">=</span> <span class="va">self</span>.projection(x)  <span class="co"># (batch_size, embed_dim, num_patches_h, num_patches_w)</span></span>
<span id="cb18-182"><a href="#cb18-182"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>)        <span class="co"># (batch_size, embed_dim, num_patches)</span></span>
<span id="cb18-183"><a href="#cb18-183"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)   <span class="co"># (batch_size, num_patches, embed_dim)</span></span>
<span id="cb18-184"><a href="#cb18-184"></a>        <span class="cf">return</span> x</span>
<span id="cb18-185"><a href="#cb18-185"></a></span>
<span id="cb18-186"><a href="#cb18-186"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb18-187"><a href="#cb18-187"></a>    <span class="co">"""Multi-head self-attention mechanism"""</span></span>
<span id="cb18-188"><a href="#cb18-188"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb18-189"><a href="#cb18-189"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-190"><a href="#cb18-190"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb18-191"><a href="#cb18-191"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb18-192"><a href="#cb18-192"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> embed_dim <span class="op">//</span> num_heads</span>
<span id="cb18-193"><a href="#cb18-193"></a>        </span>
<span id="cb18-194"><a href="#cb18-194"></a>        <span class="va">self</span>.qkv <span class="op">=</span> nn.Linear(embed_dim, embed_dim <span class="op">*</span> <span class="dv">3</span>)</span>
<span id="cb18-195"><a href="#cb18-195"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb18-196"><a href="#cb18-196"></a>    </span>
<span id="cb18-197"><a href="#cb18-197"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-198"><a href="#cb18-198"></a>        batch_size, seq_len, embed_dim <span class="op">=</span> x.shape</span>
<span id="cb18-199"><a href="#cb18-199"></a>        </span>
<span id="cb18-200"><a href="#cb18-200"></a>        <span class="co"># Generate Q, K, V</span></span>
<span id="cb18-201"><a href="#cb18-201"></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv(x).reshape(batch_size, seq_len, <span class="dv">3</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb18-202"><a href="#cb18-202"></a>        qkv <span class="op">=</span> qkv.permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>)  <span class="co"># (3, batch_size, num_heads, seq_len, head_dim)</span></span>
<span id="cb18-203"><a href="#cb18-203"></a>        q, k, v <span class="op">=</span> qkv[<span class="dv">0</span>], qkv[<span class="dv">1</span>], qkv[<span class="dv">2</span>]</span>
<span id="cb18-204"><a href="#cb18-204"></a>        </span>
<span id="cb18-205"><a href="#cb18-205"></a>        <span class="co"># Compute attention</span></span>
<span id="cb18-206"><a href="#cb18-206"></a>        attn <span class="op">=</span> (q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> (<span class="va">self</span>.head_dim <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb18-207"><a href="#cb18-207"></a>        attn <span class="op">=</span> torch.softmax(attn, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb18-208"><a href="#cb18-208"></a>        </span>
<span id="cb18-209"><a href="#cb18-209"></a>        <span class="co"># Apply attention to values</span></span>
<span id="cb18-210"><a href="#cb18-210"></a>        out <span class="op">=</span> (attn <span class="op">@</span> v).transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, seq_len, embed_dim)</span>
<span id="cb18-211"><a href="#cb18-211"></a>        out <span class="op">=</span> <span class="va">self</span>.proj(out)</span>
<span id="cb18-212"><a href="#cb18-212"></a>        </span>
<span id="cb18-213"><a href="#cb18-213"></a>        <span class="cf">return</span> out, attn</span>
<span id="cb18-214"><a href="#cb18-214"></a></span>
<span id="cb18-215"><a href="#cb18-215"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb18-216"><a href="#cb18-216"></a>    <span class="co">"""Single transformer encoder block"""</span></span>
<span id="cb18-217"><a href="#cb18-217"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">12</span>, mlp_ratio<span class="op">=</span><span class="fl">4.0</span>):</span>
<span id="cb18-218"><a href="#cb18-218"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-219"><a href="#cb18-219"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb18-220"><a href="#cb18-220"></a>        <span class="va">self</span>.attn <span class="op">=</span> MultiHeadAttention(embed_dim, num_heads)</span>
<span id="cb18-221"><a href="#cb18-221"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb18-222"><a href="#cb18-222"></a>        </span>
<span id="cb18-223"><a href="#cb18-223"></a>        <span class="co"># MLP</span></span>
<span id="cb18-224"><a href="#cb18-224"></a>        mlp_hidden_dim <span class="op">=</span> <span class="bu">int</span>(embed_dim <span class="op">*</span> mlp_ratio)</span>
<span id="cb18-225"><a href="#cb18-225"></a>        <span class="va">self</span>.mlp <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-226"><a href="#cb18-226"></a>            nn.Linear(embed_dim, mlp_hidden_dim),</span>
<span id="cb18-227"><a href="#cb18-227"></a>            nn.GELU(),</span>
<span id="cb18-228"><a href="#cb18-228"></a>            nn.Linear(mlp_hidden_dim, embed_dim)</span>
<span id="cb18-229"><a href="#cb18-229"></a>        )</span>
<span id="cb18-230"><a href="#cb18-230"></a>    </span>
<span id="cb18-231"><a href="#cb18-231"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-232"><a href="#cb18-232"></a>        <span class="co"># Self-attention with residual connection</span></span>
<span id="cb18-233"><a href="#cb18-233"></a>        attn_out, attn_weights <span class="op">=</span> <span class="va">self</span>.attn(<span class="va">self</span>.norm1(x))</span>
<span id="cb18-234"><a href="#cb18-234"></a>        x <span class="op">=</span> x <span class="op">+</span> attn_out</span>
<span id="cb18-235"><a href="#cb18-235"></a>        </span>
<span id="cb18-236"><a href="#cb18-236"></a>        <span class="co"># MLP with residual connection</span></span>
<span id="cb18-237"><a href="#cb18-237"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.norm2(x))</span>
<span id="cb18-238"><a href="#cb18-238"></a>        </span>
<span id="cb18-239"><a href="#cb18-239"></a>        <span class="cf">return</span> x, attn_weights</span>
<span id="cb18-240"><a href="#cb18-240"></a></span>
<span id="cb18-241"><a href="#cb18-241"></a><span class="kw">class</span> SimpleViT(nn.Module):</span>
<span id="cb18-242"><a href="#cb18-242"></a>    <span class="co">"""Simplified Vision Transformer"""</span></span>
<span id="cb18-243"><a href="#cb18-243"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_size<span class="op">=</span><span class="dv">224</span>, patch_size<span class="op">=</span><span class="dv">16</span>, num_classes<span class="op">=</span><span class="dv">1000</span>, </span>
<span id="cb18-244"><a href="#cb18-244"></a>                 embed_dim<span class="op">=</span><span class="dv">768</span>, depth<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb18-245"><a href="#cb18-245"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-246"><a href="#cb18-246"></a>        </span>
<span id="cb18-247"><a href="#cb18-247"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb18-248"><a href="#cb18-248"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> PatchEmbedding(img_size, patch_size, <span class="dv">3</span>, embed_dim)</span>
<span id="cb18-249"><a href="#cb18-249"></a>        num_patches <span class="op">=</span> <span class="va">self</span>.patch_embed.num_patches</span>
<span id="cb18-250"><a href="#cb18-250"></a>        </span>
<span id="cb18-251"><a href="#cb18-251"></a>        <span class="co"># Class token and position embedding</span></span>
<span id="cb18-252"><a href="#cb18-252"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, embed_dim))</span>
<span id="cb18-253"><a href="#cb18-253"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, num_patches <span class="op">+</span> <span class="dv">1</span>, embed_dim))</span>
<span id="cb18-254"><a href="#cb18-254"></a>        </span>
<span id="cb18-255"><a href="#cb18-255"></a>        <span class="co"># Transformer blocks</span></span>
<span id="cb18-256"><a href="#cb18-256"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.ModuleList([</span>
<span id="cb18-257"><a href="#cb18-257"></a>            TransformerBlock(embed_dim, num_heads) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth)</span>
<span id="cb18-258"><a href="#cb18-258"></a>        ])</span>
<span id="cb18-259"><a href="#cb18-259"></a>        </span>
<span id="cb18-260"><a href="#cb18-260"></a>        <span class="co"># Classification head</span></span>
<span id="cb18-261"><a href="#cb18-261"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb18-262"><a href="#cb18-262"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(embed_dim, num_classes)</span>
<span id="cb18-263"><a href="#cb18-263"></a>    </span>
<span id="cb18-264"><a href="#cb18-264"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-265"><a href="#cb18-265"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb18-266"><a href="#cb18-266"></a>        </span>
<span id="cb18-267"><a href="#cb18-267"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb18-268"><a href="#cb18-268"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)  <span class="co"># (batch_size, num_patches, embed_dim)</span></span>
<span id="cb18-269"><a href="#cb18-269"></a>        </span>
<span id="cb18-270"><a href="#cb18-270"></a>        <span class="co"># Add class token</span></span>
<span id="cb18-271"><a href="#cb18-271"></a>        cls_tokens <span class="op">=</span> <span class="va">self</span>.cls_token.expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb18-272"><a href="#cb18-272"></a>        x <span class="op">=</span> torch.cat((cls_tokens, x), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-273"><a href="#cb18-273"></a>        </span>
<span id="cb18-274"><a href="#cb18-274"></a>        <span class="co"># Add position embedding</span></span>
<span id="cb18-275"><a href="#cb18-275"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed</span>
<span id="cb18-276"><a href="#cb18-276"></a>        </span>
<span id="cb18-277"><a href="#cb18-277"></a>        <span class="co"># Apply transformer blocks</span></span>
<span id="cb18-278"><a href="#cb18-278"></a>        attention_maps <span class="op">=</span> []</span>
<span id="cb18-279"><a href="#cb18-279"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.blocks:</span>
<span id="cb18-280"><a href="#cb18-280"></a>            x, attn <span class="op">=</span> block(x)</span>
<span id="cb18-281"><a href="#cb18-281"></a>            attention_maps.append(attn)</span>
<span id="cb18-282"><a href="#cb18-282"></a>        </span>
<span id="cb18-283"><a href="#cb18-283"></a>        <span class="co"># Classification</span></span>
<span id="cb18-284"><a href="#cb18-284"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb18-285"><a href="#cb18-285"></a>        cls_token_final <span class="op">=</span> x[:, <span class="dv">0</span>]  <span class="co"># Use class token for classification</span></span>
<span id="cb18-286"><a href="#cb18-286"></a>        out <span class="op">=</span> <span class="va">self</span>.head(cls_token_final)</span>
<span id="cb18-287"><a href="#cb18-287"></a>        </span>
<span id="cb18-288"><a href="#cb18-288"></a>        <span class="cf">return</span> out, attention_maps</span>
<span id="cb18-289"><a href="#cb18-289"></a></span>
<span id="cb18-290"><a href="#cb18-290"></a><span class="co"># Create a simple ViT</span></span>
<span id="cb18-291"><a href="#cb18-291"></a>simple_vit <span class="op">=</span> SimpleViT(depth<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">8</span>)  <span class="co"># Smaller for demo</span></span>
<span id="cb18-292"><a href="#cb18-292"></a>vit_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> simple_vit.parameters())</span>
<span id="cb18-293"><a href="#cb18-293"></a></span>
<span id="cb18-294"><a href="#cb18-294"></a><span class="bu">print</span>(<span class="ss">f"Simple ViT parameters: </span><span class="sc">{</span>vit_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb18-295"><a href="#cb18-295"></a></span>
<span id="cb18-296"><a href="#cb18-296"></a><span class="co"># Test with dummy input</span></span>
<span id="cb18-297"><a href="#cb18-297"></a>dummy_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb18-298"><a href="#cb18-298"></a>output, attention_maps <span class="op">=</span> simple_vit(dummy_input)</span>
<span id="cb18-299"><a href="#cb18-299"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-300"><a href="#cb18-300"></a><span class="bu">print</span>(<span class="ss">f"Number of attention maps: </span><span class="sc">{</span><span class="bu">len</span>(attention_maps)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-301"><a href="#cb18-301"></a><span class="in">```</span></span>
<span id="cb18-302"><a href="#cb18-302"></a></span>
<span id="cb18-303"><a href="#cb18-303"></a><span class="fu">### Visualizing Attention: What Does the Model Look At?</span></span>
<span id="cb18-304"><a href="#cb18-304"></a></span>
<span id="cb18-307"><a href="#cb18-307"></a><span class="in">```{python}</span></span>
<span id="cb18-308"><a href="#cb18-308"></a><span class="co">#| eval: true</span></span>
<span id="cb18-309"><a href="#cb18-309"></a><span class="kw">def</span> visualize_attention(image, attention_maps, patch_size<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb18-310"><a href="#cb18-310"></a>    <span class="co">"""Visualize what the vision transformer is looking at"""</span></span>
<span id="cb18-311"><a href="#cb18-311"></a>    </span>
<span id="cb18-312"><a href="#cb18-312"></a>    <span class="co"># Use attention from the last layer, first head</span></span>
<span id="cb18-313"><a href="#cb18-313"></a>    attn <span class="op">=</span> attention_maps[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>, <span class="dv">0</span>]  <span class="co"># (seq_len, seq_len)</span></span>
<span id="cb18-314"><a href="#cb18-314"></a>    </span>
<span id="cb18-315"><a href="#cb18-315"></a>    <span class="co"># Get attention from class token to all patches</span></span>
<span id="cb18-316"><a href="#cb18-316"></a>    cls_attn <span class="op">=</span> attn[<span class="dv">0</span>, <span class="dv">1</span>:]  <span class="co"># Exclude class token to class token attention</span></span>
<span id="cb18-317"><a href="#cb18-317"></a>    </span>
<span id="cb18-318"><a href="#cb18-318"></a>    <span class="co"># Reshape to spatial dimensions</span></span>
<span id="cb18-319"><a href="#cb18-319"></a>    num_patches_per_side <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(cls_attn) <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb18-320"><a href="#cb18-320"></a>    attn_map <span class="op">=</span> cls_attn.reshape(num_patches_per_side, num_patches_per_side)</span>
<span id="cb18-321"><a href="#cb18-321"></a>    </span>
<span id="cb18-322"><a href="#cb18-322"></a>    <span class="co"># Resize to image size</span></span>
<span id="cb18-323"><a href="#cb18-323"></a>    attn_map <span class="op">=</span> torch.nn.functional.interpolate(</span>
<span id="cb18-324"><a href="#cb18-324"></a>        attn_map.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb18-325"><a href="#cb18-325"></a>        size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb18-326"><a href="#cb18-326"></a>        mode<span class="op">=</span><span class="st">'bilinear'</span></span>
<span id="cb18-327"><a href="#cb18-327"></a>    ).squeeze()</span>
<span id="cb18-328"><a href="#cb18-328"></a>    </span>
<span id="cb18-329"><a href="#cb18-329"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb18-330"><a href="#cb18-330"></a>    </span>
<span id="cb18-331"><a href="#cb18-331"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb18-332"><a href="#cb18-332"></a>    plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb18-333"><a href="#cb18-333"></a>    plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb18-334"><a href="#cb18-334"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb18-335"><a href="#cb18-335"></a>    </span>
<span id="cb18-336"><a href="#cb18-336"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-337"><a href="#cb18-337"></a>    plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb18-338"><a href="#cb18-338"></a>    plt.imshow(attn_map.detach().numpy(), alpha<span class="op">=</span><span class="fl">0.6</span>, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb18-339"><a href="#cb18-339"></a>    plt.title(<span class="st">"Attention Map"</span>)</span>
<span id="cb18-340"><a href="#cb18-340"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb18-341"><a href="#cb18-341"></a>    </span>
<span id="cb18-342"><a href="#cb18-342"></a>    plt.tight_layout()</span>
<span id="cb18-343"><a href="#cb18-343"></a>    plt.show()</span>
<span id="cb18-344"><a href="#cb18-344"></a></span>
<span id="cb18-345"><a href="#cb18-345"></a><span class="co"># Visualize attention (you would use a real image)</span></span>
<span id="cb18-346"><a href="#cb18-346"></a>dummy_image <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb18-347"><a href="#cb18-347"></a>visualize_attention(dummy_image, attention_maps)</span>
<span id="cb18-348"><a href="#cb18-348"></a><span class="in">```</span></span>
<span id="cb18-349"><a href="#cb18-349"></a></span>
<span id="cb18-350"><a href="#cb18-350"></a><span class="fu">## Foundation Models: The DINOv2 Revolution</span></span>
<span id="cb18-351"><a href="#cb18-351"></a></span>
<span id="cb18-352"><a href="#cb18-352"></a>Now we reach the cutting edge: **Foundation Models**. DINOv2 (Distillation with No Labels v2) represents a paradigm shift:</span>
<span id="cb18-353"><a href="#cb18-353"></a></span>
<span id="cb18-354"><a href="#cb18-354"></a><span class="ss">- </span>**Self-supervised learning**: No labels needed!</span>
<span id="cb18-355"><a href="#cb18-355"></a><span class="ss">- </span>**Universal features**: Works for any vision task</span>
<span id="cb18-356"><a href="#cb18-356"></a><span class="ss">- </span>**Incredible performance**: Often beats supervised methods</span>
<span id="cb18-357"><a href="#cb18-357"></a></span>
<span id="cb18-358"><a href="#cb18-358"></a><span class="fu">### Using DINOv2 with HuggingFace</span></span>
<span id="cb18-359"><a href="#cb18-359"></a></span>
<span id="cb18-362"><a href="#cb18-362"></a><span class="in">```{python}</span></span>
<span id="cb18-363"><a href="#cb18-363"></a><span class="co">#| eval: true</span></span>
<span id="cb18-364"><a href="#cb18-364"></a><span class="co"># Install required packages</span></span>
<span id="cb18-365"><a href="#cb18-365"></a><span class="co"># !pip install transformers torch torchvision</span></span>
<span id="cb18-366"><a href="#cb18-366"></a></span>
<span id="cb18-367"><a href="#cb18-367"></a><span class="im">from</span> transformers <span class="im">import</span> AutoImageProcessor, AutoModel</span>
<span id="cb18-368"><a href="#cb18-368"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb18-369"><a href="#cb18-369"></a><span class="im">import</span> requests</span>
<span id="cb18-370"><a href="#cb18-370"></a></span>
<span id="cb18-371"><a href="#cb18-371"></a><span class="co"># Load DINOv2 model and processor</span></span>
<span id="cb18-372"><a href="#cb18-372"></a>model_name <span class="op">=</span> <span class="st">"facebook/dinov2-base"</span></span>
<span id="cb18-373"><a href="#cb18-373"></a>processor <span class="op">=</span> AutoImageProcessor.from_pretrained(model_name)</span>
<span id="cb18-374"><a href="#cb18-374"></a>model <span class="op">=</span> AutoModel.from_pretrained(model_name)</span>
<span id="cb18-375"><a href="#cb18-375"></a></span>
<span id="cb18-376"><a href="#cb18-376"></a><span class="bu">print</span>(<span class="ss">f"Loaded DINOv2 model: </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-377"><a href="#cb18-377"></a><span class="bu">print</span>(<span class="ss">f"Model parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb18-378"><a href="#cb18-378"></a></span>
<span id="cb18-379"><a href="#cb18-379"></a><span class="kw">def</span> extract_dinov2_features(image_path_or_url):</span>
<span id="cb18-380"><a href="#cb18-380"></a>    <span class="co">"""Extract features using DINOv2"""</span></span>
<span id="cb18-381"><a href="#cb18-381"></a>    </span>
<span id="cb18-382"><a href="#cb18-382"></a>    <span class="co"># Load image</span></span>
<span id="cb18-383"><a href="#cb18-383"></a>    <span class="cf">if</span> image_path_or_url.startswith(<span class="st">'http'</span>):</span>
<span id="cb18-384"><a href="#cb18-384"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(image_path_or_url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb18-385"><a href="#cb18-385"></a>    <span class="cf">else</span>:</span>
<span id="cb18-386"><a href="#cb18-386"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(image_path_or_url)</span>
<span id="cb18-387"><a href="#cb18-387"></a>    </span>
<span id="cb18-388"><a href="#cb18-388"></a>    <span class="co"># Process image</span></span>
<span id="cb18-389"><a href="#cb18-389"></a>    inputs <span class="op">=</span> processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb18-390"><a href="#cb18-390"></a>    </span>
<span id="cb18-391"><a href="#cb18-391"></a>    <span class="co"># Extract features</span></span>
<span id="cb18-392"><a href="#cb18-392"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb18-393"><a href="#cb18-393"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb18-394"><a href="#cb18-394"></a>        features <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb18-395"><a href="#cb18-395"></a>        </span>
<span id="cb18-396"><a href="#cb18-396"></a>        <span class="co"># Get CLS token (global image representation)</span></span>
<span id="cb18-397"><a href="#cb18-397"></a>        cls_features <span class="op">=</span> features[:, <span class="dv">0</span>]  <span class="co"># Shape: (1, 768)</span></span>
<span id="cb18-398"><a href="#cb18-398"></a>        </span>
<span id="cb18-399"><a href="#cb18-399"></a>        <span class="co"># Get patch features (local representations)</span></span>
<span id="cb18-400"><a href="#cb18-400"></a>        patch_features <span class="op">=</span> features[:, <span class="dv">1</span>:]  <span class="co"># Shape: (1, num_patches, 768)</span></span>
<span id="cb18-401"><a href="#cb18-401"></a>    </span>
<span id="cb18-402"><a href="#cb18-402"></a>    <span class="cf">return</span> {</span>
<span id="cb18-403"><a href="#cb18-403"></a>        <span class="st">'cls_features'</span>: cls_features,</span>
<span id="cb18-404"><a href="#cb18-404"></a>        <span class="st">'patch_features'</span>: patch_features,</span>
<span id="cb18-405"><a href="#cb18-405"></a>        <span class="st">'image'</span>: image</span>
<span id="cb18-406"><a href="#cb18-406"></a>    }</span>
<span id="cb18-407"><a href="#cb18-407"></a></span>
<span id="cb18-408"><a href="#cb18-408"></a><span class="co"># Example usage</span></span>
<span id="cb18-409"><a href="#cb18-409"></a><span class="kw">def</span> demo_dinov2_features():</span>
<span id="cb18-410"><a href="#cb18-410"></a>    <span class="co">"""Demonstrate DINOv2 feature extraction"""</span></span>
<span id="cb18-411"><a href="#cb18-411"></a>    </span>
<span id="cb18-412"><a href="#cb18-412"></a>    <span class="co"># Create dummy image for demo (you would use real images)</span></span>
<span id="cb18-413"><a href="#cb18-413"></a>    dummy_image <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb18-414"><a href="#cb18-414"></a>    </span>
<span id="cb18-415"><a href="#cb18-415"></a>    <span class="co"># Save temporarily</span></span>
<span id="cb18-416"><a href="#cb18-416"></a>    dummy_image.save(<span class="st">'temp_image.jpg'</span>)</span>
<span id="cb18-417"><a href="#cb18-417"></a>    </span>
<span id="cb18-418"><a href="#cb18-418"></a>    <span class="co"># Extract features</span></span>
<span id="cb18-419"><a href="#cb18-419"></a>    result <span class="op">=</span> extract_dinov2_features(<span class="st">'temp_image.jpg'</span>)</span>
<span id="cb18-420"><a href="#cb18-420"></a>    </span>
<span id="cb18-421"><a href="#cb18-421"></a>    <span class="bu">print</span>(<span class="st">"DINOv2 Feature Extraction Results:"</span>)</span>
<span id="cb18-422"><a href="#cb18-422"></a>    <span class="bu">print</span>(<span class="ss">f"Global features shape: </span><span class="sc">{</span>result[<span class="st">'cls_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-423"><a href="#cb18-423"></a>    <span class="bu">print</span>(<span class="ss">f"Patch features shape: </span><span class="sc">{</span>result[<span class="st">'patch_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-424"><a href="#cb18-424"></a>    </span>
<span id="cb18-425"><a href="#cb18-425"></a>    <span class="co"># Visualize features</span></span>
<span id="cb18-426"><a href="#cb18-426"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb18-427"><a href="#cb18-427"></a>    </span>
<span id="cb18-428"><a href="#cb18-428"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb18-429"><a href="#cb18-429"></a>    plt.imshow(result[<span class="st">'image'</span>])</span>
<span id="cb18-430"><a href="#cb18-430"></a>    plt.title(<span class="st">"Input Image"</span>)</span>
<span id="cb18-431"><a href="#cb18-431"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb18-432"><a href="#cb18-432"></a>    </span>
<span id="cb18-433"><a href="#cb18-433"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb18-434"><a href="#cb18-434"></a>    plt.plot(result[<span class="st">'cls_features'</span>].squeeze().numpy())</span>
<span id="cb18-435"><a href="#cb18-435"></a>    plt.title(<span class="st">"Global Features (768 dimensions)"</span>)</span>
<span id="cb18-436"><a href="#cb18-436"></a>    plt.xlabel(<span class="st">"Dimension"</span>)</span>
<span id="cb18-437"><a href="#cb18-437"></a>    plt.ylabel(<span class="st">"Value"</span>)</span>
<span id="cb18-438"><a href="#cb18-438"></a>    </span>
<span id="cb18-439"><a href="#cb18-439"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb18-440"><a href="#cb18-440"></a>    <span class="co"># Visualize patch features as heatmap</span></span>
<span id="cb18-441"><a href="#cb18-441"></a>    patch_norms <span class="op">=</span> torch.norm(result[<span class="st">'patch_features'</span>].squeeze(), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-442"><a href="#cb18-442"></a>    patch_size <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(patch_norms) <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb18-443"><a href="#cb18-443"></a>    patch_map <span class="op">=</span> patch_norms.reshape(patch_size, patch_size)</span>
<span id="cb18-444"><a href="#cb18-444"></a>    plt.imshow(patch_map.numpy(), cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb18-445"><a href="#cb18-445"></a>    plt.title(<span class="st">"Patch Feature Magnitudes"</span>)</span>
<span id="cb18-446"><a href="#cb18-446"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb18-447"><a href="#cb18-447"></a>    </span>
<span id="cb18-448"><a href="#cb18-448"></a>    plt.tight_layout()</span>
<span id="cb18-449"><a href="#cb18-449"></a>    plt.show()</span>
<span id="cb18-450"><a href="#cb18-450"></a>    </span>
<span id="cb18-451"><a href="#cb18-451"></a>    <span class="cf">return</span> result</span>
<span id="cb18-452"><a href="#cb18-452"></a></span>
<span id="cb18-453"><a href="#cb18-453"></a>demo_result <span class="op">=</span> demo_dinov2_features()</span>
<span id="cb18-454"><a href="#cb18-454"></a><span class="in">```</span></span>
<span id="cb18-455"><a href="#cb18-455"></a></span>
<span id="cb18-456"><a href="#cb18-456"></a><span class="fu">### Building a DINOv2-Powered Image Similarity Engine</span></span>
<span id="cb18-457"><a href="#cb18-457"></a></span>
<span id="cb18-460"><a href="#cb18-460"></a><span class="in">```{python}</span></span>
<span id="cb18-461"><a href="#cb18-461"></a><span class="co">#| eval: true</span></span>
<span id="cb18-462"><a href="#cb18-462"></a><span class="kw">class</span> DINOv2SimilarityEngine:</span>
<span id="cb18-463"><a href="#cb18-463"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb18-464"><a href="#cb18-464"></a>        <span class="va">self</span>.processor <span class="op">=</span> AutoImageProcessor.from_pretrained(<span class="st">"facebook/dinov2-base"</span>)</span>
<span id="cb18-465"><a href="#cb18-465"></a>        <span class="va">self</span>.model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">"facebook/dinov2-base"</span>)</span>
<span id="cb18-466"><a href="#cb18-466"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb18-467"><a href="#cb18-467"></a>        <span class="va">self</span>.image_database <span class="op">=</span> {}</span>
<span id="cb18-468"><a href="#cb18-468"></a>    </span>
<span id="cb18-469"><a href="#cb18-469"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, image):</span>
<span id="cb18-470"><a href="#cb18-470"></a>        <span class="co">"""Extract DINOv2 features from an image"""</span></span>
<span id="cb18-471"><a href="#cb18-471"></a>        inputs <span class="op">=</span> <span class="va">self</span>.processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb18-472"><a href="#cb18-472"></a>        </span>
<span id="cb18-473"><a href="#cb18-473"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb18-474"><a href="#cb18-474"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(<span class="op">**</span>inputs)</span>
<span id="cb18-475"><a href="#cb18-475"></a>            <span class="co"># Use CLS token as global image representation</span></span>
<span id="cb18-476"><a href="#cb18-476"></a>            features <span class="op">=</span> outputs.last_hidden_state[:, <span class="dv">0</span>]</span>
<span id="cb18-477"><a href="#cb18-477"></a>        </span>
<span id="cb18-478"><a href="#cb18-478"></a>        <span class="cf">return</span> features</span>
<span id="cb18-479"><a href="#cb18-479"></a>    </span>
<span id="cb18-480"><a href="#cb18-480"></a>    <span class="kw">def</span> add_image(<span class="va">self</span>, image_id, image):</span>
<span id="cb18-481"><a href="#cb18-481"></a>        <span class="co">"""Add an image to the database"""</span></span>
<span id="cb18-482"><a href="#cb18-482"></a>        features <span class="op">=</span> <span class="va">self</span>.extract_features(image)</span>
<span id="cb18-483"><a href="#cb18-483"></a>        <span class="va">self</span>.image_database[image_id] <span class="op">=</span> {</span>
<span id="cb18-484"><a href="#cb18-484"></a>            <span class="st">'features'</span>: features,</span>
<span id="cb18-485"><a href="#cb18-485"></a>            <span class="st">'image'</span>: image</span>
<span id="cb18-486"><a href="#cb18-486"></a>        }</span>
<span id="cb18-487"><a href="#cb18-487"></a>        <span class="bu">print</span>(<span class="ss">f"Added image '</span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ss">' to database"</span>)</span>
<span id="cb18-488"><a href="#cb18-488"></a>    </span>
<span id="cb18-489"><a href="#cb18-489"></a>    <span class="kw">def</span> find_similar_images(<span class="va">self</span>, query_image, top_k<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb18-490"><a href="#cb18-490"></a>        <span class="co">"""Find most similar images in the database"""</span></span>
<span id="cb18-491"><a href="#cb18-491"></a>        query_features <span class="op">=</span> <span class="va">self</span>.extract_features(query_image)</span>
<span id="cb18-492"><a href="#cb18-492"></a>        </span>
<span id="cb18-493"><a href="#cb18-493"></a>        similarities <span class="op">=</span> {}</span>
<span id="cb18-494"><a href="#cb18-494"></a>        </span>
<span id="cb18-495"><a href="#cb18-495"></a>        <span class="cf">for</span> image_id, data <span class="kw">in</span> <span class="va">self</span>.image_database.items():</span>
<span id="cb18-496"><a href="#cb18-496"></a>            <span class="co"># Compute cosine similarity</span></span>
<span id="cb18-497"><a href="#cb18-497"></a>            similarity <span class="op">=</span> torch.cosine_similarity(</span>
<span id="cb18-498"><a href="#cb18-498"></a>                query_features, data[<span class="st">'features'</span>], dim<span class="op">=</span><span class="dv">1</span></span>
<span id="cb18-499"><a href="#cb18-499"></a>            ).item()</span>
<span id="cb18-500"><a href="#cb18-500"></a>            similarities[image_id] <span class="op">=</span> similarity</span>
<span id="cb18-501"><a href="#cb18-501"></a>        </span>
<span id="cb18-502"><a href="#cb18-502"></a>        <span class="co"># Sort by similarity</span></span>
<span id="cb18-503"><a href="#cb18-503"></a>        sorted_similarities <span class="op">=</span> <span class="bu">sorted</span>(</span>
<span id="cb18-504"><a href="#cb18-504"></a>            similarities.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span></span>
<span id="cb18-505"><a href="#cb18-505"></a>        )</span>
<span id="cb18-506"><a href="#cb18-506"></a>        </span>
<span id="cb18-507"><a href="#cb18-507"></a>        <span class="cf">return</span> sorted_similarities[:top_k]</span>
<span id="cb18-508"><a href="#cb18-508"></a>    </span>
<span id="cb18-509"><a href="#cb18-509"></a>    <span class="kw">def</span> visualize_results(<span class="va">self</span>, query_image, similar_images):</span>
<span id="cb18-510"><a href="#cb18-510"></a>        <span class="co">"""Visualize similarity search results"""</span></span>
<span id="cb18-511"><a href="#cb18-511"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">8</span>))</span>
<span id="cb18-512"><a href="#cb18-512"></a>        </span>
<span id="cb18-513"><a href="#cb18-513"></a>        <span class="co"># Query image</span></span>
<span id="cb18-514"><a href="#cb18-514"></a>        plt.subplot(<span class="dv">2</span>, <span class="bu">len</span>(similar_images) <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-515"><a href="#cb18-515"></a>        plt.imshow(query_image)</span>
<span id="cb18-516"><a href="#cb18-516"></a>        plt.title(<span class="st">"Query Image"</span>)</span>
<span id="cb18-517"><a href="#cb18-517"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb18-518"><a href="#cb18-518"></a>        </span>
<span id="cb18-519"><a href="#cb18-519"></a>        <span class="co"># Similar images</span></span>
<span id="cb18-520"><a href="#cb18-520"></a>        <span class="cf">for</span> i, (image_id, similarity) <span class="kw">in</span> <span class="bu">enumerate</span>(similar_images):</span>
<span id="cb18-521"><a href="#cb18-521"></a>            plt.subplot(<span class="dv">2</span>, <span class="bu">len</span>(similar_images) <span class="op">+</span> <span class="dv">1</span>, i <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb18-522"><a href="#cb18-522"></a>            plt.imshow(<span class="va">self</span>.image_database[image_id][<span class="st">'image'</span>])</span>
<span id="cb18-523"><a href="#cb18-523"></a>            plt.title(<span class="ss">f"</span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ch">\n</span><span class="ss">Similarity: </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-524"><a href="#cb18-524"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb18-525"><a href="#cb18-525"></a>        </span>
<span id="cb18-526"><a href="#cb18-526"></a>        plt.tight_layout()</span>
<span id="cb18-527"><a href="#cb18-527"></a>        plt.show()</span>
<span id="cb18-528"><a href="#cb18-528"></a></span>
<span id="cb18-529"><a href="#cb18-529"></a><span class="co"># Create similarity engine</span></span>
<span id="cb18-530"><a href="#cb18-530"></a>similarity_engine <span class="op">=</span> DINOv2SimilarityEngine()</span>
<span id="cb18-531"><a href="#cb18-531"></a></span>
<span id="cb18-532"><a href="#cb18-532"></a><span class="co"># Demo with dummy images (you would use real images)</span></span>
<span id="cb18-533"><a href="#cb18-533"></a><span class="kw">def</span> demo_similarity_engine():</span>
<span id="cb18-534"><a href="#cb18-534"></a>    <span class="co">"""Demonstrate the similarity engine"""</span></span>
<span id="cb18-535"><a href="#cb18-535"></a>    </span>
<span id="cb18-536"><a href="#cb18-536"></a>    <span class="co"># Create some dummy images with different colors</span></span>
<span id="cb18-537"><a href="#cb18-537"></a>    colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'yellow'</span>, <span class="st">'purple'</span>]</span>
<span id="cb18-538"><a href="#cb18-538"></a>    </span>
<span id="cb18-539"><a href="#cb18-539"></a>    <span class="cf">for</span> color <span class="kw">in</span> colors:</span>
<span id="cb18-540"><a href="#cb18-540"></a>        dummy_img <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span>color)</span>
<span id="cb18-541"><a href="#cb18-541"></a>        similarity_engine.add_image(<span class="ss">f"</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss">_image"</span>, dummy_img)</span>
<span id="cb18-542"><a href="#cb18-542"></a>    </span>
<span id="cb18-543"><a href="#cb18-543"></a>    <span class="co"># Query with a red image</span></span>
<span id="cb18-544"><a href="#cb18-544"></a>    query_img <span class="op">=</span> Image.new(<span class="st">'RGB'</span>, (<span class="dv">224</span>, <span class="dv">224</span>), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb18-545"><a href="#cb18-545"></a>    </span>
<span id="cb18-546"><a href="#cb18-546"></a>    <span class="co"># Find similar images</span></span>
<span id="cb18-547"><a href="#cb18-547"></a>    similar <span class="op">=</span> similarity_engine.find_similar_images(query_img, top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb18-548"><a href="#cb18-548"></a>    </span>
<span id="cb18-549"><a href="#cb18-549"></a>    <span class="bu">print</span>(<span class="st">"Most similar images:"</span>)</span>
<span id="cb18-550"><a href="#cb18-550"></a>    <span class="cf">for</span> image_id, similarity <span class="kw">in</span> similar:</span>
<span id="cb18-551"><a href="#cb18-551"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-552"><a href="#cb18-552"></a>    </span>
<span id="cb18-553"><a href="#cb18-553"></a>    <span class="co"># Visualize results</span></span>
<span id="cb18-554"><a href="#cb18-554"></a>    similarity_engine.visualize_results(query_img, similar)</span>
<span id="cb18-555"><a href="#cb18-555"></a></span>
<span id="cb18-556"><a href="#cb18-556"></a>demo_similarity_engine()</span>
<span id="cb18-557"><a href="#cb18-557"></a><span class="in">```</span></span>
<span id="cb18-558"><a href="#cb18-558"></a></span>
<span id="cb18-559"><a href="#cb18-559"></a><span class="fu">## Comparing All Approaches: The Ultimate Showdown</span></span>
<span id="cb18-560"><a href="#cb18-560"></a></span>
<span id="cb18-561"><a href="#cb18-561"></a>Let's compare all the approaches we've learned:</span>
<span id="cb18-562"><a href="#cb18-562"></a></span>
<span id="cb18-565"><a href="#cb18-565"></a><span class="in">```{python}</span></span>
<span id="cb18-566"><a href="#cb18-566"></a><span class="co">#| eval: true</span></span>
<span id="cb18-567"><a href="#cb18-567"></a><span class="kw">class</span> VisionModelComparison:</span>
<span id="cb18-568"><a href="#cb18-568"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb18-569"><a href="#cb18-569"></a>        <span class="va">self</span>.models <span class="op">=</span> {</span>
<span id="cb18-570"><a href="#cb18-570"></a>            <span class="st">'ResNet-18'</span>: models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb18-571"><a href="#cb18-571"></a>            <span class="st">'ResNet-50'</span>: models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb18-572"><a href="#cb18-572"></a>            <span class="st">'EfficientNet-B0'</span>: efficientnet_b0(pretrained<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb18-573"><a href="#cb18-573"></a>            <span class="st">'ViT-Base'</span>: <span class="va">None</span>,  <span class="co"># Would load from transformers</span></span>
<span id="cb18-574"><a href="#cb18-574"></a>            <span class="st">'DINOv2-Base'</span>: <span class="va">None</span>  <span class="co"># Already loaded above</span></span>
<span id="cb18-575"><a href="#cb18-575"></a>        }</span>
<span id="cb18-576"><a href="#cb18-576"></a>    </span>
<span id="cb18-577"><a href="#cb18-577"></a>    <span class="kw">def</span> compare_models(<span class="va">self</span>):</span>
<span id="cb18-578"><a href="#cb18-578"></a>        <span class="co">"""Compare different vision models"""</span></span>
<span id="cb18-579"><a href="#cb18-579"></a>        comparison_data <span class="op">=</span> []</span>
<span id="cb18-580"><a href="#cb18-580"></a>        </span>
<span id="cb18-581"><a href="#cb18-581"></a>        <span class="cf">for</span> name, model <span class="kw">in</span> <span class="va">self</span>.models.items():</span>
<span id="cb18-582"><a href="#cb18-582"></a>            <span class="cf">if</span> model <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb18-583"><a href="#cb18-583"></a>                params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb18-584"><a href="#cb18-584"></a>                size_mb <span class="op">=</span> params <span class="op">*</span> <span class="dv">4</span> <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)</span>
<span id="cb18-585"><a href="#cb18-585"></a>                </span>
<span id="cb18-586"><a href="#cb18-586"></a>                comparison_data.append({</span>
<span id="cb18-587"><a href="#cb18-587"></a>                    <span class="st">'Model'</span>: name,</span>
<span id="cb18-588"><a href="#cb18-588"></a>                    <span class="st">'Parameters (M)'</span>: <span class="ss">f"</span><span class="sc">{</span>params <span class="op">/</span> <span class="fl">1e6</span><span class="sc">:.1f}</span><span class="ss">"</span>,</span>
<span id="cb18-589"><a href="#cb18-589"></a>                    <span class="st">'Size (MB)'</span>: <span class="ss">f"</span><span class="sc">{</span>size_mb<span class="sc">:.1f}</span><span class="ss">"</span>,</span>
<span id="cb18-590"><a href="#cb18-590"></a>                    <span class="st">'Year'</span>: <span class="va">self</span>.get_year(name),</span>
<span id="cb18-591"><a href="#cb18-591"></a>                    <span class="st">'Type'</span>: <span class="va">self</span>.get_type(name)</span>
<span id="cb18-592"><a href="#cb18-592"></a>                })</span>
<span id="cb18-593"><a href="#cb18-593"></a>        </span>
<span id="cb18-594"><a href="#cb18-594"></a>        <span class="cf">return</span> comparison_data</span>
<span id="cb18-595"><a href="#cb18-595"></a>    </span>
<span id="cb18-596"><a href="#cb18-596"></a>    <span class="kw">def</span> get_year(<span class="va">self</span>, name):</span>
<span id="cb18-597"><a href="#cb18-597"></a>        year_map <span class="op">=</span> {</span>
<span id="cb18-598"><a href="#cb18-598"></a>            <span class="st">'ResNet-18'</span>: <span class="dv">2015</span>,</span>
<span id="cb18-599"><a href="#cb18-599"></a>            <span class="st">'ResNet-50'</span>: <span class="dv">2015</span>,</span>
<span id="cb18-600"><a href="#cb18-600"></a>            <span class="st">'EfficientNet-B0'</span>: <span class="dv">2019</span>,</span>
<span id="cb18-601"><a href="#cb18-601"></a>            <span class="st">'ViT-Base'</span>: <span class="dv">2020</span>,</span>
<span id="cb18-602"><a href="#cb18-602"></a>            <span class="st">'DINOv2-Base'</span>: <span class="dv">2023</span></span>
<span id="cb18-603"><a href="#cb18-603"></a>        }</span>
<span id="cb18-604"><a href="#cb18-604"></a>        <span class="cf">return</span> year_map.get(name, <span class="st">'Unknown'</span>)</span>
<span id="cb18-605"><a href="#cb18-605"></a>    </span>
<span id="cb18-606"><a href="#cb18-606"></a>    <span class="kw">def</span> get_type(<span class="va">self</span>, name):</span>
<span id="cb18-607"><a href="#cb18-607"></a>        <span class="cf">if</span> <span class="st">'ResNet'</span> <span class="kw">in</span> name <span class="kw">or</span> <span class="st">'EfficientNet'</span> <span class="kw">in</span> name:</span>
<span id="cb18-608"><a href="#cb18-608"></a>            <span class="cf">return</span> <span class="st">'CNN'</span></span>
<span id="cb18-609"><a href="#cb18-609"></a>        <span class="cf">elif</span> <span class="st">'ViT'</span> <span class="kw">in</span> name:</span>
<span id="cb18-610"><a href="#cb18-610"></a>            <span class="cf">return</span> <span class="st">'Transformer'</span></span>
<span id="cb18-611"><a href="#cb18-611"></a>        <span class="cf">elif</span> <span class="st">'DINOv2'</span> <span class="kw">in</span> name:</span>
<span id="cb18-612"><a href="#cb18-612"></a>            <span class="cf">return</span> <span class="st">'Foundation Model'</span></span>
<span id="cb18-613"><a href="#cb18-613"></a>        <span class="cf">return</span> <span class="st">'Unknown'</span></span>
<span id="cb18-614"><a href="#cb18-614"></a>    </span>
<span id="cb18-615"><a href="#cb18-615"></a>    <span class="kw">def</span> visualize_comparison(<span class="va">self</span>, data):</span>
<span id="cb18-616"><a href="#cb18-616"></a>        <span class="co">"""Visualize model comparison"""</span></span>
<span id="cb18-617"><a href="#cb18-617"></a>        <span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb18-618"><a href="#cb18-618"></a>        </span>
<span id="cb18-619"><a href="#cb18-619"></a>        df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb18-620"><a href="#cb18-620"></a>        </span>
<span id="cb18-621"><a href="#cb18-621"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb18-622"><a href="#cb18-622"></a>        </span>
<span id="cb18-623"><a href="#cb18-623"></a>        <span class="co"># Parameters vs Year</span></span>
<span id="cb18-624"><a href="#cb18-624"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb18-625"><a href="#cb18-625"></a>        <span class="cf">for</span> model_type <span class="kw">in</span> df[<span class="st">'Type'</span>].unique():</span>
<span id="cb18-626"><a href="#cb18-626"></a>            subset <span class="op">=</span> df[df[<span class="st">'Type'</span>] <span class="op">==</span> model_type]</span>
<span id="cb18-627"><a href="#cb18-627"></a>            plt.scatter(subset[<span class="st">'Year'</span>], subset[<span class="st">'Parameters (M)'</span>].astype(<span class="bu">float</span>), </span>
<span id="cb18-628"><a href="#cb18-628"></a>                       label<span class="op">=</span>model_type, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb18-629"><a href="#cb18-629"></a>        </span>
<span id="cb18-630"><a href="#cb18-630"></a>        plt.xlabel(<span class="st">'Year'</span>)</span>
<span id="cb18-631"><a href="#cb18-631"></a>        plt.ylabel(<span class="st">'Parameters (Millions)'</span>)</span>
<span id="cb18-632"><a href="#cb18-632"></a>        plt.title(<span class="st">'Model Size Evolution'</span>)</span>
<span id="cb18-633"><a href="#cb18-633"></a>        plt.legend()</span>
<span id="cb18-634"><a href="#cb18-634"></a>        plt.grid(<span class="va">True</span>)</span>
<span id="cb18-635"><a href="#cb18-635"></a>        </span>
<span id="cb18-636"><a href="#cb18-636"></a>        <span class="co"># Model types distribution</span></span>
<span id="cb18-637"><a href="#cb18-637"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-638"><a href="#cb18-638"></a>        type_counts <span class="op">=</span> df[<span class="st">'Type'</span>].value_counts()</span>
<span id="cb18-639"><a href="#cb18-639"></a>        plt.pie(type_counts.values, labels<span class="op">=</span>type_counts.index, autopct<span class="op">=</span><span class="st">'</span><span class="sc">%1.1f%%</span><span class="st">'</span>)</span>
<span id="cb18-640"><a href="#cb18-640"></a>        plt.title(<span class="st">'Model Types Distribution'</span>)</span>
<span id="cb18-641"><a href="#cb18-641"></a>        </span>
<span id="cb18-642"><a href="#cb18-642"></a>        <span class="co"># Size comparison</span></span>
<span id="cb18-643"><a href="#cb18-643"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb18-644"><a href="#cb18-644"></a>        plt.bar(df[<span class="st">'Model'</span>], df[<span class="st">'Size (MB)'</span>].astype(<span class="bu">float</span>))</span>
<span id="cb18-645"><a href="#cb18-645"></a>        plt.xticks(rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb18-646"><a href="#cb18-646"></a>        plt.ylabel(<span class="st">'Size (MB)'</span>)</span>
<span id="cb18-647"><a href="#cb18-647"></a>        plt.title(<span class="st">'Model Size Comparison'</span>)</span>
<span id="cb18-648"><a href="#cb18-648"></a>        </span>
<span id="cb18-649"><a href="#cb18-649"></a>        <span class="co"># Timeline</span></span>
<span id="cb18-650"><a href="#cb18-650"></a>        plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb18-651"><a href="#cb18-651"></a>        timeline_data <span class="op">=</span> df.sort_values(<span class="st">'Year'</span>)</span>
<span id="cb18-652"><a href="#cb18-652"></a>        plt.plot(timeline_data[<span class="st">'Year'</span>], <span class="bu">range</span>(<span class="bu">len</span>(timeline_data)), <span class="st">'o-'</span>)</span>
<span id="cb18-653"><a href="#cb18-653"></a>        <span class="cf">for</span> i, (idx, row) <span class="kw">in</span> <span class="bu">enumerate</span>(timeline_data.iterrows()):</span>
<span id="cb18-654"><a href="#cb18-654"></a>            plt.annotate(row[<span class="st">'Model'</span>], (row[<span class="st">'Year'</span>], i), </span>
<span id="cb18-655"><a href="#cb18-655"></a>                        xytext<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">0</span>), textcoords<span class="op">=</span><span class="st">'offset points'</span>)</span>
<span id="cb18-656"><a href="#cb18-656"></a>        plt.xlabel(<span class="st">'Year'</span>)</span>
<span id="cb18-657"><a href="#cb18-657"></a>        plt.ylabel(<span class="st">'Model Index'</span>)</span>
<span id="cb18-658"><a href="#cb18-658"></a>        plt.title(<span class="st">'Vision Models Timeline'</span>)</span>
<span id="cb18-659"><a href="#cb18-659"></a>        plt.grid(<span class="va">True</span>)</span>
<span id="cb18-660"><a href="#cb18-660"></a>        </span>
<span id="cb18-661"><a href="#cb18-661"></a>        plt.tight_layout()</span>
<span id="cb18-662"><a href="#cb18-662"></a>        plt.show()</span>
<span id="cb18-663"><a href="#cb18-663"></a></span>
<span id="cb18-664"><a href="#cb18-664"></a><span class="co"># Run comparison</span></span>
<span id="cb18-665"><a href="#cb18-665"></a>comparison <span class="op">=</span> VisionModelComparison()</span>
<span id="cb18-666"><a href="#cb18-666"></a>comparison_data <span class="op">=</span> comparison.compare_models()</span>
<span id="cb18-667"><a href="#cb18-667"></a></span>
<span id="cb18-668"><a href="#cb18-668"></a><span class="bu">print</span>(<span class="st">"Vision Models Comparison:"</span>)</span>
<span id="cb18-669"><a href="#cb18-669"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">80</span>)</span>
<span id="cb18-670"><a href="#cb18-670"></a><span class="cf">for</span> data <span class="kw">in</span> comparison_data:</span>
<span id="cb18-671"><a href="#cb18-671"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>data[<span class="st">'Model'</span>]<span class="sc">:15}</span><span class="ss"> | </span><span class="sc">{</span>data[<span class="st">'Year'</span>]<span class="sc">}</span><span class="ss"> | </span><span class="sc">{</span>data[<span class="st">'Type'</span>]<span class="sc">:15}</span><span class="ss"> | "</span></span>
<span id="cb18-672"><a href="#cb18-672"></a>          <span class="ss">f"</span><span class="sc">{</span>data[<span class="st">'Parameters (M)'</span>]<span class="sc">:&gt;8}</span><span class="ss"> M | </span><span class="sc">{</span>data[<span class="st">'Size (MB)'</span>]<span class="sc">:&gt;8}</span><span class="ss"> MB"</span>)</span>
<span id="cb18-673"><a href="#cb18-673"></a></span>
<span id="cb18-674"><a href="#cb18-674"></a>comparison.visualize_comparison(comparison_data)</span>
<span id="cb18-675"><a href="#cb18-675"></a><span class="in">```</span></span>
<span id="cb18-676"><a href="#cb18-676"></a></span>
<span id="cb18-677"><a href="#cb18-677"></a><span class="fu">## The Future: What's Next?</span></span>
<span id="cb18-678"><a href="#cb18-678"></a></span>
<span id="cb18-679"><a href="#cb18-679"></a>As we look ahead, several trends are shaping the future of computer vision:</span>
<span id="cb18-680"><a href="#cb18-680"></a></span>
<span id="cb18-681"><a href="#cb18-681"></a><span class="fu">### 1. **Multimodal Foundation Models**</span></span>
<span id="cb18-682"><a href="#cb18-682"></a><span class="in">```python</span></span>
<span id="cb18-683"><a href="#cb18-683"></a><span class="co"># Future: Models that understand both vision and language</span></span>
<span id="cb18-684"><a href="#cb18-684"></a><span class="co"># Example: CLIP, GPT-4V, LLaVA</span></span>
<span id="cb18-685"><a href="#cb18-685"></a><span class="in">```</span></span>
<span id="cb18-686"><a href="#cb18-686"></a></span>
<span id="cb18-687"><a href="#cb18-687"></a><span class="fu">### 2. **Efficient Architectures**</span></span>
<span id="cb18-688"><a href="#cb18-688"></a><span class="in">```python</span></span>
<span id="cb18-689"><a href="#cb18-689"></a><span class="co"># Trend: Smaller, faster models for mobile devices</span></span>
<span id="cb18-690"><a href="#cb18-690"></a><span class="co"># Example: MobileViT, EfficientViT</span></span>
<span id="cb18-691"><a href="#cb18-691"></a><span class="in">```</span></span>
<span id="cb18-692"><a href="#cb18-692"></a></span>
<span id="cb18-693"><a href="#cb18-693"></a><span class="fu">### 3. **Self-Supervised Learning**</span></span>
<span id="cb18-694"><a href="#cb18-694"></a><span class="in">```python</span></span>
<span id="cb18-695"><a href="#cb18-695"></a><span class="co"># Growing trend: Learning without labels</span></span>
<span id="cb18-696"><a href="#cb18-696"></a><span class="co"># Example: MAE, SimCLR, DINOv2</span></span>
<span id="cb18-697"><a href="#cb18-697"></a><span class="in">```</span></span>
<span id="cb18-698"><a href="#cb18-698"></a></span>
<span id="cb18-699"><a href="#cb18-699"></a><span class="fu">## Your Challenge: Build a Modern Vision Pipeline</span></span>
<span id="cb18-700"><a href="#cb18-700"></a></span>
<span id="cb18-701"><a href="#cb18-701"></a>Now it's your turn to build a complete modern vision system:</span>
<span id="cb18-702"><a href="#cb18-702"></a></span>
<span id="cb18-705"><a href="#cb18-705"></a><span class="in">```{python}</span></span>
<span id="cb18-706"><a href="#cb18-706"></a><span class="co">#| eval: true</span></span>
<span id="cb18-707"><a href="#cb18-707"></a><span class="kw">class</span> ModernVisionPipeline:</span>
<span id="cb18-708"><a href="#cb18-708"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb18-709"><a href="#cb18-709"></a>        <span class="co"># Load multiple models for different tasks</span></span>
<span id="cb18-710"><a href="#cb18-710"></a>        <span class="va">self</span>.classification_model <span class="op">=</span> models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-711"><a href="#cb18-711"></a>        <span class="va">self</span>.feature_extractor <span class="op">=</span> <span class="va">None</span>  <span class="co"># DINOv2 model</span></span>
<span id="cb18-712"><a href="#cb18-712"></a>        <span class="va">self</span>.similarity_engine <span class="op">=</span> DINOv2SimilarityEngine()</span>
<span id="cb18-713"><a href="#cb18-713"></a>    </span>
<span id="cb18-714"><a href="#cb18-714"></a>    <span class="kw">def</span> classify_image(<span class="va">self</span>, image):</span>
<span id="cb18-715"><a href="#cb18-715"></a>        <span class="co">"""Classify image using ResNet"""</span></span>
<span id="cb18-716"><a href="#cb18-716"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb18-717"><a href="#cb18-717"></a>        <span class="cf">pass</span></span>
<span id="cb18-718"><a href="#cb18-718"></a>    </span>
<span id="cb18-719"><a href="#cb18-719"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, image):</span>
<span id="cb18-720"><a href="#cb18-720"></a>        <span class="co">"""Extract features using DINOv2"""</span></span>
<span id="cb18-721"><a href="#cb18-721"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb18-722"><a href="#cb18-722"></a>        <span class="cf">pass</span></span>
<span id="cb18-723"><a href="#cb18-723"></a>    </span>
<span id="cb18-724"><a href="#cb18-724"></a>    <span class="kw">def</span> find_similar_images(<span class="va">self</span>, query_image, database):</span>
<span id="cb18-725"><a href="#cb18-725"></a>        <span class="co">"""Find similar images using DINOv2 features"""</span></span>
<span id="cb18-726"><a href="#cb18-726"></a>        <span class="co"># Your implementation here</span></span>
<span id="cb18-727"><a href="#cb18-727"></a>        <span class="cf">pass</span></span>
<span id="cb18-728"><a href="#cb18-728"></a>    </span>
<span id="cb18-729"><a href="#cb18-729"></a>    <span class="kw">def</span> analyze_image(<span class="va">self</span>, image):</span>
<span id="cb18-730"><a href="#cb18-730"></a>        <span class="co">"""Complete image analysis pipeline"""</span></span>
<span id="cb18-731"><a href="#cb18-731"></a>        results <span class="op">=</span> {</span>
<span id="cb18-732"><a href="#cb18-732"></a>            <span class="st">'classification'</span>: <span class="va">self</span>.classify_image(image),</span>
<span id="cb18-733"><a href="#cb18-733"></a>            <span class="st">'features'</span>: <span class="va">self</span>.extract_features(image),</span>
<span id="cb18-734"><a href="#cb18-734"></a>            <span class="st">'similar_images'</span>: <span class="va">self</span>.find_similar_images(image, <span class="va">self</span>.image_database)</span>
<span id="cb18-735"><a href="#cb18-735"></a>        }</span>
<span id="cb18-736"><a href="#cb18-736"></a>        <span class="cf">return</span> results</span>
<span id="cb18-737"><a href="#cb18-737"></a></span>
<span id="cb18-738"><a href="#cb18-738"></a><span class="co"># Build your pipeline!</span></span>
<span id="cb18-739"><a href="#cb18-739"></a>pipeline <span class="op">=</span> ModernVisionPipeline()</span>
<span id="cb18-740"><a href="#cb18-740"></a><span class="in">```</span></span>
<span id="cb18-741"><a href="#cb18-741"></a></span>
<span id="cb18-742"><a href="#cb18-742"></a><span class="fu">## What's Coming Next?</span></span>
<span id="cb18-743"><a href="#cb18-743"></a></span>
<span id="cb18-744"><a href="#cb18-744"></a>In our next post, <span class="co">[</span><span class="ot">**"Your First CV Project: Putting It All Together"**</span><span class="co">](../08-first-cv-project/)</span>, we'll:</span>
<span id="cb18-745"><a href="#cb18-745"></a></span>
<span id="cb18-746"><a href="#cb18-746"></a><span class="ss">- </span>**Build a complete computer vision application**</span>
<span id="cb18-747"><a href="#cb18-747"></a><span class="ss">- </span>**Combine classical and modern techniques**</span>
<span id="cb18-748"><a href="#cb18-748"></a><span class="ss">- </span>**Deploy your model for real-world use**</span>
<span id="cb18-749"><a href="#cb18-749"></a><span class="ss">- </span>**Create an interactive demo**</span>
<span id="cb18-750"><a href="#cb18-750"></a></span>
<span id="cb18-751"><a href="#cb18-751"></a>You've just learned about the most advanced vision models ever created—next, we'll put everything together into a real project!</span>
<span id="cb18-752"><a href="#cb18-752"></a></span>
<span id="cb18-753"><a href="#cb18-753"></a><span class="fu">## Key Takeaways</span></span>
<span id="cb18-754"><a href="#cb18-754"></a></span>
<span id="cb18-755"><a href="#cb18-755"></a><span class="ss">- </span>**CNNs dominated** computer vision for a decade</span>
<span id="cb18-756"><a href="#cb18-756"></a><span class="ss">- </span>**Vision Transformers** brought attention mechanisms to vision</span>
<span id="cb18-757"><a href="#cb18-757"></a><span class="ss">- </span>**Foundation models** like DINOv2 learn universal representations</span>
<span id="cb18-758"><a href="#cb18-758"></a><span class="ss">- </span>**Self-supervised learning** eliminates the need for labels</span>
<span id="cb18-759"><a href="#cb18-759"></a><span class="ss">- </span>**Modern pipelines** combine multiple approaches</span>
<span id="cb18-760"><a href="#cb18-760"></a><span class="ss">- </span>**The field evolves rapidly**—stay curious and keep learning!</span>
<span id="cb18-761"><a href="#cb18-761"></a></span>
<span id="cb18-762"><a href="#cb18-762"></a>:::{.callout-tip}</span>
<span id="cb18-763"><a href="#cb18-763"></a><span class="fu">## Hands-On Lab</span></span>
<span id="cb18-764"><a href="#cb18-764"></a>Ready to experiment with cutting-edge vision models? Try the complete interactive notebook: <span class="co">[</span><span class="ot">**Modern Vision Models Lab**</span><span class="co">](https://colab.research.google.com/drive/1Modern_Vision_Models_123456)</span></span>
<span id="cb18-765"><a href="#cb18-765"></a></span>
<span id="cb18-766"><a href="#cb18-766"></a>Compare CNNs, Vision Transformers, and DINOv2 on your own images!</span>
<span id="cb18-767"><a href="#cb18-767"></a>:::</span>
<span id="cb18-768"><a href="#cb18-768"></a></span>
<span id="cb18-769"><a href="#cb18-769"></a>:::{.callout-note}</span>
<span id="cb18-770"><a href="#cb18-770"></a><span class="fu">## Series Navigation</span></span>
<span id="cb18-771"><a href="#cb18-771"></a><span class="ss">- </span>**Previous**: <span class="co">[</span><span class="ot">Why Deep Learning? When Classical Methods Hit the Wall</span><span class="co">](07-why-deep-learning.qmd)</span></span>
<span id="cb18-772"><a href="#cb18-772"></a><span class="ss">- </span>**Next**: <span class="co">[</span><span class="ot">Your First CV Project: Putting It All Together</span><span class="co">](09-first-cv-project.qmd)</span></span>
<span id="cb18-773"><a href="#cb18-773"></a><span class="ss">- </span>**Series Home**: <span class="co">[</span><span class="ot">Computer Vision Foundations</span><span class="co">](../computer-vision-foundations.qmd)</span></span>
<span id="cb18-774"><a href="#cb18-774"></a>:::</span>
<span id="cb18-775"><a href="#cb18-775"></a></span>
<span id="cb18-776"><a href="#cb18-776"></a>---</span>
<span id="cb18-777"><a href="#cb18-777"></a></span>
<span id="cb18-778"><a href="#cb18-778"></a>*You've just explored the cutting edge of computer vision! From ResNet's skip connections to DINOv2's self-supervised learning—you now understand the models that power today's AI applications. Next, we'll build something amazing with all this knowledge!* </span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



<script src="../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>