[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quarto_blog_hasan",
    "section": "",
    "text": "Matrix Multiplication\n\n\n\n\n\n\n\nfastai\n\n\nmatrix multiplication\n\n\n\n\nMatrix Multiplication from fastai course 2022\n\n\n\n\n\n\nAug 13, 2024\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nHow to Use Nougat to Read Scientific Paper\n\n\nResearch paper working\n\n\n\n\nNlp\n\n\nResearch paper\n\n\nText\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow -05\n\n\nFeature Extraction from Image\n\n\n\n\ncode\n\n\nimage\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow -06\n\n\nValidation Strategy\n\n\n\n\ncode\n\n\ntabular\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow -04\n\n\nFeature Extraction from Text\n\n\n\n\ncode\n\n\ntext\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow -03\n\n\nExploring Anonymized data\n\n\n\n\ncode\n\n\ntabular\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow -02\n\n\nFeature preprocessing and generation\n\n\n\n\ncode\n\n\ntabular\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow -01\n\n\nExplanatory data analysis steps\n\n\n\n\ncode\n\n\ntabular\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2022\n\n\nHasan Goni\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html",
    "href": "posts/data-science-steps-to-follow-part06/index.html",
    "title": "Data Science Steps to Follow -06",
    "section": "",
    "text": "This is a summary of the course How to win a data science competition: Learn from top Kagglers. The course is available on coursera. The course is taught by Yandex and MIPT. This is my note taken from that course."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#concept-of-validation",
    "href": "posts/data-science-steps-to-follow-part06/index.html#concept-of-validation",
    "title": "Data Science Steps to Follow -06",
    "section": "Concept of validation",
    "text": "Concept of validation\n\nWe want to make sure, whether model is performing well on unseen data. Very easy way to understand that, e.g. you went to class for taking a subject, but you didn’t understand the topics well. So you just memorized it without understanding its detail. During exam the professor asked you similar question but in a different way. So you could not answer it, because you just did cramming. In case of validaiton, if model did not learn the function well, then with unseen data, it will perform bad.\nNormally we have some training data. And we have some test data, which we never seen during training and also we don’t have label for those data.\nThen we split our given training data into two parts.\n\nOne part is used for training the model.\nOther part is used for validation. Be careful here, there is a possibility we sometime overfit on validation data, so we need to be very careful here.\n\n\n\nOverfitting and underfitting\n\nUnderfitting: If model is not able to learn the function well, then it is called underfitting. In this case, model will perform bad on both training and validation data.\nOverfitting: If we use a very complex function, then it is called overfitting. In this case, model will perform good on training data, but it will perform bad on validation data.\nFollowing figure shows the concept of underfitting and overfitting.\n\n\n\n\noverfitting\n\n\n\nAnother useful way to understand overfitting and underfitting is to use following figure.\n\n\n\n\noverfitting"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#validation-strategies",
    "href": "posts/data-science-steps-to-follow-part06/index.html#validation-strategies",
    "title": "Data Science Steps to Follow -06",
    "section": "Validation Strategies",
    "text": "Validation Strategies\n\nCommon validation types are\n\nHoldout validation\nK-fold validation\nLeave-one-out validation\n\n\n\nHoldout validation\nflowchart TB\n    A[data] --&gt; B[Validation data]\n    A[data] --&gt; C[Training data]\n\nSo here one sample can go to either validation data or training data. So we can have different validation data and training data. So we can have different validation score. After selecting best model, we can select all the data and train model will all data.\n\nfrom sklearn.model_selection import ShuffleSplit\n\nIt is a good choice when we have enough data for validation.\n\n\n\nK-fold validation\n\nK-fold is actually repeated holdout validation. Split the data in K folds.\nIterate though each fold: retrain the model on all folds except current fold, predict for the current fold.\nUse the predictions to calculate quality on each fold. Find such hyper-parameters, that quality on each fold is maximized. You can also estimate mean and variance of the loss. This is very helpful in order to understand significance of improvement.\n\nflowchart TB\n    A[data] --&gt; B[Fold 1]\n    B[Fold 1] --&gt; C[Training data]\n    B[Fold 1] --&gt; D[Validation data]\n\n    A[data] --&gt; E[Fold 2]\n    E[Fold 2] --&gt; F[Training data]\n    E[Fold 2] --&gt; G[Validation data]\n\n    A[data] --&gt; H[Fold 3]\n    H[Fold 3] --&gt; I[Training data]\n    H[Fold 3] --&gt; J[Validation data]\n\n    C(Training data) --&gt; K[Average]\n      D(Training data) --&gt; K[Average]\n    F(Training data) --&gt; K[Average]\n      G(Training data) --&gt; K[Average]\n    I(Training data) --&gt; K[Average]\n      J(Training data) --&gt; K[Average]\n\n    K(Average) --&gt; L[Final model performance]\nsklearn.model_selection.KFold\n\nit is good choice when we have major amount of data.\n\n\n\nLeave-one-out validation\nIterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model N times (if N is the number of samples in the dataset).\nIn the end you will get LOO predictions for every sample in the trainset and can calculate loss.\n\nIt is used when we have very small amount of data and sufficient time to trian a model with small amount of time."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#stratification",
    "href": "posts/data-science-steps-to-follow-part06/index.html#stratification",
    "title": "Data Science Steps to Follow -06",
    "section": "Stratification",
    "text": "Stratification\n\nwe normally use random split for k fold or hold out. But sometimes random split can fail, because some target sample never went into validaiton set.\nJust to make sure, similar target distribution over different folds, we use stratification.\nStratification is usefull for\n\nsmall dataset\nUnbalanced dataset\nMalticlass classification\n\n\n\n\n\n\n\n\nNotice, that these are validation schemes are supposed to be used to estimate quality of the model. When you found the right hyper-parameters and want to get test predictions don’t forget to retrain your model using all training data"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#most-frequent-ways-to-generate-train-test-split",
    "href": "posts/data-science-steps-to-follow-part06/index.html#most-frequent-ways-to-generate-train-test-split",
    "title": "Data Science Steps to Follow -06",
    "section": "Most frequent ways to generate train-test split",
    "text": "Most frequent ways to generate train-test split\n\nTime series data\n\nIf we create features which is time based, then we need to split with time into account\nIf we want to create a random split, then we need to create features which don’t account time in it. Here may be time base features will not work, although in case of time data, this model could be the best model.\n\nDifferent splitting strategies can differ significantly\n\nin generated features\nin a way the model will rely on that features\nin some kind of target leak\n\nSplitting data\n\nRandom split, row-wise (assumption is rows are independent)\nTime based split (assumption some type of time dependency)\n\nspecial case of time based validaiton is moving window validation\n\nBy id\n\nMay be user, shop or item id\nUser which was in train set, should not be in test set\n\nCombined\n\nfor example we have to predict sales for a shop and a shelf of this shop\nin that case we need to search for shope and date in this shop\nor in case of search result for different users, we can split based on different user id and search engine"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#problems-occurinng-during-validation",
    "href": "posts/data-science-steps-to-follow-part06/index.html#problems-occurinng-during-validation",
    "title": "Data Science Steps to Follow -06",
    "section": "Problems occurinng during validation",
    "text": "Problems occurinng during validation\n\nProblem in validation stage.\nProblem in leader -board or when in production\n\n\na. Problem in validation stage\n\nActually we find it during validation\n\ne.g. we have a model which predict the sells in Febraury. Say we have model and our validation set is for Month January. In January the holidays are signficantly more than February. And people tend to buy more in January. So our model will perform well in January, but it will perform bad in February. So we need to be careful about this.\nThis type of behviour is expected. But in case of we can already know what could be the problem in validation low score.\nTool little data.\n\nThere are lots of patterns in data and we have little data to genrate such patterns.\n\nToo diverse and inconsistant data\n\nsimilar features but different target values. One of them in train and other in test. Then we will get error which is big. In case of both in validation data, then we will get less error.\nIn case of January and February problem, this is expected, we can remove such diverstiy if we can validate it with last year February data.\n\n\n\n\n\nSolution\n\nWe need to do more thorogh validation on such case.\n\nAverage score from different KFold splits ( normally 5 folds are enough , but can be increased). May be try different random seed and then try to estimate the model quality from them.\nTune model on one split, evaluate score on the other split. In case of hyperparameter tuning.\n\n\n\n\nb. Problem in leader-board or when in production\n\nDuring serving the score is constantly higher, lower than validaiton score\nDuring serving the score is not correlated with validation score\n\n\nThese problems can be much more problematic. Because we can’t find it during validation. The solution for this type of problem, is somehow recreate train-test split during validation, which is not easy.\nHighly recommended to submit solution or in case of production, compare with production data, as eary as possible.\n\n\nAs we already know, we have different type of score in differnt validation. So we can assume leader-board/production is another type of validation. If we have different score on different fold and differnt result on leader-board/production result is not surprising. We can calculate mean and standard deviation of validaiton scores and estimate if the leader-board/produciton score is expected or not.\nIf this is not the case then there is something which is wrong.\n\nToo little data, which ok. Trust your validation and it will be ok.\nTrain and test are from differnt distribution. (here train test means train and production)\n\n\n\nIf the train data just see the men and test data just see women, so model will have problem in production. So we need to make sure that train and test data are from same distribution.\nSolution:\n\nWe need to find a way to tackle differnt distribution in train and test. Sometimes it is possible by adjusting during training. But sometimes it is only possible to adjust production/leader-board data.\nIn this particular solution, we can some how figure out a constant from train and test data\n\nMean from train data\nMean from test data\n\nThen shift the prediciton with this difference.\nNormally this type of case is rare. However the most frequent type of problem is following.\n\n\n\n\nleaderboard_problem\n\n\n\nThen as we said, try to create a validation set , which reflects the test data. So we need to create a similar distribution in validation data. Then we can find out the problem.\n\n\n\n\nminimize_difference"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/00_matmul.html",
    "href": "posts/matrix_multiplication_from_fastai_course/00_matmul.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "This notebook is actually reprodcution fo course practical deep learning for coders part2. Matrix Multiplication starts with Lession 11 (1:08:47).\nTo get the noebook shown in the video, one should clone the course repository and run the notebook 01_matmul.ipynb."
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#first-description",
    "href": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#first-description",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "This notebook is actually reprodcution fo course practical deep learning for coders part2. Matrix Multiplication starts with Lession 11 (1:08:47).\nTo get the noebook shown in the video, one should clone the course repository and run the notebook 01_matmul.ipynb."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html",
    "href": "posts/data-science-steps-to-follow-part02/index.html",
    "title": "Data Science Steps to Follow -02",
    "section": "",
    "text": "This is a second part of the class note of how to proceed to a data science project. First part can be found here. In this part I will try to write down, how prprocessing steps vary based on model selection. What type of preprocessing needs to be done on different types of data and how to handle missing values in a project."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#numeric-features",
    "href": "posts/data-science-steps-to-follow-part02/index.html#numeric-features",
    "title": "Data Science Steps to Follow -02",
    "section": "Numeric features",
    "text": "Numeric features\n\nPrprocessing\n\nModel Types\n\nTree based models\n\nNormally if features are multiplied with some numbers, then model performance doesnot change, thats Noralization doesnot necessary for tree based models\n\nNon-tree based models\n\nBut in case of non tree based model scaling and normalization can affect the model\nSome non tree based models are\n\nk nearest neighbors\nlinear models\nnueral networks\n\n\n\nPreprocessing: Scaling\n\nTo[0, 1] sklearn.preprocessing.MinMaxScaler \\[ X = (X - X.min()) / (X.max() - X.min()) \\]\nTo [mean=0 std=1] sklearn.preprocessing.StandardScaler \\[ X = (X - X.mean()) / X.std() \\]\n\nPreprocessing: Outliers\n\nCan be found both in features and outputs\nIf outliers are present in features, then we can use robust scaler\nWe can also use lower bound and upper bound between two values, may be 0.01 and 0.99 percentile, well known in finance called winsorization\n\npython     UPPER_BOUND, LOWER_BOUND = np.percentile(X, [1, 99])     y = np.clip(y, UPPER_BOUND, LOWER_BOUND)     pd.Series(y).hist(bins=30)\n\nAnother way to preprocess is rank transformation, which actually sets spaces between proper assorted values to to equal, it could be a better option from MInMaxScaler, if we have outliers in features, because rank transformation will move the outlier to closer to object\n\nLinear models, neural networks, k nearest neighbors can benefits from this, if we have no times to handle outliers manually\n   scipy.stats.rankdata\nOne important things about rank transformaiton, when applying to test data, you need to store the created mapping from feature values to rank values. Or alternatively you can concatenate train and test data and apply rank transformation to the whole dataset, and then split it back to train and test\n\n\nPreprocessing: Transformation\n\nThere is one more preprocessing which often helps to non tree based models, specially neural networks.\n\nIt is called log transformation\n  np.log(1 + X)\nOr Raising to power &lt;1:\n  np.sqrt(X, 2/3) \nBoth this transformations could be helpful, because they drive too big values closer to features average value, so values near 0 a bit distuinguishable. Despite simplicity, this transformation could be helpful in many cases for neural network. Also data seems to be normllay distributed after this transformation (if not try to use other transformation and see histogram[Andrew ng])\n\nSometimes it is benefecial to apply concatenated dataframes produced be different preprocessing, or to mix models training different pre-porcessed data. Specially Linear models, KNN and Neural networks can benefit from this\n\n\n\n\nFeature generation\n\nWays to proceed :\n\nprior knowledge\nEDA(exploratory data analysis)\nExample:\n\nWe have Real state price and real state squared data, from this we can create a new feature which is price per square meter\nForest cover type prediction dataset, if we have horizontal and vertical distance from hydrology, we can create a new feature which is distance from hydrology, \\[ combined = \\sqrt{horizontal\\_distance\\_to\\_hydrology^2 + vertical\\_distance\\_to\\_hydrology^2} \\]\n\nThis type of featur generation is not only helpful for neural network and linear models, but also for tree based models, sometimes we can have a peak in performance with less amount of trees with thiess type of feature generation\n\nAnother example is, if we have a price of a product, we can take fraction of the price. for example if the price is 9.99 the fractionaly part of the price is 0.99. This type of feature of features also can distinguish between robot and human. Normally in finance, this is very helpful. As a human we normally use whole number.\n\nThis is just some example, one can be very creative and can get a peak in model performance with feature generation"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#categorical-and-ordinal-features",
    "href": "posts/data-science-steps-to-follow-part02/index.html#categorical-and-ordinal-features",
    "title": "Data Science Steps to Follow -02",
    "section": "Categorical and Ordinal features",
    "text": "Categorical and Ordinal features\n\nPreprocessing\n\nIn Titanice dataset, caegorical features are Sex, Cabin and Embarked and ordinal features are Pclass, which stands for passenger class. We have a value [1,2,3] for PClas, which is ordered based on the price of the ticket, so it is ordinal feature.\nOne needs to be very careful here, because if we put PClass as a numeric features, we say the difference between class 1 and class 2 is the same as the difference between class 2 and class 3, which is not true.\nOther ordinal features are\n\nDriver’s license: A, B, C, D, E, F, G\nEducation: Elementary, Middle, High, College, Master, PhD\n\nFor Ordinal features one can use LabelEncoder, which will assign a number to each category, but it will not be ordered, so we need to use OrdinalEncoder, In case of tree based models, such model helps, but in case of non tree based models, you need to use the preprocess the data in a different way.\nFor Titanic dataset again, let’s see the Embarked feature, which has category S, C, Q.\n\nAlphabetical : [S, C, Q] -&gt; [2, 1, 3]\n  from sklearn.preprocessing import LabelEncoder\n  le = LabelEncoder()\n  le.fit_transform(df['Embarked'])\nOrder of apperance:\n[S, C, Q] -&gt; [1, 2, 3] S comes first to the data and C comes after that\npandas.factorize\nFrequency Encodeing: [S, C, Q] -&gt; [0.5, 0.3, 0.2] S is the most frequent category and Q is the least frequent category\nencoding = df.groupby('Embarked').size() / len(df)\ndf['enc'] = df['Embarked'].map(encoding)\n\nOne important thing regarding frequency encoding, if we have multiple features with same frequency, then the features are not distinguishable, so we need to rank transformation\n\nencoding = df.groupby('Embarked').size() / len(df)\ndf['enc'] = df['Embarked'].map(encoding)\nfrom scipy.stats import rankdata\nThere are different encodings possible, one can be very creative here, one need to try different encodings here.\nCategorical features for non-tree based models\n\nOne hot encoding\n\npd.get_dummies, sklearn.preprocessing.OneHotEncoder\n\nTree methods will slow down here, because they will have to split on each category, so one hot encoding is not good for tree based models\nIf too many unique categories, then one hot encoding will create too many features, To work with them we need to know how to handle sparse data.XGBoost, LightGBM, CatBoost can handle sparse data, but other models cannot handle sparse data, so we need to use some other methods to handle sparse data.\n\n\n\n\n\nFeature generation\n\nFeature interaction between several categorica features -&gt; normally usefull linear model and KNN, e.g. Titanic dataset, sex and Pclass. We can concatenate both features and then one hot encode them. Advanced feature generation will be discussed in the next sections."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#datetime-and-coordinates",
    "href": "posts/data-science-steps-to-follow-part02/index.html#datetime-and-coordinates",
    "title": "Data Science Steps to Follow -02",
    "section": "Datetime and coordinates",
    "text": "Datetime and coordinates\n\nFeature generation:Datetime\n\nMost of the time, we have a datetime feature, which is a string, we need to convert it to a datetime object, so we can extract different features from it, such as year, month, day, hour, minute, second, day of week, day of year, week of year, quarter, etc.\nNormally it falls into three categories\n\nPeridocity: year, month, day, hour, minute, second, day of week, day of year, week of year, quarter, etc.\n\nhelpful to capture repetative patterns in the data\n\nTime Since:\n\nRow independent moment, e.g. time since last purchase, time since last login, time since last visit, day since lasth holiday, day to next holiday, etc.\nRow dependent moment, e.g. since 00:00:00 UTC, 1 January 1970, etc.\n\nDifference between dates:\n\ndatetime_feature_1 - datretime_feature_2. e.g. churn prediciton. The likelihood, the customers will churn. feature: last_purchase_date, last_call_date, date_diff = last_purchase_date - last_call_date\n\nAfter generating features in datetime, we will get numeric or categorical features, so we need to apply the same preprocessing as we did for numeric and categorical features.\n\n\n\n\nFeature generation: Coordinates\n\nDistance:\n\nReal state price prediction. We can create new features to calculate distance from the city center, distance from the airport, distance from the sea, etc. If such data is not available we can extract some features from the maps. We can convert the map into differnt squares and extract the most expensive flats in this square and then calculate the distance from the center of the square. We can also use the same method to calculate the distance from the airport, sea, etc.\n\nCreate cluster from datapoints and calculate the center of the cluster and then calculate the distance from the center of the cluster.\nFind some very special areas and add distance from these areas.\n\n\nAggreagated statistics, objects sorrounding area:\n\nCalcualte number of important place in sorrounding area, which can be interpreted as the quality of the area.\nOr mean price of the sorrounding area. How expensive sorrounding area is.\n\nIn case of decision trees, slightyl rotated coordinates can be helpful, because decision trees are not able to capture the rotation of the data."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#missing-values",
    "href": "posts/data-science-steps-to-follow-part02/index.html#missing-values",
    "title": "Data Science Steps to Follow -02",
    "section": "Missing Values",
    "text": "Missing Values\n\nExamples of missing values\n\nNA Values\nEmpty string\n-1\nVery large number\n-99999 (or less)\n999\n99\n\nhow to know, whether this is a missing value or not. plot histogram, and see some values are very different from other values, So missing values can be hidden means may be used other values in plac of missing values.\nFillna approaches:\n\n\n-999, -1 etc -&gt; for Neural networks it will be difficult\nmean, median, mode -&gt; for trees it will be harder to find differnt splits\nReconsturct value\n\n\nFeature generation:\nNeed to create a new column named as is_missing, which will be 1 if the value is missing and 0 if the value is not missing. This will help the model to capture the missing values. -&gt; help both tree an neural networks but only problem is number of columns will increated\nCarful during missing value imputation, e.g. we have two features, one is datetime and temperature of whole year. If we impute the missing values with mean, then the mean will be calculated from the whole year, which is not correct, because the temperature is not same in the whole year. So we need to calculate the mean for each month and then impute the missing values with the mean of the month. Then we creeate a new feature, which will be the difference between the mean of the month and the actual value. In case of imputation with mean, it will be 0, as we already impute it with mean, which is 0 , so new feature will not create some sense, or in other way new feature will not help the model. So we need to be careful during imputation. Unfortunately we don’t have enough time to be careful here.\nFor example we have a categorical featue, we have created a numeric feature from this categorical feature. So A-&gt;1, B-&gt;9, B-&gt;?, if we impute missing value with some outer range value like -999 in missing value. Then the category will be going to closer to -999 and the more missing value and category will go more closer to this big or small number. In case of mean and median imputation it will also a problem. So we need to be careful during imputation. Simple solution will be ignore missing value, while calculating the mean and median.\nXGBoost, LightGBM, CatBoost can handle missing values.\nSometimes outliers can be handled as missing values.\nTreat missing values which appeear in test but not in trainig data.\n\nCreate a category which will tell how frequently occures this category. If there is some correlation between occurence frequeny and target, then it will sucessly tell the prediction.\n\n\n\nSummray missing values\n\nThe choice of filling depends on situation\nUsual way -999, mean, median, mode but careful\nMissing values sometimes hidden.\nBinary feature is_missing is helpful\navoid missing values during feature generation\nXGBoost can handle missing values\n\nNext post can be found here"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html",
    "href": "posts/data-science-steps-to-follow-part04/index.html",
    "title": "Data Science Steps to Follow -04",
    "section": "",
    "text": "This is a part of series. First, Second, Third parts are connected in link. In this part I will try to write down about text data # Feature extraction from text\n\n\nBag of words\nEmbeddings(~word2vec)\n\n\n\n1.1. CountVectorizer\n\neach word is separated and count number of occurences\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n\nWe may be need to do some post processing. As we know KNN, neural networks are sensitive to the scale of the features. So we need to scale the features. We can use TF-IDF to do this.\n\n1.2. TfidfVectorizer\n\nWhat actually is just not frequency but normalized frequency.\nTerm frequency:\n\ntf = 1/ x.sum[axis=1](:,None)\nx = x * tf\n\nInverse document frequency:\n\nidf = np.log(x.shape[0]/(x&gt;0).sum(axis=0)))\nx = x*idf\nsklearn.feature_extraction.text.TfidfVectorizer\n1.3 N-grams\n\nNot only words but n-consequent words\n\nsklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2)) \n# may be parameter analyzer\n\n\n\nActually before applying any Bag of words we need to preprocess the text. We need to remove the stop words, stemming, lemmatization, etc.* Conventionally preprocessing are\n\nTokenization -&gt; Very very sunny day -&gt; [Very, very, sunny, day]\nLowercasing -&gt; [very, very, sunny, day] -&gt; [very, very, sunny, day] -&gt;CountVectorizer from sklearn will automatically do this\nRemoving punctuation\nRemoving stopwords -&gt; [The cow jumped over the moon] -&gt; [cow, jumped, moon]\n\nAriticles or preprositon words\nVery common words\nCan be used NLTK library\nsklearn.feature_extraction.text.CountVectorizer(max_df)\nmax_df is the frequency threshold, after which the word is removed\n\nStemming/Lemmatization\nStemming\n\n[democracy, democratic, democratization] -&gt; [democr]\n[Saw] -&gt; [s]\n\nLemitization\n\n[democracy, democratic, democratization] -&gt; [democracy]\n[Saw, sawing, sawed] -&gt; [see or saw] depending on text\n\n\n\n\n\n\n\n\nPreprocessing Lowercasing, removing punctuation, removing stopwords, stemming/lemmatization\nN-grams helps to get local context\nPost processing TF-IDF\n\n\n\n\n\n\n\nVector representation of words and text\nEach word is represented as a vector, in some sophisticated way, which could have 100 dimensions or more.\nSame words will have similar vectors. king-&gt;queen\nAlso addition and subtraction of vectors will have some meaning. -&gt; king + woman - man = queen\nSeveral implementaton of word2vec\n\nWord2vec\nGlove\nFastText\n\nSentences\n\nDoc2vec\n\nBased on situation we can use word or sentence embeddings. Actually try both and take the best one.\nAll the preprocessing steps can be applied to the text before applying word2vec.\n\n\n\n\n\nBag of words\n\nVery large vector\nmeaning is easy value in vector is known\n\nWord2vec\n\nRelative Small vector\nValues of vector can be interpreted only some cases\nThe words with simlar meaning will have similar embeddings\n\n\nNext post can be found here"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html#bag-of-words",
    "href": "posts/data-science-steps-to-follow-part04/index.html#bag-of-words",
    "title": "Data Science Steps to Follow -04",
    "section": "",
    "text": "1.1. CountVectorizer\n\neach word is separated and count number of occurences\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n\nWe may be need to do some post processing. As we know KNN, neural networks are sensitive to the scale of the features. So we need to scale the features. We can use TF-IDF to do this.\n\n1.2. TfidfVectorizer\n\nWhat actually is just not frequency but normalized frequency.\nTerm frequency:\n\ntf = 1/ x.sum[axis=1](:,None)\nx = x * tf\n\nInverse document frequency:\n\nidf = np.log(x.shape[0]/(x&gt;0).sum(axis=0)))\nx = x*idf\nsklearn.feature_extraction.text.TfidfVectorizer\n1.3 N-grams\n\nNot only words but n-consequent words\n\nsklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2)) \n# may be parameter analyzer\n\n\n\nActually before applying any Bag of words we need to preprocess the text. We need to remove the stop words, stemming, lemmatization, etc.* Conventionally preprocessing are\n\nTokenization -&gt; Very very sunny day -&gt; [Very, very, sunny, day]\nLowercasing -&gt; [very, very, sunny, day] -&gt; [very, very, sunny, day] -&gt;CountVectorizer from sklearn will automatically do this\nRemoving punctuation\nRemoving stopwords -&gt; [The cow jumped over the moon] -&gt; [cow, jumped, moon]\n\nAriticles or preprositon words\nVery common words\nCan be used NLTK library\nsklearn.feature_extraction.text.CountVectorizer(max_df)\nmax_df is the frequency threshold, after which the word is removed\n\nStemming/Lemmatization\nStemming\n\n[democracy, democratic, democratization] -&gt; [democr]\n[Saw] -&gt; [s]\n\nLemitization\n\n[democracy, democratic, democratization] -&gt; [democracy]\n[Saw, sawing, sawed] -&gt; [see or saw] depending on text"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html#summray-of-bag-of-words-pipeline",
    "href": "posts/data-science-steps-to-follow-part04/index.html#summray-of-bag-of-words-pipeline",
    "title": "Data Science Steps to Follow -04",
    "section": "",
    "text": "Preprocessing Lowercasing, removing punctuation, removing stopwords, stemming/lemmatization\nN-grams helps to get local context\nPost processing TF-IDF"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html#embeddings",
    "href": "posts/data-science-steps-to-follow-part04/index.html#embeddings",
    "title": "Data Science Steps to Follow -04",
    "section": "",
    "text": "Vector representation of words and text\nEach word is represented as a vector, in some sophisticated way, which could have 100 dimensions or more.\nSame words will have similar vectors. king-&gt;queen\nAlso addition and subtraction of vectors will have some meaning. -&gt; king + woman - man = queen\nSeveral implementaton of word2vec\n\nWord2vec\nGlove\nFastText\n\nSentences\n\nDoc2vec\n\nBased on situation we can use word or sentence embeddings. Actually try both and take the best one.\nAll the preprocessing steps can be applied to the text before applying word2vec.\n\n\n\n\n\nBag of words\n\nVery large vector\nmeaning is easy value in vector is known\n\nWord2vec\n\nRelative Small vector\nValues of vector can be interpreted only some cases\nThe words with simlar meaning will have similar embeddings\n\n\nNext post can be found here"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html",
    "href": "posts/data-science-steps-to-follow-part01/index.html",
    "title": "Data Science Steps to Follow -01",
    "section": "",
    "text": "This is a summary of the course How to win a data science competition: Learn from top Kagglers. The course is available on coursera. The course is taught by Yandex and MIPT. This is my note taken from that course.\nAs the summary seems a little bit long. So I tried to convert it different parts. This is introduction part. In this part I will try to explain what is EDA and how should one proceed with EDA in a project.\n\nFeature preprocessing and generation\nExploring anonymized data\nFeature extraction from text\nFeature extraction from images"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#what-is-eda",
    "href": "posts/data-science-steps-to-follow-part01/index.html#what-is-eda",
    "title": "Data Science Steps to Follow -01",
    "section": "What is EDA",
    "text": "What is EDA\n\nUnderstand the data\nBuild intuition about the data\nGeneratge hypothesis of possible new features\nFind some insights about the data"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#visualization",
    "href": "posts/data-science-steps-to-follow-part01/index.html#visualization",
    "title": "Data Science Steps to Follow -01",
    "section": "Visualization",
    "text": "Visualization\n\nOne EDA tool is visualization -&gt; see the patterns-&gt; what are these patterns -&gt; why we see them\ne.g. There was a competition in kaggle, where you just don’t need anaything, if you just understand it very well\n\nGoal of the competition: whether a person will use the promo, or not\nFeature of the person are the inputs, e.g. age, sex, marital status, etc. and there are features which described the promo.\nBut two intersting features were\n\nNumber of promos sent by the person before\nNUmber of promos the person has used before\n\nIf we sort the rows based on number of promos sent by the person before and we calcualte the difference between number of promos used by the person. Most of the time this difference is equal to target values.\nAlthough it is not given what to do with the Nan values, but we can see that without doing modeling we can get 81% accuracy. It is only possible after doing EDA.\nActually this is very good example of data leakage, done by the person who created the dataset."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#building-intuition",
    "href": "posts/data-science-steps-to-follow-part01/index.html#building-intuition",
    "title": "Data Science Steps to Follow -01",
    "section": "Building intuition",
    "text": "Building intuition\n\nGetting domain knowledge\nChecking the data is intuitive\nUnderstand how data is generated\n\n\n2. Getting domain knowledge\n\nWhat is our goal?\nWhat data we have?\nhow people actually tackle this kind of problem to build this problem to build baseline?\nSo may be first goal is to read wikipedia, do google search, read some blogs, etc. to get some domain knowledge.\ne.g. We need to predict advisers’s cost\n\nFirst to understand this is about web adverstisemnt.\nBy seeing column names we can understand, that data is exported from Google AdWords system.\nAfter reading several articles we get the meaning of the columns.\n\n\n\n\n3. Checking the data is intuitive\n\nAfter getting the domain knowledge, we can check the data is intuitive or not. Whether it agrees with our domain knowledge or not.\n\ne.g. if this is a age data, we can be sure that the age is between 1 and 100. If it is not, then we need to check it. If we get a value 336, may be it’s a typo and it should be 33.6. If it is not, then we need to check it.\nThe value of Clicks columns should be less or equal to Impressions. If it is not, then we need to check it. We need to check whethe it is a error or some kind of logic there.\n\nNote: Fastai founder Jeremy says there is outlier in statistics, but in case of data science, if there is some anomaly, we need to check it and go deep down to understand it.\n\nWe also createa a new feature with incorrect, whether 1 or 1, based on the above conditions.\n\n\n\n\n4. Understand how data is generated\n\nAlgorithm for sampling object from database.\nRandom sample, or over sampling, or under sampling\nValidation scheme can only ge genreated if we understand how data is generated\nIf theere is a diffrence between train and test, then we need to check it, try to understand why it is done and whether to gain any intuition about it.\n\nNext post can be found here"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part05/index.html",
    "href": "posts/data-science-steps-to-follow-part05/index.html",
    "title": "Data Science Steps to Follow -05",
    "section": "",
    "text": "Introduction\n\nThis is a part of the series. First, Second, Third and Fourth parts are connected in link. In this part I will try to write about EDA, when the data is image type\n\n\n\nImages Vector extraction\n\nSimilar to text vector extraction, we can extract features from images\nBesides getting outputs from last layer, we also can get outputs from intermediate layers, we call them descriptors.\nDescriptors from later layers are better to solve task similar to one network was trained on.\nIn contraray descriptors from earlier layers has some task independent features.\nImage net descriptors from last layer can be used for like car classification, but for medical image task earlier layers descriptos may be a better choice or train from scratch.\nFine tuning is a good option to get better results.\n\ne.g. Data Science game 2016. The task was to classify roofs into groups. Log Loss was the metric. Competitors has 8000 images. Fine tuning with image net helps to get better results.\n\nAugmentations:\n\nCreate different versions of same image, may be rotation, random crop, etc.\n\nBe careful with validation and don’t overfit.\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nougat-to-read-scientific-pdf-files/index.html",
    "href": "posts/nougat-to-read-scientific-pdf-files/index.html",
    "title": "How to Use Nougat to Read Scientific Paper",
    "section": "",
    "text": "Copied from here"
  },
  {
    "objectID": "posts/nougat-to-read-scientific-pdf-files/index.html#jupter-notebook-is-the-following",
    "href": "posts/nougat-to-read-scientific-pdf-files/index.html#jupter-notebook-is-the-following",
    "title": "How to Use Nougat to Read Scientific Paper",
    "section": "Jupter notebook is the following",
    "text": "Jupter notebook is the following\n#pip install -q pymupdf python-Levenshtein nltk\nfrom transformers import AutoProcessor, VisionEncoderDecoderModel\nimport torch\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Tuple \nimport io\nimport fitz\nfrom huggingface_hub import hf_hub_download\nfrom PIL import Image\nfrom collections import defaultdict\nfrom transformers import StoppingCriteria, StoppingCriteriaList\nprocessor = AutoProcessor.from_pretrained(\"facebook/nougat-small\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-small\")\nDownloading (…)rocessor_config.json:   0%|          | 0.00/479 [00:00&lt;?, ?B/s]\n\n\n\nDownloading (…)okenizer_config.json:   0%|          | 0.00/4.49k [00:00&lt;?, ?B/s]\n\n\n\nDownloading (…)/main/tokenizer.json:   0%|          | 0.00/2.14M [00:00&lt;?, ?B/s]\n\n\n\nDownloading (…)cial_tokens_map.json:   0%|          | 0.00/96.0 [00:00&lt;?, ?B/s]\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\nDownloading (…)lve/main/config.json:   0%|          | 0.00/4.77k [00:00&lt;?, ?B/s]\n\n\n\nDownloading pytorch_model.bin:   0%|          | 0.00/990M [00:00&lt;?, ?B/s]\n\n\n\nDownloading (…)neration_config.json:   0%|          | 0.00/165 [00:00&lt;?, ?B/s]\n%%capture\n#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = \"cpu\"\nmodel.to(device)\n     \nfilepath = hf_hub_download(repo_id=\"ysharma/nougat\", filename=\"input/nougat.pdf\", repo_type=\"space\")\nDownloading nougat.pdf:   0%|          | 0.00/4.13M [00:00&lt;?, ?B/s]\ndef rasterize_paper(\n    pdf: Path,\n    outpath: Optional[Path] = None,\n    dpi: int = 96,\n    return_pil=False,\n    pages=None,\n) -&gt; Optional[List[io.BytesIO]]:\n    \"\"\"\n    Rasterize a PDF file to PNG images.\n\n    Args:\n        pdf (Path): The path to the PDF file.\n        outpath (Optional[Path], optional): The output directory. If None, the PIL images will be returned instead. Defaults to None.\n        dpi (int, optional): The output DPI. Defaults to 96.\n        return_pil (bool, optional): Whether to return the PIL images instead of writing them to disk. Defaults to False.\n        pages (Optional[List[int]], optional): The pages to rasterize. If None, all pages will be rasterized. Defaults to None.\n\n    Returns:\n        Optional[List[io.BytesIO]]: The PIL images if `return_pil` is True, otherwise None.\n    \"\"\"\n\n    pillow_images = []\n    if outpath is None:\n        return_pil = True\n    try:\n        if isinstance(pdf, (str, Path)):\n            pdf = fitz.open(pdf)\n        if pages is None:\n            pages = range(len(pdf))\n        for i in pages:\n            page_bytes: bytes = pdf[i].get_pixmap(dpi=dpi).pil_tobytes(format=\"PNG\")\n            if return_pil:\n                pillow_images.append(io.BytesIO(page_bytes))\n            else:\n                with (outpath / (\"%02d.png\" % (i + 1))).open(\"wb\") as f:\n                    f.write(page_bytes)\n    except Exception:\n        pass\n    if return_pil:\n        return pillow_images\nimages = rasterize_paper(pdf=filepath, return_pil=True)\nlen(images)\n17\nimage = Image.open(images[0])\nimage\n\n\n\npng\n\n\n# prepare image for the model\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\nprint(pixel_values.shape)\n     \ntorch.Size([1, 3, 896, 672])\nclass RunningVarTorch:\n    def __init__(self, L=15, norm=False):\n        self.values = None\n        self.L = L\n        self.norm = norm\n\n    def push(self, x: torch.Tensor):\n        assert x.dim() == 1\n        if self.values is None:\n            self.values = x[:, None]\n        elif self.values.shape[1] &lt; self.L:\n            self.values = torch.cat((self.values, x[:, None]), 1)\n        else:\n            self.values = torch.cat((self.values[:, 1:], x[:, None]), 1)\n\n    def variance(self):\n        if self.values is None:\n            return\n        if self.norm:\n            return torch.var(self.values, 1) / self.values.shape[1]\n        else:\n            return torch.var(self.values, 1)\nclass StoppingCriteriaScores(StoppingCriteria):\n    def __init__(self, threshold: float = 0.015, window_size: int = 200):\n        super().__init__()\n        self.threshold = threshold\n        self.vars = RunningVarTorch(norm=True)\n        self.varvars = RunningVarTorch(L=window_size)\n        self.stop_inds = defaultdict(int)\n        self.stopped = defaultdict(bool)\n        self.size = 0\n        self.window_size = window_size\n\n    @torch.no_grad()\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n        last_scores = scores[-1]\n        self.vars.push(last_scores.max(1)[0].float().cpu())\n        self.varvars.push(self.vars.variance())\n        self.size += 1\n        if self.size &lt; self.window_size:\n            return False\n\n        varvar = self.varvars.variance()\n        for b in range(len(last_scores)):\n            if varvar[b] &lt; self.threshold:\n                if self.stop_inds[b] &gt; 0 and not self.stopped[b]:\n                    self.stopped[b] = self.stop_inds[b] &gt;= self.size\n                else:\n                    self.stop_inds[b] = int(\n                        min(max(self.size, 1) * 1.15 + 150 + self.window_size, 4095)\n                    )\n            else:\n                self.stop_inds[b] = 0\n                self.stopped[b] = False\n        return all(self.stopped.values()) and len(self.stopped) &gt; 0\n\n# autoregressively generate tokens, with custom stopping criteria (as defined by the Nougat authors)\noutputs = model.generate(pixel_values.to(device),\n                          min_length=1,\n                          max_length=3584,\n                          bad_words_ids=[[processor.tokenizer.unk_token_id]],\n                          return_dict_in_generate=True,\n                          output_scores=True,\n                          stopping_criteria=StoppingCriteriaList([StoppingCriteriaScores()]),\n)\ngenerated = processor.batch_decode(outputs[0], skip_special_tokens=True)[0]\ngenerated = processor.post_process_generation(generated, fix_markdown=False)\nprint(generated)\n# Nougat: Neural Optical Understanding for Academic Documents\n\n Lukas Blecher\n\nCorrespondence to: lblecher@meta.com\n\nGuillem Cucurull\n\nThomas Scialom\n\nRobert Stojnic\n\nMeta AI\n\nThe paper reports 8.1M papers but the authors recently updated the numbers on the GitHub page https://github.com/allenai/s2orc\n\n###### Abstract\n\nScientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (**N**eural **O**ptical **U**nderstanding for **A**cademic Documents), a Visual Transformer model that performs an _Optical Character Recognition_ (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.\n\n## 1 Introduction\n\nThe majority of scientific knowledge is stored in books or published in scientific journals, most commonly in the Portable Document Format (PDF). Next to HTML, PDFs are the second most prominent data format on the internet, making up 2.4% of common crawl [1]. However, the information stored in these files is very difficult to extract into any other formats. This is especially true for highly specialized documents, such as scientific research papers, where the semantic information of mathematical expressions is lost.\n\nExisting Optical Character Recognition (OCR) engines, such as Tesseract OCR [2], excel at detecting and classifying individual characters and words in an image, but fail to understand the relationship between them due to their line-by-line approach. This means that they treat superscripts and subscripts in the same way as the surrounding text, which is a significant drawback for mathematical expressions. In mathematical notations like fractions, exponents, and matrices, relative positions of characters are crucial.\n\nConverting academic research papers into machine-readable text also enables accessibility and searchability of science as a whole. The information of millions of academic papers can not be fully accessed because they are locked behind an unreadable format. Existing corpora, such as the S2ORC dataset [3], capture the text of 12M2 papers using GROBID [4], but are missing meaningful representations of the mathematical equations.\n\nFootnote 2: The paper reports 8.1M papers but the authors recently updated the numbers on the GitHub page https://github.com/allenai/s2orc\n\nTo this end, we introduce Nougat, a transformer based model that can convert images of document pages to formatted markup text.\n\nThe primary contributions in this paper are\n\n* Release of a pre-trained model capable of converting a PDF to a lightweight markup language. We release the code and the model on GitHub3 Footnote 3: https://github.com/facebookresearch/nougat\n* We introduce a pipeline to create dataset for pairing PDFs to source code\n* Our method is only dependent on the image of a page, allowing access to scanned papers and books"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html",
    "href": "posts/data-science-steps-to-follow-part03/index.html",
    "title": "Data Science Steps to Follow -03",
    "section": "",
    "text": "This is third part of the series. First and second part needs to be read for better understanding. In this part description can be found, when we have anonymized data and how to explore it."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html#explore-individual-feature",
    "href": "posts/data-science-steps-to-follow-part03/index.html#explore-individual-feature",
    "title": "Data Science Steps to Follow -03",
    "section": "1. Explore individual feature",
    "text": "1. Explore individual feature\n\nGuess the meaning of columns.\nGuess the types of columns. Separate them numerical, categorical, ordinal, date, text, etc."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations",
    "href": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations",
    "title": "Data Science Steps to Follow -03",
    "section": "2. Explore feature relations",
    "text": "2. Explore feature relations\n\nFind relation between pairs.\nFind feature groups.\n\n\n1. Individual feature\n\nThere was a competition, where like there are different features, which were anonymized means the features names are x1, x2, etc. There were some numerical and some hash value, which could be categorical feature.\nWhat the lecturer did, first create a baseline with random forest. fillna with -999, categorical featueres factorize.\nThen plot feature importance from this baseline model.\nHe found that, one feature named as x8 has highest influence on the target variable.\nSo he starts to invesitage a little bit deeper.\nThen tried to find mean and std values. It seems close to 0 and 1. It seems normalized but it is not exactly 0 and 1 but extra decimal places. May be because of train and test.\nThen search for other reapeatd values by value counts. It seems there are lots of repeted values.\nAs we understood them, they are normalized, we tried to find the normalize parameter, means scaling and shift parameter. Lets’s try to find it, or is it actaully possible.\n\nSearch for unique values and sort them.\nThen use np.diff to find the difference between two consecutive values. It seems the values are same all the time.\nThen devide this values with to our sorted array. It is almost 1. May be not 1 because of some numerical error.\nSo if we devide this vlaue to our feature, we will get the original values. It is also visible each positive number decimal places are same and also each negative number decimal places are same. This could be part of shifting parameter.\nSo we devide to our previous value and substract to decimal place, we get almost integer values.\nAfter that it seems that we are right direction, because we are getting integer values. However we got shifing value, a fractioanal part, but how to get the full part of shifting value.\nSo the lecturer had a hanch. He just tried value counts of integer values. Then he found an extremely different number from other and the value was - 1968 . So he assumed may be it is some kind of year and one person put 0 or forgot to enter, then system converted to 0. So may be the shifting value is 1968. So he tried to substract 1968 from the feature and then he got the original values.\nBut how it helps in the competition. One can use different things from it. But at that competition he could not use this feature. But it was very interseting to see how he found the original values.\n\nif there are small features we can see manually. If there are many features.\ndf.dtypes\ndf.info()\nx.value_counts()\nx.isnull()"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations-1",
    "href": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations-1",
    "title": "Data Science Steps to Follow -03",
    "section": "2. Explore feature relations",
    "text": "2. Explore feature relations\n\nTo explore different features, important things can be done is visualization\n\nExplore individual features\n\n\nHistograms\nplots\nstatistics\n\n\nExplore feature relations\n\n\nScatter plots\nCorrelation plots\nPlot(index vs feature statistics)\nAnd more.\n\n\nEDA is art and visualization is the tool.\n\n\n\n1. Individual features\nplt.hist(x)\n\nSometimes it is misleading. So change number of bins.\nNever make a hypothesis from a single plot. Try to plot different things and then make a decision.\nSometimes in histogram you will see some spikes. It could be anything. Actually in particular case the organizer put the missing vlaue with its mean. We can change this value with other than mean.\nwe can also plot x is index and y is feature value. Not conenct with line but with circles only. python plt.plot(x,'.' )\n\n\n\nindex_image\n\n\nIf we see horizontal line in such plots, it means there are repeated values and if there is no vertical lines, it means the data is shuffled nicely.\nWe can also color code based on labels.\nplt.scater(x,y,c=y)\nPandas describe function also helps a lot\npd.describe()\nx.mean()\nx.var()\nAlso value counts and isnull is very helpfull\nx.value_counts()\nx.isnull()\n\n\n\n2. Feature relation\n\nScatter plot\n\nplt.scatter(x,y)\n\nfor classificaiton we can color map the label\nfor regression heatmap can be used.\nAlso we can compare the scatter plot in trianing and test set is same.\n\n\n\ncolor_code_train_test\n\n\nThe following graph show the diagonal realtion. The equation of a diagonal, x1 -&gt; xaxis and x2-&gt; y axis\n\n\\[x2&lt;=1 -x1\\]\n\nThe equation of diagonal line is $ x1 + x2 = 1 $\n\n\n\n\ndiagonal_equation\n\n\n\nSuppose we found this relaiton but how to use them. There are differet ways but for tree based model we can create the difference or ratio of these two features.\nIf we see the following scatter plot, we can see that there are some outliers. So we can remove them.\n\n\n\nscatter plot\n\n\nSo how this is helpful, our goal is to generate features. How to generate feature from this plot. As we see two traingles, we can create a feature where each triangle will get a set of points and hope this feature will help.\nIf we have smaller number of features, we can use pandas for all features together.\n\npd.scatter_matrix(df)\n\nIt is always good to use scatter plot and histogram in same plot. Scatter plot -&gt; week information about densities, while histogram -&gt; don’t show feature interaction.\nWe can also create a correlation plot. It is a heatmap. It is very helpfull to find the correlation between features. It is also good to see the correlation between features and labels.\n\ndf.corr(), plt.matshow(..)\n\nWe can also create other matrix other than correlation matrix, How many times, one feature is greater than another feature.\nif the matrix is a total mess like following\n\n\n\n\nmessy_matrix\n\n\nwe can create some kind of clsutering and then plot them, like k means clustering or rows and columnd and reorder those features. The following plot is the result of k means clustering.\n\n\n\nordered\n\n\n\nFeature groups\n\nNew features based on groups\nOne important feature plot could\n\ndf.mean().plot(style='.')\nx -&gt; feature y -&gt; feature mean\n\nIf this is random, then may be we will see random. But if we sort them.\n\ndf.mean().sort_values().plot(style='.')\n\n\n\nordered_feature_mean\n\n\n\nNow we can have close look to each group and use imagination to create new features.\n\nNext post can be found here"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is more about my notes. I am inthusiast about machine learning"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#get-data",
    "href": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#get-data",
    "title": "Matrix Multiplication",
    "section": "Get data",
    "text": "Get data\nWe firt download the mnist data to work with it\n\nfrom pathlib import Path\nimport gzip, time, os, pickle, math\nfrom urllib.request import urlretrieve\n\n\nHOME = os.getenv('HOME')\ndata_path = Path(fr'{HOME}/Schreibtisch/projects/git_data/course22p2/nbs/data')\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = data_path\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\n\nwith gzip.open(path_gz, 'rb') as f_in:\n    ((x_train, y_train), (x_valid, y_valid),_ ) =pickle.load(f_in, encoding='latin1')\n\n\nx_train.shape\n\n(50000, 784)"
  }
]