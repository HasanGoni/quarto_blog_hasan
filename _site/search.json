[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quarto_blog_hasan",
    "section": "",
    "text": "Hasan\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps Series\n\n\n\n\n\n\n\nseries\n\n\ndata-science\n\n\n\n\nA practical guide to data science following FastAI’s top-down learning approach\n\n\n\n\n\n\nMay 1, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Multiplication\n\n\n\n\n\n\n\nfastai\n\n\nmatrix multiplication\n\n\n\n\nMatrix Multiplication from fastai course 2022\n\n\n\n\n\n\nAug 13, 2024\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Matrix Multiplication from FastAI\n\n\nA Deep Dive into Neural Network Fundamentals\n\n\n\n\ndeep-learning\n\n\nmathematics\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nHow to Use Nougat to Read Scientific Paper\n\n\nResearch paper working\n\n\n\n\nNlp\n\n\nResearch paper\n\n\nText\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Data Science & Machine Learning Blog\n\n\n\n\n\n\n\nnews\n\n\ndata-science\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow -05\n\n\nFeature Extraction from Image\n\n\n\n\ncode\n\n\nimage\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow - Part 6\n\n\nValidation Strategy and Model Evaluation\n\n\n\n\ndata-science\n\n\nmachine-learning\n\n\nvalidation\n\n\n\n\nLearn about different validation strategies and how to properly evaluate your models\n\n\n\n\n\n\nNov 22, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow -04\n\n\nFeature Extraction from Text\n\n\n\n\ncode\n\n\ntext\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow -03\n\n\nExploring Anonymized data\n\n\n\n\ncode\n\n\ntabular\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow - Part 2\n\n\nFeature Preprocessing and Generation\n\n\n\n\ndata-science\n\n\nfeature-engineering\n\n\ntutorial\n\n\n\n\nLearn essential techniques for preparing and engineering features in your data science projects.\n\n\n\n\n\n\nNov 20, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow - Part 1\n\n\nExploratory Data Analysis Fundamentals\n\n\n\n\ncode\n\n\nanalysis\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2022\n\n\nHasan Goni\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is more about my notes. I am inthusiast about machine learning"
  },
  {
    "objectID": "posts/series/data-science-steps.html",
    "href": "posts/series/data-science-steps.html",
    "title": "Data Science Steps Series",
    "section": "",
    "text": "This series follows FastAI’s practical, top-down approach to learning data science. Instead of starting with theory, we’ll begin with practical applications and gradually dive deeper into the underlying concepts.\n\n\n\nStart with the practical: We begin with working code and real applications\nSpiral learning: Revisit concepts multiple times with increasing depth\nLearn by doing: Each post includes hands-on notebooks and exercises\n\n\n\n\n\nGetting Started with Data Science\n\nSetting up your environment\nYour first end-to-end project\n\nData Exploration and Visualization\n\nPractical EDA techniques\nCreating impactful visualizations\n\nFeature Engineering and Selection\n\nTransforming data for better models\nSelecting the most important features\n\nModel Selection and Training\n\nChoosing the right model\nTraining best practices\n\nModel Evaluation and Interpretation\n\nUnderstanding model performance\nInterpreting model decisions\n\nDeployment and Monitoring\n\nTaking models to production\nMonitoring and maintaining models"
  },
  {
    "objectID": "posts/series/data-science-steps.html#series-overview",
    "href": "posts/series/data-science-steps.html#series-overview",
    "title": "Data Science Steps Series",
    "section": "",
    "text": "This series follows FastAI’s practical, top-down approach to learning data science. Instead of starting with theory, we’ll begin with practical applications and gradually dive deeper into the underlying concepts.\n\n\n\nStart with the practical: We begin with working code and real applications\nSpiral learning: Revisit concepts multiple times with increasing depth\nLearn by doing: Each post includes hands-on notebooks and exercises\n\n\n\n\n\nGetting Started with Data Science\n\nSetting up your environment\nYour first end-to-end project\n\nData Exploration and Visualization\n\nPractical EDA techniques\nCreating impactful visualizations\n\nFeature Engineering and Selection\n\nTransforming data for better models\nSelecting the most important features\n\nModel Selection and Training\n\nChoosing the right model\nTraining best practices\n\nModel Evaluation and Interpretation\n\nUnderstanding model performance\nInterpreting model decisions\n\nDeployment and Monitoring\n\nTaking models to production\nMonitoring and maintaining models"
  },
  {
    "objectID": "posts/series/data-science-steps.html#prerequisites",
    "href": "posts/series/data-science-steps.html#prerequisites",
    "title": "Data Science Steps Series",
    "section": "0.2 Prerequisites",
    "text": "0.2 Prerequisites\n\nBasic Python knowledge\nFamiliarity with Jupyter notebooks\nUnderstanding of basic statistics (we’ll review as needed)"
  },
  {
    "objectID": "posts/series/data-science-steps.html#tools-well-use",
    "href": "posts/series/data-science-steps.html#tools-well-use",
    "title": "Data Science Steps Series",
    "section": "0.3 Tools We’ll Use",
    "text": "0.3 Tools We’ll Use\n\nPyTorch for deep learning\nPandas for data manipulation\nScikit-learn for traditional ML\nFastAI for rapid prototyping"
  },
  {
    "objectID": "posts/series/data-science-steps.html#getting-help",
    "href": "posts/series/data-science-steps.html#getting-help",
    "title": "Data Science Steps Series",
    "section": "0.4 Getting Help",
    "text": "0.4 Getting Help\n\nUse the comments section below each post\nCheck the GitHub repository for code\nJoin our discussion forum (coming soon)\n\n\n\n\n\n\n\nNote\n\n\n\nThis series is continuously updated with new content and improvements based on reader feedback."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Data Science & Machine Learning Blog",
    "section": "",
    "text": "Welcome to my technical blog where I share insights about Data Science, Machine Learning, and Deep Learning!"
  },
  {
    "objectID": "posts/welcome/index.html#what-to-expect",
    "href": "posts/welcome/index.html#what-to-expect",
    "title": "Welcome To My Data Science & Machine Learning Blog",
    "section": "0.1 What to Expect",
    "text": "0.1 What to Expect\nIn this blog, you’ll find:\n\n0.1.1 1. Data Science Series\nA comprehensive guide covering everything from exploratory data analysis to advanced modeling:\n\nPart 1: Exploratory Data Analysis Fundamentals\nPart 2: Feature Preprocessing and Generation\nPart 3: Handling Anonymized Data\nPart 4: Text Feature Extraction\nPart 5: Image Feature Extraction\nPart 6: Advanced Techniques\n\n\n\n0.1.2 2. Deep Learning Concepts\nExplanations inspired by Jeremy Howard’s teaching style:\n\nUnderstanding Matrix Multiplication\nMore coming soon!\n\n\n\n0.1.3 3. Technical Tools\nExploring modern data science tools:\n\nUsing Nougat to Read Scientific Papers\n\n\n\n0.1.4 4. Practical Tutorials\nHands-on guides with real-world applications (Coming Soon)"
  },
  {
    "objectID": "posts/welcome/index.html#getting-started",
    "href": "posts/welcome/index.html#getting-started",
    "title": "Welcome To My Data Science & Machine Learning Blog",
    "section": "0.2 Getting Started",
    "text": "0.2 Getting Started\nIf you’re new to data science, I recommend starting with the Data Science Series Part 1, where we explore the fundamentals of data analysis.\nFor those interested in deep learning, check out the matrix multiplication tutorial to understand the building blocks of neural networks.\nStay tuned for regular updates and deep technical content! Feel free to connect with me on GitHub or Twitter."
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/00_matmul.html",
    "href": "posts/matrix_multiplication_from_fastai_course/00_matmul.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "This notebook is actually reprodcution fo course practical deep learning for coders part2. Matrix Multiplication starts with Lession 11 (1:08:47).\nTo get the noebook shown in the video, one should clone the course repository and run the notebook 01_matmul.ipynb."
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#first-description",
    "href": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#first-description",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "This notebook is actually reprodcution fo course practical deep learning for coders part2. Matrix Multiplication starts with Lession 11 (1:08:47).\nTo get the noebook shown in the video, one should clone the course repository and run the notebook 01_matmul.ipynb."
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#get-data",
    "href": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#get-data",
    "title": "Matrix Multiplication",
    "section": "0.2 Get data",
    "text": "0.2 Get data\nWe firt download the mnist data to work with it\n\n\nCode\nfrom pathlib import Path\nimport gzip, time, os, pickle, math\nfrom urllib.request import urlretrieve\n\n\n\n\nCode\nHOME = os.getenv('HOME')\ndata_path = Path(fr'{HOME}/Schreibtisch/projects/git_data/course22p2/nbs/data')\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = data_path\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\n\n\n\nCode\nwith gzip.open(path_gz, 'rb') as f_in:\n    ((x_train, y_train), (x_valid, y_valid),_ ) =pickle.load(f_in, encoding='latin1')\n\n\n\n\nCode\nx_train.shape\n\n\n(50000, 784)"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html",
    "href": "posts/data-science-steps-to-follow-part02/index.html",
    "title": "Data Science Steps to Follow - Part 2",
    "section": "",
    "text": "Data Science Steps Series\n\n\n\nThis is Part 2 of a 6-part series on data science fundamentals:\n\nPart 1: EDA Fundamentals\nPart 2: Feature Preprocessing and Generation (You are here)\nPart 3: Handling Anonymized Data\nPart 4: Text Feature Extraction\nPart 5: Image Feature Extraction\nPart 6: Advanced Techniques"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#interactive-data-explorer",
    "href": "posts/data-science-steps-to-follow-part02/index.html#interactive-data-explorer",
    "title": "Data Science Steps to Follow - Part 2",
    "section": "2.1 Interactive Data Explorer",
    "text": "2.1 Interactive Data Explorer\n\n\nShow/hide code for interactive data explorer\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Generate sample data\nnp.random.seed(42)\nn_samples = 1000\n\ndata = {\n    'age': np.random.normal(35, 10, n_samples),\n    'income': np.random.lognormal(10, 1, n_samples),\n    'education_years': np.random.randint(8, 22, n_samples),\n    'satisfaction': np.random.randint(1, 6, n_samples)\n}\n\ndf = pd.DataFrame(data)\n\n# Create interactive scatter plot\nfig = px.scatter(df, x='age', y='income', \n                 color='satisfaction',\n                 size='education_years',\n                 hover_data=['education_years'],\n                 title='Interactive Feature Relationships')\nfig.show()"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#feature-preprocessing-steps",
    "href": "posts/data-science-steps-to-follow-part02/index.html#feature-preprocessing-steps",
    "title": "Data Science Steps to Follow - Part 2",
    "section": "2.2 Feature Preprocessing Steps",
    "text": "2.2 Feature Preprocessing Steps\n\n2.2.1 1. Handling Missing Values\n\nCodeExample\n\n\ndef handle_missing_values(df):\n    # Numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n    \n    # Categorical columns\n    cat_cols = df.select_dtypes(exclude=[np.number]).columns\n    df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n    \n    return df\n\n\n# Example usage\ndf_clean = handle_missing_values(df.copy())\nprint(\"Missing values after cleaning:\")\nprint(df_clean.isnull().sum())\n\n\n\n\n\n2.2.2 2. Scaling Features\nLet’s compare different scaling methods:\n\n\nShow/hide scaling comparison code\ndef compare_scaling_methods(data):\n    # Original data\n    original = data['age'].copy()\n    \n    # Standard scaling\n    scaler = StandardScaler()\n    standard_scaled = scaler.fit_transform(original.values.reshape(-1, 1))\n    \n    # Min-max scaling\n    min_max_scaler = MinMaxScaler()\n    minmax_scaled = min_max_scaler.fit_transform(original.values.reshape(-1, 1))\n    \n    # Plotting\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Original distribution\n    sns.histplot(original, ax=axes[0])\n    axes[0].set_title('Original Data')\n    \n    # Standard scaled\n    sns.histplot(standard_scaled, ax=axes[1])\n    axes[1].set_title('Standard Scaled')\n    \n    # Min-max scaled\n    sns.histplot(minmax_scaled, ax=axes[2])\n    axes[2].set_title('Min-Max Scaled')\n    \n    plt.tight_layout()\n    plt.show()\n\ncompare_scaling_methods(df)\n\n\n\n\n\n\n\n2.2.3 3. Feature Generation\n\n\n\n\n\n\nInteractive Feature Generator\n\n\n\nUse the code below to experiment with different feature combinations:\n\n\n\n\nShow/hide feature generation code\ndef generate_features(df):\n    \"\"\"Generate new features from existing ones\"\"\"\n    # Polynomial features\n    df['income_squared'] = df['income'] ** 2\n    \n    # Interaction features\n    df['income_per_education'] = df['income'] / df['education_years']\n    \n    # Binning\n    df['age_group'] = pd.qcut(df['age'], q=5, labels=['Very Young', 'Young', 'Middle', 'Senior', 'Elder'])\n    \n    return df\n\n# Generate new features\ndf_featured = generate_features(df.copy())\n\n# Show correlations\nplt.figure(figsize=(10, 8))\nsns.heatmap(df_featured.select_dtypes(include=[np.number]).corr(), \n            annot=True, cmap='coolwarm', center=0)\nplt.title('Feature Correlations')\nplt.show()"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#best-practices",
    "href": "posts/data-science-steps-to-follow-part02/index.html#best-practices",
    "title": "Data Science Steps to Follow - Part 2",
    "section": "2.3 Best Practices",
    "text": "2.3 Best Practices\n\n\n\n\n\n\nKey Points to Remember\n\n\n\n\nAlways scale features after splitting into train/test sets\nHandle missing values before feature generation\nDocument all preprocessing steps for reproducibility\nValidate generated features with domain experts"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#interactive-feature-selection-tool",
    "href": "posts/data-science-steps-to-follow-part02/index.html#interactive-feature-selection-tool",
    "title": "Data Science Steps to Follow - Part 2",
    "section": "2.4 Interactive Feature Selection Tool",
    "text": "2.4 Interactive Feature Selection Tool\n\n\nShow/hide feature selection tool\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\ndef plot_feature_importance(X, y, k=5):\n    \"\"\"Plot top k most important features\"\"\"\n    selector = SelectKBest(score_func=f_regression, k=k)\n    selector.fit(X, y)\n    \n    # Get feature scores\n    scores = pd.DataFrame({\n        'Feature': X.columns,\n        'Score': selector.scores_\n    }).sort_values('Score', ascending=False)\n    \n    # Create interactive bar plot\n    fig = px.bar(scores, x='Feature', y='Score',\n                 title=f'Top {k} Most Important Features',\n                 labels={'Score': 'Importance Score'})\n    fig.show()\n\n# Example usage\nX = df_featured.select_dtypes(include=[np.number]).drop('satisfaction', axis=1)\ny = df_featured['satisfaction']\nplot_feature_importance(X, y)"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#next-steps",
    "href": "posts/data-science-steps-to-follow-part02/index.html#next-steps",
    "title": "Data Science Steps to Follow - Part 2",
    "section": "2.5 Next Steps",
    "text": "2.5 Next Steps\nContinue to Part 3: Handling Anonymized Data to learn about working with masked and anonymized datasets."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#related-resources",
    "href": "posts/data-science-steps-to-follow-part02/index.html#related-resources",
    "title": "Data Science Steps to Follow - Part 2",
    "section": "2.6 Related Resources",
    "text": "2.6 Related Resources\n\nUnderstanding Matrix Multiplication - Important for feature transformations\nData Science Series Overview"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#references",
    "href": "posts/data-science-steps-to-follow-part02/index.html#references",
    "title": "Data Science Steps to Follow - Part 2",
    "section": "2.7 References",
    "text": "2.7 References\n\nScikit-learn Preprocessing Guide\nFeature Engineering for Machine Learning\nPython Data Science Handbook"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{malloc2023,\n  author = {Malloc, Harlow},\n  title = {Post {With} {Code}},\n  date = {2023-10-20},\n  url = {https://hasangoni.quarto.pub/hasan-blog-post/posts/post-with-code},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMalloc, Harlow. 2023. “Post With Code.” October 20, 2023.\nhttps://hasangoni.quarto.pub/hasan-blog-post/posts/post-with-code."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html",
    "href": "posts/data-science-steps-to-follow-part04/index.html",
    "title": "Data Science Steps to Follow -04",
    "section": "",
    "text": "This is a part of series. First, Second, Third parts are connected in link. In this part I will try to write down about text data # Feature extraction from text\n\n\nBag of words\nEmbeddings(~word2vec)\n\n\n\n1.1. CountVectorizer\n\neach word is separated and count number of occurences\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n\nWe may be need to do some post processing. As we know KNN, neural networks are sensitive to the scale of the features. So we need to scale the features. We can use TF-IDF to do this.\n\n1.2. TfidfVectorizer\n\nWhat actually is just not frequency but normalized frequency.\nTerm frequency:\n\ntf = 1/ x.sum[axis=1](:,None)\nx = x * tf\n\nInverse document frequency:\n\nidf = np.log(x.shape[0]/(x&gt;0).sum(axis=0)))\nx = x*idf\nsklearn.feature_extraction.text.TfidfVectorizer\n1.3 N-grams\n\nNot only words but n-consequent words\n\nsklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2)) \n# may be parameter analyzer\n\n\n\nActually before applying any Bag of words we need to preprocess the text. We need to remove the stop words, stemming, lemmatization, etc.* Conventionally preprocessing are\n\nTokenization -&gt; Very very sunny day -&gt; [Very, very, sunny, day]\nLowercasing -&gt; [very, very, sunny, day] -&gt; [very, very, sunny, day] -&gt;CountVectorizer from sklearn will automatically do this\nRemoving punctuation\nRemoving stopwords -&gt; [The cow jumped over the moon] -&gt; [cow, jumped, moon]\n\nAriticles or preprositon words\nVery common words\nCan be used NLTK library\nsklearn.feature_extraction.text.CountVectorizer(max_df)\nmax_df is the frequency threshold, after which the word is removed\n\nStemming/Lemmatization\nStemming\n\n[democracy, democratic, democratization] -&gt; [democr]\n[Saw] -&gt; [s]\n\nLemitization\n\n[democracy, democratic, democratization] -&gt; [democracy]\n[Saw, sawing, sawed] -&gt; [see or saw] depending on text\n\n\n\n\n\n\n\n\nPreprocessing Lowercasing, removing punctuation, removing stopwords, stemming/lemmatization\nN-grams helps to get local context\nPost processing TF-IDF\n\n\n\n\n\n\n\nVector representation of words and text\nEach word is represented as a vector, in some sophisticated way, which could have 100 dimensions or more.\nSame words will have similar vectors. king-&gt;queen\nAlso addition and subtraction of vectors will have some meaning. -&gt; king + woman - man = queen\nSeveral implementaton of word2vec\n\nWord2vec\nGlove\nFastText\n\nSentences\n\nDoc2vec\n\nBased on situation we can use word or sentence embeddings. Actually try both and take the best one.\nAll the preprocessing steps can be applied to the text before applying word2vec.\n\n\n\n\n\nBag of words\n\nVery large vector\nmeaning is easy value in vector is known\n\nWord2vec\n\nRelative Small vector\nValues of vector can be interpreted only some cases\nThe words with simlar meaning will have similar embeddings\n\n\nNext post can be found here"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html#bag-of-words",
    "href": "posts/data-science-steps-to-follow-part04/index.html#bag-of-words",
    "title": "Data Science Steps to Follow -04",
    "section": "",
    "text": "1.1. CountVectorizer\n\neach word is separated and count number of occurences\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n\nWe may be need to do some post processing. As we know KNN, neural networks are sensitive to the scale of the features. So we need to scale the features. We can use TF-IDF to do this.\n\n1.2. TfidfVectorizer\n\nWhat actually is just not frequency but normalized frequency.\nTerm frequency:\n\ntf = 1/ x.sum[axis=1](:,None)\nx = x * tf\n\nInverse document frequency:\n\nidf = np.log(x.shape[0]/(x&gt;0).sum(axis=0)))\nx = x*idf\nsklearn.feature_extraction.text.TfidfVectorizer\n1.3 N-grams\n\nNot only words but n-consequent words\n\nsklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2)) \n# may be parameter analyzer\n\n\n\nActually before applying any Bag of words we need to preprocess the text. We need to remove the stop words, stemming, lemmatization, etc.* Conventionally preprocessing are\n\nTokenization -&gt; Very very sunny day -&gt; [Very, very, sunny, day]\nLowercasing -&gt; [very, very, sunny, day] -&gt; [very, very, sunny, day] -&gt;CountVectorizer from sklearn will automatically do this\nRemoving punctuation\nRemoving stopwords -&gt; [The cow jumped over the moon] -&gt; [cow, jumped, moon]\n\nAriticles or preprositon words\nVery common words\nCan be used NLTK library\nsklearn.feature_extraction.text.CountVectorizer(max_df)\nmax_df is the frequency threshold, after which the word is removed\n\nStemming/Lemmatization\nStemming\n\n[democracy, democratic, democratization] -&gt; [democr]\n[Saw] -&gt; [s]\n\nLemitization\n\n[democracy, democratic, democratization] -&gt; [democracy]\n[Saw, sawing, sawed] -&gt; [see or saw] depending on text"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html#summray-of-bag-of-words-pipeline",
    "href": "posts/data-science-steps-to-follow-part04/index.html#summray-of-bag-of-words-pipeline",
    "title": "Data Science Steps to Follow -04",
    "section": "",
    "text": "Preprocessing Lowercasing, removing punctuation, removing stopwords, stemming/lemmatization\nN-grams helps to get local context\nPost processing TF-IDF"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html#embeddings",
    "href": "posts/data-science-steps-to-follow-part04/index.html#embeddings",
    "title": "Data Science Steps to Follow -04",
    "section": "",
    "text": "Vector representation of words and text\nEach word is represented as a vector, in some sophisticated way, which could have 100 dimensions or more.\nSame words will have similar vectors. king-&gt;queen\nAlso addition and subtraction of vectors will have some meaning. -&gt; king + woman - man = queen\nSeveral implementaton of word2vec\n\nWord2vec\nGlove\nFastText\n\nSentences\n\nDoc2vec\n\nBased on situation we can use word or sentence embeddings. Actually try both and take the best one.\nAll the preprocessing steps can be applied to the text before applying word2vec.\n\n\n\n\n\nBag of words\n\nVery large vector\nmeaning is easy value in vector is known\n\nWord2vec\n\nRelative Small vector\nValues of vector can be interpreted only some cases\nThe words with simlar meaning will have similar embeddings\n\n\nNext post can be found here"
  },
  {
    "objectID": "tags.html",
    "href": "tags.html",
    "title": "Tags",
    "section": "",
    "text": "Welcome to the tag index! Here you can find all posts organized by their tags. Use the search and filter options to find specific content.\n\n\n\n\n\n\n\n\nBelow is a searchable table of all posts with their associated tags:"
  },
  {
    "objectID": "tags.html#all-posts",
    "href": "tags.html#all-posts",
    "title": "Tags",
    "section": "",
    "text": "Below is a searchable table of all posts with their associated tags:"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html",
    "href": "posts/data-science-steps-to-follow-part01/index.html",
    "title": "Data Science Steps to Follow - Part 1",
    "section": "",
    "text": "Data Science Steps Series\n\n\n\nThis is Part 1 of a 6-part series on data science fundamentals:\n\nPart 1: EDA Fundamentals (You are here)\nPart 2: Feature Preprocessing and Generation\nPart 3: Handling Anonymized Data\nPart 4: Text Feature Extraction\nPart 5: Image Feature Extraction\nPart 6: Advanced Techniques"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#what-is-eda",
    "href": "posts/data-science-steps-to-follow-part01/index.html#what-is-eda",
    "title": "Data Science Steps to Follow - Part 1",
    "section": "2.1 What is EDA?",
    "text": "2.1 What is EDA?\nEDA is the critical first step in any data science project. It helps us:\n\nUnderstand the data deeply\nBuild intuition about patterns and relationships\nGenerate hypotheses for feature engineering\nFind insights that inform modeling decisions"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#the-power-of-visualization",
    "href": "posts/data-science-steps-to-follow-part01/index.html#the-power-of-visualization",
    "title": "Data Science Steps to Follow - Part 1",
    "section": "2.2 The Power of Visualization",
    "text": "2.2 The Power of Visualization\nOne of the most powerful EDA tools is visualization. Let’s look at a fascinating example from a Kaggle competition:\n\n\n\n\n\n\nReal-World Example\n\n\n\nIn a promotion prediction competition, simple visualization revealed: - Two key features: promos sent and promos used - A direct relationship between their difference and the target - This insight led to 81% accuracy without complex modeling!\n\n\n# Example visualization code\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_promo_relationship(df):\n    plt.figure(figsize=(10, 6))\n    plt.scatter('promos_sent', 'promos_used', data=df)\n    plt.xlabel('Number of Promos Sent')\n    plt.ylabel('Number of Promos Used')\n    plt.title('Promo Usage Pattern')\n    plt.show()"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#building-intuition",
    "href": "posts/data-science-steps-to-follow-part01/index.html#building-intuition",
    "title": "Data Science Steps to Follow - Part 1",
    "section": "2.3 Building Intuition",
    "text": "2.3 Building Intuition\n\n2.3.1 1. Domain Knowledge Acquisition\nBefore diving into analysis, focus on:\n\nUnderstanding the business goal\nResearching similar problems\nLearning industry-specific metrics\nReading relevant documentation\n\n\n\n2.3.2 2. Data Validation\n\n\n\n\n\n\nKey Validation Checks\n\n\n\n\nValue ranges (e.g., age between 0-120)\nLogical relationships (clicks ≤ impressions)\nMissing value patterns\nOutliers and anomalies\n\n\n\ndef validate_data(df):\n    \"\"\"Basic data validation checks\"\"\"\n    issues = []\n    \n    # Age check\n    if df['age'].max() &gt; 120:\n        issues.append(\"Found age &gt; 120\")\n        \n    # Click validation\n    if any(df['clicks'] &gt; df['impressions']):\n        issues.append(\"Found clicks &gt; impressions\")\n    \n    return issues\n\n\n2.3.3 3. Understanding Data Generation\nKey considerations:\n\nSampling methodology\nTrain/test split rationale\nTime-based patterns\nData collection process\n\n\n\n\n\n\n\nJeremy Howard’s Insight\n\n\n\n“In data science, there are no outliers - only opportunities to understand your data better.”"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#related-resources",
    "href": "posts/data-science-steps-to-follow-part01/index.html#related-resources",
    "title": "Data Science Steps to Follow - Part 1",
    "section": "2.4 Related Resources",
    "text": "2.4 Related Resources\n\nUnderstanding Matrix Multiplication - Essential math for data science\nUsing Nougat for Research Papers - Tool for data science research"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#next-steps",
    "href": "posts/data-science-steps-to-follow-part01/index.html#next-steps",
    "title": "Data Science Steps to Follow - Part 1",
    "section": "2.5 Next Steps",
    "text": "2.5 Next Steps\nContinue to Part 2: Feature Preprocessing and Generation, where we’ll explore how to transform raw data into meaningful features."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#references",
    "href": "posts/data-science-steps-to-follow-part01/index.html#references",
    "title": "Data Science Steps to Follow - Part 1",
    "section": "2.6 References",
    "text": "2.6 References\n\nCoursera: How to Win a Data Science Competition\nFast.ai: Practical Deep Learning\nPython Data Science Handbook"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part05/index.html",
    "href": "posts/data-science-steps-to-follow-part05/index.html",
    "title": "Data Science Steps to Follow -05",
    "section": "",
    "text": "1 Introduction\n\nThis is a part of the series. First, Second, Third and Fourth parts are connected in link. In this part I will try to write about EDA, when the data is image type\n\n\n\n2 Images Vector extraction\n\nSimilar to text vector extraction, we can extract features from images\nBesides getting outputs from last layer, we also can get outputs from intermediate layers, we call them descriptors.\nDescriptors from later layers are better to solve task similar to one network was trained on.\nIn contraray descriptors from earlier layers has some task independent features.\nImage net descriptors from last layer can be used for like car classification, but for medical image task earlier layers descriptos may be a better choice or train from scratch.\nFine tuning is a good option to get better results.\n\ne.g. Data Science game 2016. The task was to classify roofs into groups. Log Loss was the metric. Competitors has 8000 images. Fine tuning with image net helps to get better results.\n\nAugmentations:\n\nCreate different versions of same image, may be rotation, random crop, etc.\n\nBe careful with validation and don’t overfit.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{goni2022,\n  author = {Goni, Hasan},\n  title = {Data {Science} {Steps} to {Follow} -05},\n  date = {2022-11-22},\n  url = {https://hasangoni.quarto.pub/hasan-blog-post/posts/data-science-steps-to-follow-part05},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGoni, Hasan. 2022. “Data Science Steps to Follow -05.”\nNovember 22, 2022. https://hasangoni.quarto.pub/hasan-blog-post/posts/data-science-steps-to-follow-part05."
  },
  {
    "objectID": "posts/nougat-to-read-scientific-pdf-files/index.html",
    "href": "posts/nougat-to-read-scientific-pdf-files/index.html",
    "title": "How to Use Nougat to Read Scientific Paper",
    "section": "",
    "text": "Copied from here"
  },
  {
    "objectID": "posts/nougat-to-read-scientific-pdf-files/index.html#jupter-notebook-is-the-following",
    "href": "posts/nougat-to-read-scientific-pdf-files/index.html#jupter-notebook-is-the-following",
    "title": "How to Use Nougat to Read Scientific Paper",
    "section": "2.1 Jupter notebook is the following",
    "text": "2.1 Jupter notebook is the following\n#pip install -q pymupdf python-Levenshtein nltk\nfrom transformers import AutoProcessor, VisionEncoderDecoderModel\nimport torch\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Tuple \nimport io\nimport fitz\nfrom huggingface_hub import hf_hub_download\nfrom PIL import Image\nfrom collections import defaultdict\nfrom transformers import StoppingCriteria, StoppingCriteriaList\nprocessor = AutoProcessor.from_pretrained(\"facebook/nougat-small\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-small\")\nDownloading (…)rocessor_config.json:   0%|          | 0.00/479 [00:00&lt;?, ?B/s]\n\n\n\nDownloading (…)okenizer_config.json:   0%|          | 0.00/4.49k [00:00&lt;?, ?B/s]\n\n\n\nDownloading (…)/main/tokenizer.json:   0%|          | 0.00/2.14M [00:00&lt;?, ?B/s]\n\n\n\nDownloading (…)cial_tokens_map.json:   0%|          | 0.00/96.0 [00:00&lt;?, ?B/s]\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\nDownloading (…)lve/main/config.json:   0%|          | 0.00/4.77k [00:00&lt;?, ?B/s]\n\n\n\nDownloading pytorch_model.bin:   0%|          | 0.00/990M [00:00&lt;?, ?B/s]\n\n\n\nDownloading (…)neration_config.json:   0%|          | 0.00/165 [00:00&lt;?, ?B/s]\n%%capture\n#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = \"cpu\"\nmodel.to(device)\n     \nfilepath = hf_hub_download(repo_id=\"ysharma/nougat\", filename=\"input/nougat.pdf\", repo_type=\"space\")\nDownloading nougat.pdf:   0%|          | 0.00/4.13M [00:00&lt;?, ?B/s]\ndef rasterize_paper(\n    pdf: Path,\n    outpath: Optional[Path] = None,\n    dpi: int = 96,\n    return_pil=False,\n    pages=None,\n) -&gt; Optional[List[io.BytesIO]]:\n    \"\"\"\n    Rasterize a PDF file to PNG images.\n\n    Args:\n        pdf (Path): The path to the PDF file.\n        outpath (Optional[Path], optional): The output directory. If None, the PIL images will be returned instead. Defaults to None.\n        dpi (int, optional): The output DPI. Defaults to 96.\n        return_pil (bool, optional): Whether to return the PIL images instead of writing them to disk. Defaults to False.\n        pages (Optional[List[int]], optional): The pages to rasterize. If None, all pages will be rasterized. Defaults to None.\n\n    Returns:\n        Optional[List[io.BytesIO]]: The PIL images if `return_pil` is True, otherwise None.\n    \"\"\"\n\n    pillow_images = []\n    if outpath is None:\n        return_pil = True\n    try:\n        if isinstance(pdf, (str, Path)):\n            pdf = fitz.open(pdf)\n        if pages is None:\n            pages = range(len(pdf))\n        for i in pages:\n            page_bytes: bytes = pdf[i].get_pixmap(dpi=dpi).pil_tobytes(format=\"PNG\")\n            if return_pil:\n                pillow_images.append(io.BytesIO(page_bytes))\n            else:\n                with (outpath / (\"%02d.png\" % (i + 1))).open(\"wb\") as f:\n                    f.write(page_bytes)\n    except Exception:\n        pass\n    if return_pil:\n        return pillow_images\nimages = rasterize_paper(pdf=filepath, return_pil=True)\nlen(images)\n17\nimage = Image.open(images[0])\nimage\n\n\n\npng\n\n\n# prepare image for the model\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\nprint(pixel_values.shape)\n     \ntorch.Size([1, 3, 896, 672])\nclass RunningVarTorch:\n    def __init__(self, L=15, norm=False):\n        self.values = None\n        self.L = L\n        self.norm = norm\n\n    def push(self, x: torch.Tensor):\n        assert x.dim() == 1\n        if self.values is None:\n            self.values = x[:, None]\n        elif self.values.shape[1] &lt; self.L:\n            self.values = torch.cat((self.values, x[:, None]), 1)\n        else:\n            self.values = torch.cat((self.values[:, 1:], x[:, None]), 1)\n\n    def variance(self):\n        if self.values is None:\n            return\n        if self.norm:\n            return torch.var(self.values, 1) / self.values.shape[1]\n        else:\n            return torch.var(self.values, 1)\nclass StoppingCriteriaScores(StoppingCriteria):\n    def __init__(self, threshold: float = 0.015, window_size: int = 200):\n        super().__init__()\n        self.threshold = threshold\n        self.vars = RunningVarTorch(norm=True)\n        self.varvars = RunningVarTorch(L=window_size)\n        self.stop_inds = defaultdict(int)\n        self.stopped = defaultdict(bool)\n        self.size = 0\n        self.window_size = window_size\n\n    @torch.no_grad()\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n        last_scores = scores[-1]\n        self.vars.push(last_scores.max(1)[0].float().cpu())\n        self.varvars.push(self.vars.variance())\n        self.size += 1\n        if self.size &lt; self.window_size:\n            return False\n\n        varvar = self.varvars.variance()\n        for b in range(len(last_scores)):\n            if varvar[b] &lt; self.threshold:\n                if self.stop_inds[b] &gt; 0 and not self.stopped[b]:\n                    self.stopped[b] = self.stop_inds[b] &gt;= self.size\n                else:\n                    self.stop_inds[b] = int(\n                        min(max(self.size, 1) * 1.15 + 150 + self.window_size, 4095)\n                    )\n            else:\n                self.stop_inds[b] = 0\n                self.stopped[b] = False\n        return all(self.stopped.values()) and len(self.stopped) &gt; 0\n\n# autoregressively generate tokens, with custom stopping criteria (as defined by the Nougat authors)\noutputs = model.generate(pixel_values.to(device),\n                          min_length=1,\n                          max_length=3584,\n                          bad_words_ids=[[processor.tokenizer.unk_token_id]],\n                          return_dict_in_generate=True,\n                          output_scores=True,\n                          stopping_criteria=StoppingCriteriaList([StoppingCriteriaScores()]),\n)\ngenerated = processor.batch_decode(outputs[0], skip_special_tokens=True)[0]\ngenerated = processor.post_process_generation(generated, fix_markdown=False)\nprint(generated)\n# Nougat: Neural Optical Understanding for Academic Documents\n\n Lukas Blecher\n\nCorrespondence to: lblecher@meta.com\n\nGuillem Cucurull\n\nThomas Scialom\n\nRobert Stojnic\n\nMeta AI\n\nThe paper reports 8.1M papers but the authors recently updated the numbers on the GitHub page https://github.com/allenai/s2orc\n\n###### Abstract\n\nScientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (**N**eural **O**ptical **U**nderstanding for **A**cademic Documents), a Visual Transformer model that performs an _Optical Character Recognition_ (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.\n\n## 1 Introduction\n\nThe majority of scientific knowledge is stored in books or published in scientific journals, most commonly in the Portable Document Format (PDF). Next to HTML, PDFs are the second most prominent data format on the internet, making up 2.4% of common crawl [1]. However, the information stored in these files is very difficult to extract into any other formats. This is especially true for highly specialized documents, such as scientific research papers, where the semantic information of mathematical expressions is lost.\n\nExisting Optical Character Recognition (OCR) engines, such as Tesseract OCR [2], excel at detecting and classifying individual characters and words in an image, but fail to understand the relationship between them due to their line-by-line approach. This means that they treat superscripts and subscripts in the same way as the surrounding text, which is a significant drawback for mathematical expressions. In mathematical notations like fractions, exponents, and matrices, relative positions of characters are crucial.\n\nConverting academic research papers into machine-readable text also enables accessibility and searchability of science as a whole. The information of millions of academic papers can not be fully accessed because they are locked behind an unreadable format. Existing corpora, such as the S2ORC dataset [3], capture the text of 12M2 papers using GROBID [4], but are missing meaningful representations of the mathematical equations.\n\nFootnote 2: The paper reports 8.1M papers but the authors recently updated the numbers on the GitHub page https://github.com/allenai/s2orc\n\nTo this end, we introduce Nougat, a transformer based model that can convert images of document pages to formatted markup text.\n\nThe primary contributions in this paper are\n\n* Release of a pre-trained model capable of converting a PDF to a lightweight markup language. We release the code and the model on GitHub3 Footnote 3: https://github.com/facebookresearch/nougat\n* We introduce a pipeline to create dataset for pairing PDFs to source code\n* Our method is only dependent on the image of a page, allowing access to scanned papers and books"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html",
    "href": "posts/data-science-steps-to-follow-part03/index.html",
    "title": "Data Science Steps to Follow -03",
    "section": "",
    "text": "This is third part of the series. First and second part needs to be read for better understanding. In this part description can be found, when we have anonymized data and how to explore it."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html#explore-individual-feature",
    "href": "posts/data-science-steps-to-follow-part03/index.html#explore-individual-feature",
    "title": "Data Science Steps to Follow -03",
    "section": "3.1 1. Explore individual feature",
    "text": "3.1 1. Explore individual feature\n\nGuess the meaning of columns.\nGuess the types of columns. Separate them numerical, categorical, ordinal, date, text, etc."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations",
    "href": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations",
    "title": "Data Science Steps to Follow -03",
    "section": "3.2 2. Explore feature relations",
    "text": "3.2 2. Explore feature relations\n\nFind relation between pairs.\nFind feature groups.\n\n\n3.2.1 1. Individual feature\n\nThere was a competition, where like there are different features, which were anonymized means the features names are x1, x2, etc. There were some numerical and some hash value, which could be categorical feature.\nWhat the lecturer did, first create a baseline with random forest. fillna with -999, categorical featueres factorize.\nThen plot feature importance from this baseline model.\nHe found that, one feature named as x8 has highest influence on the target variable.\nSo he starts to invesitage a little bit deeper.\nThen tried to find mean and std values. It seems close to 0 and 1. It seems normalized but it is not exactly 0 and 1 but extra decimal places. May be because of train and test.\nThen search for other reapeatd values by value counts. It seems there are lots of repeted values.\nAs we understood them, they are normalized, we tried to find the normalize parameter, means scaling and shift parameter. Lets’s try to find it, or is it actaully possible.\n\nSearch for unique values and sort them.\nThen use np.diff to find the difference between two consecutive values. It seems the values are same all the time.\nThen devide this values with to our sorted array. It is almost 1. May be not 1 because of some numerical error.\nSo if we devide this vlaue to our feature, we will get the original values. It is also visible each positive number decimal places are same and also each negative number decimal places are same. This could be part of shifting parameter.\nSo we devide to our previous value and substract to decimal place, we get almost integer values.\nAfter that it seems that we are right direction, because we are getting integer values. However we got shifing value, a fractioanal part, but how to get the full part of shifting value.\nSo the lecturer had a hanch. He just tried value counts of integer values. Then he found an extremely different number from other and the value was - 1968 . So he assumed may be it is some kind of year and one person put 0 or forgot to enter, then system converted to 0. So may be the shifting value is 1968. So he tried to substract 1968 from the feature and then he got the original values.\nBut how it helps in the competition. One can use different things from it. But at that competition he could not use this feature. But it was very interseting to see how he found the original values.\n\nif there are small features we can see manually. If there are many features.\ndf.dtypes\ndf.info()\nx.value_counts()\nx.isnull()"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations-1",
    "href": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations-1",
    "title": "Data Science Steps to Follow -03",
    "section": "3.3 2. Explore feature relations",
    "text": "3.3 2. Explore feature relations\n\nTo explore different features, important things can be done is visualization\n\nExplore individual features\n\n\nHistograms\nplots\nstatistics\n\n\nExplore feature relations\n\n\nScatter plots\nCorrelation plots\nPlot(index vs feature statistics)\nAnd more.\n\n\nEDA is art and visualization is the tool.\n\n\n\n3.3.1 1. Individual features\nplt.hist(x)\n\nSometimes it is misleading. So change number of bins.\nNever make a hypothesis from a single plot. Try to plot different things and then make a decision.\nSometimes in histogram you will see some spikes. It could be anything. Actually in particular case the organizer put the missing vlaue with its mean. We can change this value with other than mean.\nwe can also plot x is index and y is feature value. Not conenct with line but with circles only. python plt.plot(x,'.' )\n\n\n\nindex_image\n\n\nIf we see horizontal line in such plots, it means there are repeated values and if there is no vertical lines, it means the data is shuffled nicely.\nWe can also color code based on labels.\nplt.scater(x,y,c=y)\nPandas describe function also helps a lot\npd.describe()\nx.mean()\nx.var()\nAlso value counts and isnull is very helpfull\nx.value_counts()\nx.isnull()\n\n\n\n3.3.2 2. Feature relation\n\nScatter plot\n\nplt.scatter(x,y)\n\nfor classificaiton we can color map the label\nfor regression heatmap can be used.\nAlso we can compare the scatter plot in trianing and test set is same.\n\n\n\ncolor_code_train_test\n\n\nThe following graph show the diagonal realtion. The equation of a diagonal, x1 -&gt; xaxis and x2-&gt; y axis\n\n\\[x2&lt;=1 -x1\\]\n\nThe equation of diagonal line is $ x1 + x2 = 1 $\n\n\n\n\ndiagonal_equation\n\n\n\nSuppose we found this relaiton but how to use them. There are differet ways but for tree based model we can create the difference or ratio of these two features.\nIf we see the following scatter plot, we can see that there are some outliers. So we can remove them.\n\n\n\nscatter plot\n\n\nSo how this is helpful, our goal is to generate features. How to generate feature from this plot. As we see two traingles, we can create a feature where each triangle will get a set of points and hope this feature will help.\nIf we have smaller number of features, we can use pandas for all features together.\n\npd.scatter_matrix(df)\n\nIt is always good to use scatter plot and histogram in same plot. Scatter plot -&gt; week information about densities, while histogram -&gt; don’t show feature interaction.\nWe can also create a correlation plot. It is a heatmap. It is very helpfull to find the correlation between features. It is also good to see the correlation between features and labels.\n\ndf.corr(), plt.matshow(..)\n\nWe can also create other matrix other than correlation matrix, How many times, one feature is greater than another feature.\nif the matrix is a total mess like following\n\n\n\n\nmessy_matrix\n\n\nwe can create some kind of clsutering and then plot them, like k means clustering or rows and columnd and reorder those features. The following plot is the result of k means clustering.\n\n\n\nordered\n\n\n\n3.3.2.1 Feature groups\n\nNew features based on groups\nOne important feature plot could\n\ndf.mean().plot(style='.')\nx -&gt; feature y -&gt; feature mean\n\nIf this is random, then may be we will see random. But if we sort them.\n\ndf.mean().sort_values().plot(style='.')\n\n\n\nordered_feature_mean\n\n\n\nNow we can have close look to each group and use imagination to create new features.\n\nNext post can be found here"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "",
    "text": "Related Content\n\n\n\nThis post is part of our deep learning foundations series. You might also be interested in: - Data Science Steps Series - Feature Preprocessing"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#why-matrix-multiplication-matters",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#why-matrix-multiplication-matters",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.1 Why Matrix Multiplication Matters",
    "text": "1.1 Why Matrix Multiplication Matters\nMatrix multiplication is fundamental to deep learning because:\n\nIt’s the core operation in neural network layers\nIt enables efficient parallel computation\nIt allows us to represent complex transformations compactly"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#implementation-from-scratch",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#implementation-from-scratch",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.2 Implementation from Scratch",
    "text": "1.2 Implementation from Scratch\nLet’s implement matrix multiplication using Python and NumPy:\n\nimport numpy as np\nimport torch\nfrom typing import List, Tuple\nimport matplotlib.pyplot as plt\n\ndef matmul(a: List[List[float]], b: List[List[float]]) -&gt; List[List[float]]:\n    \"\"\"Matrix multiplication from scratch\"\"\"\n    # Check dimensions\n    assert len(a[0]) == len(b), \"Incompatible dimensions\"\n    \n    # Initialize result matrix\n    result = [[0.0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    \n    # Perform multiplication\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example matrices\nA = [[1, 2], [3, 4]]\nB = [[5, 6], [7, 8]]\n\n# Calculate result\nresult = matmul(A, B)\nprint(\"Result of matrix multiplication:\")\nprint(np.array(result))\n\nResult of matrix multiplication:\n[[19. 22.]\n [43. 50.]]"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#visualizing-matrix-multiplication",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#visualizing-matrix-multiplication",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.3 Visualizing Matrix Multiplication",
    "text": "1.3 Visualizing Matrix Multiplication\nLet’s create a visual representation of how matrix multiplication works:\n\ndef plot_matrix_mult(A: np.ndarray, B: np.ndarray) -&gt; None:\n    \"\"\"Visualize matrix multiplication process\"\"\"\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Plot first matrix\n    ax1.imshow(A, cmap='viridis')\n    ax1.set_title('Matrix A')\n    \n    # Plot second matrix\n    ax2.imshow(B, cmap='viridis')\n    ax2.set_title('Matrix B')\n    \n    # Plot result\n    result = np.dot(A, B)\n    ax3.imshow(result, cmap='viridis')\n    ax3.set_title('A × B')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Create example matrices\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nplot_matrix_mult(A, B)"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#pytorch-implementation",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#pytorch-implementation",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.4 PyTorch Implementation",
    "text": "1.4 PyTorch Implementation\nIn practice, we use optimized libraries like PyTorch:\n\n# Convert to PyTorch tensors\nA_torch = torch.tensor(A, dtype=torch.float32)\nB_torch = torch.tensor(B, dtype=torch.float32)\n\n# PyTorch matrix multiplication\nresult_torch = torch.matmul(A_torch, B_torch)\nprint(\"PyTorch result:\")\nprint(result_torch)\n\nPyTorch result:\ntensor([[19., 22.],\n        [43., 50.]])"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#performance-comparison",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#performance-comparison",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.5 Performance Comparison",
    "text": "1.5 Performance Comparison\nLet’s compare our implementation with NumPy and PyTorch:\n\nimport time\n\ndef benchmark_matmul(size: int = 100) -&gt; None:\n    \"\"\"Compare performance of different implementations\"\"\"\n    # Generate random matrices\n    A = np.random.randn(size, size)\n    B = np.random.randn(size, size)\n    \n    # Custom implementation\n    start = time.time()\n    _ = matmul(A.tolist(), B.tolist())\n    custom_time = time.time() - start\n    \n    # NumPy\n    start = time.time()\n    _ = np.dot(A, B)\n    numpy_time = time.time() - start\n    \n    # PyTorch\n    A_torch = torch.tensor(A)\n    B_torch = torch.tensor(B)\n    start = time.time()\n    _ = torch.matmul(A_torch, B_torch)\n    torch_time = time.time() - start\n    \n    print(f\"Custom implementation: {custom_time:.4f}s\")\n    print(f\"NumPy: {numpy_time:.4f}s\")\n    print(f\"PyTorch: {torch_time:.4f}s\")\n\nbenchmark_matmul()\n\nCustom implementation: 0.0618s\nNumPy: 0.0094s\nPyTorch: 0.0001s"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#key-takeaways",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#key-takeaways",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.6 Key Takeaways",
    "text": "1.6 Key Takeaways\n\nMatrix multiplication is a fundamental operation in deep learning\nUnderstanding it from first principles helps debug neural networks\nLibraries like PyTorch provide highly optimized implementations\nThe operation is inherently parallelizable\n\n\n\n\n\n\n\nFastAI Insight\n\n\n\nJeremy Howard emphasizes understanding matrix multiplication from scratch because it’s the foundation of neural network operations. This understanding helps in debugging and optimizing deep learning models."
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#next-steps",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#next-steps",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.7 Next Steps",
    "text": "1.7 Next Steps\nIn future posts, we’ll explore: - How matrix multiplication enables neural network layers - Efficient implementations using CUDA - Common optimization techniques"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#related-posts",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#related-posts",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.8 Related Posts",
    "text": "1.8 Related Posts\n\nData Science Steps Series\nFeature Preprocessing\nUsing Nougat for Research Papers"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#references",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#references",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.9 References",
    "text": "1.9 References\n\nFastAI Course\nDeep Learning Book - Linear Algebra Chapter\nPyTorch Documentation"
  },
  {
    "objectID": "posts/series/computer-vision-with-pytorch.html",
    "href": "posts/series/computer-vision-with-pytorch.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{untitled,\n  author = {, Hasan},\n  url = {https://hasangoni.quarto.pub/hasan-blog-post/posts/series/computer-vision-with-pytorch.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHasan. n.d. https://hasangoni.quarto.pub/hasan-blog-post/posts/series/computer-vision-with-pytorch.html."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html",
    "href": "posts/data-science-steps-to-follow-part06/index.html",
    "title": "Data Science Steps to Follow - Part 6",
    "section": "",
    "text": "This post covers essential validation strategies and model evaluation techniques in data science. We’ll explore different approaches to ensure your models generalize well to unseen data.\n\n\n\n\nThe simplest form of validation where we split our data into training and validation sets:\nflowchart TB\n    A[data] --&gt; B[Validation data]\n    A[data] --&gt; C[Training data]\nThis approach is good when: - You have sufficient data - The data distribution is relatively uniform - Time-based dependencies are not critical\n\n\n\nA more robust approach that uses multiple training-validation splits:\nflowchart TB\n    A[data] --&gt; B[Fold 1]\n    B[Fold 1] --&gt; C[Training data]\n    B[Fold 1] --&gt; D[Validation data]\n\n    A[data] --&gt; E[Fold 2]\n    E[Fold 2] --&gt; F[Training data]\n    E[Fold 2] --&gt; G[Validation data]\n\n    A[data] --&gt; H[Fold 3]\n    H[Fold 3] --&gt; I[Training data]\n    H[Fold 3] --&gt; J[Validation data]\n\n    C(Training data) --&gt; K[Average]\n    D(Training data) --&gt; K[Average]\n    F(Training data) --&gt; K[Average]\n    G(Training data) --&gt; K[Average]\n    I(Training data) --&gt; K[Average]\n    J(Training data) --&gt; K[Average]\n\n    K(Average) --&gt; L[Final model performance]\n\n\n\nBest for very small datasets where we need to maximize training data usage.\n\n\n\n\n\n\n\nTarget leakage\nTrain-test contamination\nFeature leakage\n\n\n\n\n\nTraining data not representative\nTemporal dependencies\nConcept drift\n\n\n\n\n\nToo little validation data\nImbalanced splits\nHigh variance in metrics\n\n\n\n\n\n\nAlways use stratification for:\n\nSmall datasets\nImbalanced classes\nMulti-class problems\n\nConsider temporal aspects:\n\nTime-based splits for time series\nRolling window validation\nForward-chaining\n\nMonitor multiple metrics:\n\nAccuracy/RMSE\nROC-AUC\nPrecision-Recall\nBusiness metrics\n\n\n\n\n\nHere’s how to implement different validation strategies using scikit-learn:\nfrom sklearn.model_selection import (\n    train_test_split,\n    KFold,\n    StratifiedKFold,\n    LeaveOneOut\n)\n\n# Simple holdout\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# K-Fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfor train_idx, val_idx in kf.split(X):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n# Stratified K-Fold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor train_idx, val_idx in skf.split(X, y):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n\n\nNow that you understand validation strategies, you can: 1. Choose appropriate validation methods for your data 2. Implement robust validation pipelines 3. Avoid common pitfalls in model evaluation\n\n\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning\nRaschka, S. (2018). Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning\nKohavi, R. (1995). A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#validation-strategies",
    "href": "posts/data-science-steps-to-follow-part06/index.html#validation-strategies",
    "title": "Data Science Steps to Follow - Part 6",
    "section": "",
    "text": "The simplest form of validation where we split our data into training and validation sets:\nflowchart TB\n    A[data] --&gt; B[Validation data]\n    A[data] --&gt; C[Training data]\nThis approach is good when: - You have sufficient data - The data distribution is relatively uniform - Time-based dependencies are not critical\n\n\n\nA more robust approach that uses multiple training-validation splits:\nflowchart TB\n    A[data] --&gt; B[Fold 1]\n    B[Fold 1] --&gt; C[Training data]\n    B[Fold 1] --&gt; D[Validation data]\n\n    A[data] --&gt; E[Fold 2]\n    E[Fold 2] --&gt; F[Training data]\n    E[Fold 2] --&gt; G[Validation data]\n\n    A[data] --&gt; H[Fold 3]\n    H[Fold 3] --&gt; I[Training data]\n    H[Fold 3] --&gt; J[Validation data]\n\n    C(Training data) --&gt; K[Average]\n    D(Training data) --&gt; K[Average]\n    F(Training data) --&gt; K[Average]\n    G(Training data) --&gt; K[Average]\n    I(Training data) --&gt; K[Average]\n    J(Training data) --&gt; K[Average]\n\n    K(Average) --&gt; L[Final model performance]\n\n\n\nBest for very small datasets where we need to maximize training data usage."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#common-validation-problems",
    "href": "posts/data-science-steps-to-follow-part06/index.html#common-validation-problems",
    "title": "Data Science Steps to Follow - Part 6",
    "section": "",
    "text": "Target leakage\nTrain-test contamination\nFeature leakage\n\n\n\n\n\nTraining data not representative\nTemporal dependencies\nConcept drift\n\n\n\n\n\nToo little validation data\nImbalanced splits\nHigh variance in metrics"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#best-practices",
    "href": "posts/data-science-steps-to-follow-part06/index.html#best-practices",
    "title": "Data Science Steps to Follow - Part 6",
    "section": "",
    "text": "Always use stratification for:\n\nSmall datasets\nImbalanced classes\nMulti-class problems\n\nConsider temporal aspects:\n\nTime-based splits for time series\nRolling window validation\nForward-chaining\n\nMonitor multiple metrics:\n\nAccuracy/RMSE\nROC-AUC\nPrecision-Recall\nBusiness metrics"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#implementation-examples",
    "href": "posts/data-science-steps-to-follow-part06/index.html#implementation-examples",
    "title": "Data Science Steps to Follow - Part 6",
    "section": "",
    "text": "Here’s how to implement different validation strategies using scikit-learn:\nfrom sklearn.model_selection import (\n    train_test_split,\n    KFold,\n    StratifiedKFold,\n    LeaveOneOut\n)\n\n# Simple holdout\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# K-Fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfor train_idx, val_idx in kf.split(X):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n# Stratified K-Fold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor train_idx, val_idx in skf.split(X, y):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#next-steps",
    "href": "posts/data-science-steps-to-follow-part06/index.html#next-steps",
    "title": "Data Science Steps to Follow - Part 6",
    "section": "",
    "text": "Now that you understand validation strategies, you can: 1. Choose appropriate validation methods for your data 2. Implement robust validation pipelines 3. Avoid common pitfalls in model evaluation"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#references",
    "href": "posts/data-science-steps-to-follow-part06/index.html#references",
    "title": "Data Science Steps to Follow - Part 6",
    "section": "",
    "text": "Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning\nRaschka, S. (2018). Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning\nKohavi, R. (1995). A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection"
  },
  {
    "objectID": "categories.html",
    "href": "categories.html",
    "title": "Categories",
    "section": "",
    "text": "Welcome to the category index! Here you can find all posts organized by their main topics.\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nFeature Preprocessing and Generation\n\n\n\ndata-science\n\n\nfeature-engineering\n\n\ntutorial\n\n\n\n\nHasan Goni\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation Strategy and Model Evaluation\n\n\n\ndata-science\n\n\nmachine-learning\n\n\nvalidation\n\n\n\n\nHasan Goni\n\n\nNov 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\ndata-science\n\n\n\n\nHasan Goni\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nA Deep Dive into Neural Network Fundamentals\n\n\n\ndeep-learning\n\n\nmathematics\n\n\nfastai\n\n\n\n\nHasan Goni\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items\n\n\n\n\n\nUse the links below to browse specific categories:\n\n\n\n\nData Science\nMachine Learning\nDeep Learning\n\n\n\n\n\nPython\nPyTorch\nNumPy\n\n\n\n\n\nEDA\nVisualization\nTools"
  },
  {
    "objectID": "categories.html#data-science",
    "href": "categories.html#data-science",
    "title": "Categories",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nFeature Preprocessing and Generation\n\n\n\ndata-science\n\n\nfeature-engineering\n\n\ntutorial\n\n\n\n\nHasan Goni\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation Strategy and Model Evaluation\n\n\n\ndata-science\n\n\nmachine-learning\n\n\nvalidation\n\n\n\n\nHasan Goni\n\n\nNov 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\ndata-science\n\n\n\n\nHasan Goni\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "categories.html#deep-learning",
    "href": "categories.html#deep-learning",
    "title": "Categories",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nA Deep Dive into Neural Network Fundamentals\n\n\n\ndeep-learning\n\n\nmathematics\n\n\nfastai\n\n\n\n\nHasan Goni\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "categories.html#tools-and-technologies",
    "href": "categories.html#tools-and-technologies",
    "title": "Categories",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "categories.html#all-categories",
    "href": "categories.html#all-categories",
    "title": "Categories",
    "section": "",
    "text": "Use the links below to browse specific categories:\n\n\n\n\nData Science\nMachine Learning\nDeep Learning\n\n\n\n\n\nPython\nPyTorch\nNumPy\n\n\n\n\n\nEDA\nVisualization\nTools"
  }
]