[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS notes",
    "section": "",
    "text": "Computer Vision Foundations Series\n\n\n\n\n\n\n\nseries\n\n\ncomputer-vision\n\n\nopencv\n\n\npytorch\n\n\nfoundation-models\n\n\n\n\nFrom Pixels to Foundation Models ‚Äì A Gentle Journey for Beginners. Learn computer vision the fastai way: start with working code, build intuition, then dive deeper.\n\n\n\n\n\n\nJun 28, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nVLM Series\n\n\n\n\n\n\n\nseries\n\n\nvision-language\n\n\nVLM\n\n\n\n\nA practical guide to Vision-Language Models (VLMs) with a focus on open-source models and hands-on PyTorch/HuggingFace workflows\n\n\n\n\n\n\nMay 3, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nFine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists\n\n\n\n\n\n\n\nLLM\n\n\nFine-tuning\n\n\nHuggingFace\n\n\nLoRA\n\n\nQLoRA\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nHands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists\n\n\n\n\n\n\n\nAI\n\n\nLLM\n\n\nReasoning\n\n\nNLP\n\n\nHuggingFace\n\n\nColab\n\n\nTransformers\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nFinding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)\n\n\n\n\n\n\n\ndata science\n\n\nmachine learning\n\n\ntutorial\n\n\nanomaly detection\n\n\n\n\nA fun, accessible explanation of anomaly detection using density estimation techniques\n\n\n\n\n\n\nMay 3, 2025\n\n\nYour Name\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection Series\n\n\n\n\n\n\n\ndata science\n\n\nmachine learning\n\n\nanomaly detection\n\n\n\n\nA series exploring anomaly detection techniques with fun, accessible explanations.\n\n\n\n\n\n\nMay 3, 2025\n\n\nYour Name\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps Series\n\n\n\n\n\n\n\nseries\n\n\ndata-science\n\n\n\n\nA practical guide to data science following FastAI‚Äôs top-down learning approach\n\n\n\n\n\n\nMay 2, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nImage Segmentation: Dividing and Conquering\n\n\n\n\n\n\n\ncomputer-vision\n\n\nsegmentation\n\n\nmorphology\n\n\nthresholding\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nWhy Deep Learning? When Classical Methods Hit the Wall\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeep-learning\n\n\nneural-networks\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nWhere to Go Next: Your Computer Vision Journey\n\n\n\n\n\n\n\ncomputer-vision\n\n\ncareer\n\n\nlearning\n\n\nresources\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nYour First CV Project: Putting It All Together\n\n\n\n\n\n\n\ncomputer-vision\n\n\nproject\n\n\napplication\n\n\ndeployment\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nFinding Patterns: Edges, Contours, and Shapes\n\n\n\n\n\n\n\ncomputer-vision\n\n\nedge-detection\n\n\ncontours\n\n\nopencv\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nFeature Magic: What Makes Images Unique\n\n\n\n\n\n\n\ncomputer-vision\n\n\nfeatures\n\n\nkeypoints\n\n\nmatching\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nOpenCV Essentials: Your First Computer Vision Toolkit\n\n\n\n\n\n\n\ncomputer-vision\n\n\nopencv\n\n\nimage-processing\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nModern Vision Models: CNNs, Vision Transformers, and DINOv2\n\n\n\n\n\n\n\ncomputer-vision\n\n\ntransformers\n\n\nfoundation-models\n\n\ndinov2\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nImages as Data: How Computers See the World\n\n\n\n\n\n\n\ncomputer-vision\n\n\nnumpy\n\n\nmatplotlib\n\n\nbasics\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nWhy Computer Vision? Teaching a Robot to See\n\n\n\n\n\n\n\nComputer Vision\n\n\nAI\n\n\nMachine Learning\n\n\nOpenCV\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nHasan\n\n\n\n\n\n\n  \n\n\n\n\nUV: Python Packaging, Reimagined (and Lightning Fast)\n\n\n\n\n\n\n\npython\n\n\npackaging\n\n\ndevelopment\n\n\ntools\n\n\nuv\n\n\nscripting\n\n\nworkflow\n\n\nopinion\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2024\n\n\nAn Expert Storyteller\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Multiplication\n\n\n\n\n\n\n\nfastai\n\n\nmatrix multiplication\n\n\n\n\nMatrix Multiplication from fastai course 2022\n\n\n\n\n\n\nAug 13, 2024\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nTools Series\n\n\n\n\n\n\n\nseries\n\n\ntools\n\n\npython\n\n\ndevelopment\n\n\n\n\nA series exploring modern Python development tools for productivity, reproducibility, and speed.\n\n\n\n\n\n\nMay 10, 2024\n\n\nYour Name\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Matrix Multiplication from FastAI\n\n\nA Deep Dive into Neural Network Fundamentals\n\n\n\n\ndeep-learning\n\n\nmathematics\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nHow to Use Nougat to Read Scientific Paper\n\n\nResearch paper working\n\n\n\n\nNlp\n\n\nResearch paper\n\n\nText\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Data Science & Machine Learning Blog\n\n\n\n\n\n\n\nnews\n\n\ndata-science\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow - 05\n\n\nFeature Extraction from Image\n\n\n\n\ncode\n\n\nimage\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow - 06\n\n\nValidation Strategy and Model Evaluation\n\n\n\n\ndata-science\n\n\nmachine-learning\n\n\nvalidation\n\n\n\n\nLearn about different validation strategies and how to properly evaluate your models\n\n\n\n\n\n\nNov 22, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow - 04\n\n\nFeature Extraction from Text\n\n\n\n\ncode\n\n\ntext\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow - 03\n\n\nExploring Anonymized data\n\n\n\n\ncode\n\n\ntabular\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow - 02\n\n\nFeature Preprocessing and Generation\n\n\n\n\ndata-science\n\n\nfeature-engineering\n\n\ntutorial\n\n\n\n\nLearn essential techniques for preparing and engineering features in your data science projects.\n\n\n\n\n\n\nNov 20, 2022\n\n\nHasan Goni\n\n\n\n\n\n\n  \n\n\n\n\nData Science Steps to Follow - 01\n\n\nExploratory Data Analysis Fundamentals\n\n\n\n\ncode\n\n\nanalysis\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2022\n\n\nHasan Goni\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is more about my notes. I am inthusiast about machine learning"
  },
  {
    "objectID": "notebooks/cv-foundations-08-first-cv-project.html",
    "href": "notebooks/cv-foundations-08-first-cv-project.html",
    "title": "Computer Vision Foundations #8: 08 First Cv Project",
    "section": "",
    "text": "This notebook accompanies the blog post series.\nüöß **Coming Soon-p notebooks* This notebook is being prepared with interactive examples.\nFor now, you can read the blog post and we‚Äôll have the full interactive notebook ready soon!\n\n\nCode\n# Setup code will go here\nprint('Notebook coming soon! üöÄ')"
  },
  {
    "objectID": "notebooks/cv-foundations-04-finding-patterns.html",
    "href": "notebooks/cv-foundations-04-finding-patterns.html",
    "title": "Computer Vision Foundations #4: 04 Finding Patterns",
    "section": "",
    "text": "This notebook accompanies the blog post series.\nüöß **Coming Soon-p notebooks* This notebook is being prepared with interactive examples.\nFor now, you can read the blog post and we‚Äôll have the full interactive notebook ready soon!\n\n\nCode\n# Setup code will go here\nprint('Notebook coming soon! üöÄ')"
  },
  {
    "objectID": "notebooks/cv-foundations-01-why-computer-vision.html",
    "href": "notebooks/cv-foundations-01-why-computer-vision.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code"
  },
  {
    "objectID": "notebooks/cv-foundations-01-why-computer-vision.html#what-youll-learn",
    "href": "notebooks/cv-foundations-01-why-computer-vision.html#what-youll-learn",
    "title": "",
    "section": "What You‚Äôll Learn",
    "text": "What You‚Äôll Learn\n\nWhat computer vision really is\nHow computers ‚Äúsee‚Äù images as numbers\nYour first computer vision program\nReal-world applications that will inspire you"
  },
  {
    "objectID": "notebooks/cv-foundations-01-why-computer-vision.html#setup-installing-required-libraries",
    "href": "notebooks/cv-foundations-01-why-computer-vision.html#setup-installing-required-libraries",
    "title": "",
    "section": "Setup: Installing Required Libraries",
    "text": "Setup: Installing Required Libraries\nLet‚Äôs start by installing the libraries we‚Äôll need:\n\n\nCode\n# Install required packages\n%pip install opencv-python-headless matplotlib numpy pillow requests\n\n# Import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport cv2\nfrom google.colab.patches import cv2_imshow\n\nprint(\"‚úÖ All libraries installed and imported successfully!\")"
  },
  {
    "objectID": "notebooks/cv-foundations-01-why-computer-vision.html#your-first-computer-vision-moment",
    "href": "notebooks/cv-foundations-01-why-computer-vision.html#your-first-computer-vision-moment",
    "title": "",
    "section": "Your First Computer Vision Moment",
    "text": "Your First Computer Vision Moment\nLet‚Äôs see what happens when we show a computer an image. To the computer, this beautiful photo is just numbers!\n\n\nCode\n# Load image from URL\nurl = \"https://images.unsplash.com/photo-1601758228041-f3b2795255f1?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=400&q=80\"\nresponse = requests.get(url)\nimg = Image.open(BytesIO(response.content))\nimg_array = np.array(img)\n\n# Display the image\nplt.figure(figsize=(8, 6))\nplt.imshow(img_array)\nplt.title(\"What We See: A Cute Cat and Dog\")\nplt.axis('off')\nplt.show()\n\n# Show what the computer sees\nprint(f\"Image shape: {img_array.shape}\")  # height x width x color channels\nprint(f\"Data type: {img_array.dtype}\")\nprint(f\"First few pixels (top-left corner): {img_array[0, 0]}\")\nprint(f\"Pixel value range: {img_array.min()} to {img_array.max()}\")"
  },
  {
    "objectID": "notebooks/cv-foundations-01-why-computer-vision.html#your-first-computer-vision-program",
    "href": "notebooks/cv-foundations-01-why-computer-vision.html#your-first-computer-vision-program",
    "title": "",
    "section": "Your First Computer Vision Program",
    "text": "Your First Computer Vision Program\nLet‚Äôs create some magic! We‚Äôll apply simple filters to transform an image:\n\n\nCode\n# Convert PIL image to OpenCV format\nimg_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n\n# Apply different \"magic\" filters\nblur_img = cv2.GaussianBlur(img_cv, (51, 51), 0)\nedge_img = cv2.Canny(img_cv, 100, 200)\n\n# Convert back to RGB for display\nblur_img_rgb = cv2.cvtColor(blur_img, cv2.COLOR_BGR2RGB)\n\n# Display results\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(img_array)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(blur_img_rgb)\nplt.title(\"Dreamy Blur Effect\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(edge_img, cmap='gray')\nplt.title(\"Edge Detection\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üéâ Congratulations! You just applied your first computer vision algorithms!\")"
  },
  {
    "objectID": "notebooks/cv-foundations-07-modern-vision-models.html",
    "href": "notebooks/cv-foundations-07-modern-vision-models.html",
    "title": "Computer Vision Foundations #7: 07 Modern Vision Models",
    "section": "",
    "text": "This notebook accompanies the blog post series.\nüöß **Coming Soon-p notebooks* This notebook is being prepared with interactive examples.\nFor now, you can read the blog post and we‚Äôll have the full interactive notebook ready soon!\n\n\nCode\n# Setup code will go here\nprint('Notebook coming soon! üöÄ')"
  },
  {
    "objectID": "posts/series/data-science-steps.html",
    "href": "posts/series/data-science-steps.html",
    "title": "Data Science Steps Series",
    "section": "",
    "text": "This series follows FastAI‚Äôs practical, top-down approach to learning data science. Instead of starting with theory, we‚Äôll begin with practical applications and gradually dive deeper into the underlying concepts.\n\n\n\nStart with the practical: We begin with working code and real applications\nSpiral learning: Revisit concepts multiple times with increasing depth\nLearn by doing: Each post includes hands-on notebooks and exercises\n\n\n\n\n\nGetting Started with Data Science\n\nSetting up your environment\nYour first end-to-end project\n\nData Exploration and Visualization\n\nPractical EDA techniques\nCreating impactful visualizations\n\nFeature Engineering and Selection\n\nTransforming data for better models\nSelecting the most important features\n\nModel Selection and Training\n\nChoosing the right model\nTraining best practices\n\nModel Evaluation and Interpretation\n\nUnderstanding model performance\nInterpreting model decisions\n\nDeployment and Monitoring\n\nTaking models to production\nMonitoring and maintaining models"
  },
  {
    "objectID": "posts/series/data-science-steps.html#series-overview",
    "href": "posts/series/data-science-steps.html#series-overview",
    "title": "Data Science Steps Series",
    "section": "",
    "text": "This series follows FastAI‚Äôs practical, top-down approach to learning data science. Instead of starting with theory, we‚Äôll begin with practical applications and gradually dive deeper into the underlying concepts.\n\n\n\nStart with the practical: We begin with working code and real applications\nSpiral learning: Revisit concepts multiple times with increasing depth\nLearn by doing: Each post includes hands-on notebooks and exercises\n\n\n\n\n\nGetting Started with Data Science\n\nSetting up your environment\nYour first end-to-end project\n\nData Exploration and Visualization\n\nPractical EDA techniques\nCreating impactful visualizations\n\nFeature Engineering and Selection\n\nTransforming data for better models\nSelecting the most important features\n\nModel Selection and Training\n\nChoosing the right model\nTraining best practices\n\nModel Evaluation and Interpretation\n\nUnderstanding model performance\nInterpreting model decisions\n\nDeployment and Monitoring\n\nTaking models to production\nMonitoring and maintaining models"
  },
  {
    "objectID": "posts/series/data-science-steps.html#prerequisites",
    "href": "posts/series/data-science-steps.html#prerequisites",
    "title": "Data Science Steps Series",
    "section": "0.2 Prerequisites",
    "text": "0.2 Prerequisites\n\nBasic Python knowledge\nFamiliarity with Jupyter notebooks\nUnderstanding of basic statistics (we‚Äôll review as needed)"
  },
  {
    "objectID": "posts/series/data-science-steps.html#tools-well-use",
    "href": "posts/series/data-science-steps.html#tools-well-use",
    "title": "Data Science Steps Series",
    "section": "0.3 Tools We‚Äôll Use",
    "text": "0.3 Tools We‚Äôll Use\n\nPyTorch for deep learning\nPandas for data manipulation\nScikit-learn for traditional ML\nFastAI for rapid prototyping"
  },
  {
    "objectID": "posts/series/data-science-steps.html#getting-help",
    "href": "posts/series/data-science-steps.html#getting-help",
    "title": "Data Science Steps Series",
    "section": "0.4 Getting Help",
    "text": "0.4 Getting Help\n\nUse the comments section below each post\nCheck the GitHub repository for code\nJoin our discussion forum (coming soon)\n\n\n\n\n\n\n\nNote\n\n\n\nThis series is continuously updated with new content and improvements based on reader feedback."
  },
  {
    "objectID": "posts/series/cv-foundations/08-modern-vision-models.html",
    "href": "posts/series/cv-foundations/08-modern-vision-models.html",
    "title": "Modern Vision Models: CNNs, Vision Transformers, and DINOv2",
    "section": "",
    "text": "Remember when we thought AlexNet was revolutionary in 2012? That was just the beginning! In the past decade, computer vision has evolved at breakneck speed:\n\n2012: AlexNet - 8 layers, 60M parameters\n2015: ResNet - 152 layers, skip connections\n2017: Attention mechanisms emerge\n2020: Vision Transformers - ‚ÄúAttention is all you need‚Äù for vision\n2023: DINOv2 - Foundation models that understand everything\n\nToday, we‚Äôre going to explore this incredible journey and show you how to use the most powerful vision models ever created!"
  },
  {
    "objectID": "posts/series/cv-foundations/08-modern-vision-models.html#the-evolution-of-vision-from-alexnet-to-dinov2",
    "href": "posts/series/cv-foundations/08-modern-vision-models.html#the-evolution-of-vision-from-alexnet-to-dinov2",
    "title": "Modern Vision Models: CNNs, Vision Transformers, and DINOv2",
    "section": "",
    "text": "Remember when we thought AlexNet was revolutionary in 2012? That was just the beginning! In the past decade, computer vision has evolved at breakneck speed:\n\n2012: AlexNet - 8 layers, 60M parameters\n2015: ResNet - 152 layers, skip connections\n2017: Attention mechanisms emerge\n2020: Vision Transformers - ‚ÄúAttention is all you need‚Äù for vision\n2023: DINOv2 - Foundation models that understand everything\n\nToday, we‚Äôre going to explore this incredible journey and show you how to use the most powerful vision models ever created!"
  },
  {
    "objectID": "posts/series/cv-foundations/08-modern-vision-models.html#the-cnn-dynasty-resnet-efficientnet-and-beyond",
    "href": "posts/series/cv-foundations/08-modern-vision-models.html#the-cnn-dynasty-resnet-efficientnet-and-beyond",
    "title": "Modern Vision Models: CNNs, Vision Transformers, and DINOv2",
    "section": "0.2 The CNN Dynasty: ResNet, EfficientNet, and Beyond",
    "text": "0.2 The CNN Dynasty: ResNet, EfficientNet, and Beyond\nBefore transformers took over, CNNs ruled the vision world. Let‚Äôs explore the key innovations:\n\n0.2.1 ResNet: The Skip Connection Revolution\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Understanding ResNet's key innovation: skip connections\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        \n        # Main path\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        # Skip connection (the magic!)\n        self.skip = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        # Main path\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        \n        # Add skip connection (this is the key!)\n        out += self.skip(x)\n        out = torch.relu(out)\n        \n        return out\n\n# Create a simple ResNet-like model\nclass MiniResNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(MiniResNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.pool = nn.MaxPool2d(3, 2, 1)\n        \n        # Stack residual blocks\n        self.layer1 = self._make_layer(64, 64, 2, 1)\n        self.layer2 = self._make_layer(64, 128, 2, 2)\n        self.layer3 = self._make_layer(128, 256, 2, 2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)\n    \n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = []\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Compare with official ResNet\nmini_resnet = MiniResNet(num_classes=1000)\nofficial_resnet = models.resnet18(pretrained=True)\n\nprint(\"Mini ResNet:\")\nprint(f\"Parameters: {sum(p.numel() for p in mini_resnet.parameters()):,}\")\n\nprint(\"\\nOfficial ResNet-18:\")\nprint(f\"Parameters: {sum(p.numel() for p in official_resnet.parameters()):,}\")\n\n\nMini ResNet:\nParameters: 3,042,024\n\nOfficial ResNet-18:\nParameters: 11,689,512\n\n\nüéØ Try it yourself! Open in Colab\n\n\n0.2.2 EfficientNet: Scaling Done Right\n\n\nCode\n# EfficientNet's key insight: compound scaling\nfrom torchvision.models import efficientnet_b0, efficientnet_b7\n\n# Load different EfficientNet variants\nefficient_b0 = efficientnet_b0(pretrained=True)\nefficient_b7 = efficientnet_b7(pretrained=True)\n\ndef model_info(model, name):\n    total_params = sum(p.numel() for p in model.parameters())\n    return {\n        'name': name,\n        'parameters': total_params,\n        'size_mb': total_params * 4 / (1024 * 1024)  # Rough estimate\n    }\n\nmodels_comparison = [\n    model_info(efficient_b0, 'EfficientNet-B0'),\n    model_info(efficient_b7, 'EfficientNet-B7'),\n    model_info(official_resnet, 'ResNet-18')\n]\n\nprint(\"Model Comparison:\")\nprint(\"-\" * 50)\nfor info in models_comparison:\n    print(f\"{info['name']:20} | {info['parameters']:&gt;10,} params | {info['size_mb']:&gt;6.1f} MB\")\n\n\nModel Comparison:\n--------------------------------------------------\nEfficientNet-B0      |  5,288,548 params |   20.2 MB\nEfficientNet-B7      | 66,347,960 params |  253.1 MB\nResNet-18            | 11,689,512 params |   44.6 MB"
  },
  {
    "objectID": "posts/series/cv-foundations/08-modern-vision-models.html#the-transformer-revolution-vision-meets-attention",
    "href": "posts/series/cv-foundations/08-modern-vision-models.html#the-transformer-revolution-vision-meets-attention",
    "title": "Modern Vision Models: CNNs, Vision Transformers, and DINOv2",
    "section": "0.3 The Transformer Revolution: Vision Meets Attention",
    "text": "0.3 The Transformer Revolution: Vision Meets Attention\nIn 2020, everything changed when researchers asked: ‚ÄúWhat if we applied transformers to vision?‚Äù\n\n0.3.1 Understanding Vision Transformers\n\n\nCode\nclass PatchEmbedding(nn.Module):\n    \"\"\"Convert image to sequence of patches\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        # Patch embedding using convolution\n        self.projection = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n    \n    def forward(self, x):\n        # x shape: (batch_size, channels, height, width)\n        x = self.projection(x)  # (batch_size, embed_dim, num_patches_h, num_patches_w)\n        x = x.flatten(2)        # (batch_size, embed_dim, num_patches)\n        x = x.transpose(1, 2)   # (batch_size, num_patches, embed_dim)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head self-attention mechanism\"\"\"\n    def __init__(self, embed_dim=768, num_heads=12):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n    \n    def forward(self, x):\n        batch_size, seq_len, embed_dim = x.shape\n        \n        # Generate Q, K, V\n        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_len, head_dim)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Compute attention\n        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        attn = torch.softmax(attn, dim=-1)\n        \n        # Apply attention to values\n        out = (attn @ v).transpose(1, 2).reshape(batch_size, seq_len, embed_dim)\n        out = self.proj(out)\n        \n        return out, attn\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Single transformer encoder block\"\"\"\n    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadAttention(embed_dim, num_heads)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        # MLP\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Linear(mlp_hidden_dim, embed_dim)\n        )\n    \n    def forward(self, x):\n        # Self-attention with residual connection\n        attn_out, attn_weights = self.attn(self.norm1(x))\n        x = x + attn_out\n        \n        # MLP with residual connection\n        x = x + self.mlp(self.norm2(x))\n        \n        return x, attn_weights\n\nclass SimpleViT(nn.Module):\n    \"\"\"Simplified Vision Transformer\"\"\"\n    def __init__(self, img_size=224, patch_size=16, num_classes=1000, \n                 embed_dim=768, depth=12, num_heads=12):\n        super().__init__()\n        \n        # Patch embedding\n        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n        num_patches = self.patch_embed.num_patches\n        \n        # Class token and position embedding\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        \n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads) for _ in range(depth)\n        ])\n        \n        # Classification head\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n    \n    def forward(self, x):\n        batch_size = x.shape[0]\n        \n        # Patch embedding\n        x = self.patch_embed(x)  # (batch_size, num_patches, embed_dim)\n        \n        # Add class token\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        \n        # Add position embedding\n        x = x + self.pos_embed\n        \n        # Apply transformer blocks\n        attention_maps = []\n        for block in self.blocks:\n            x, attn = block(x)\n            attention_maps.append(attn)\n        \n        # Classification\n        x = self.norm(x)\n        cls_token_final = x[:, 0]  # Use class token for classification\n        out = self.head(cls_token_final)\n        \n        return out, attention_maps\n\n# Create a simple ViT\nsimple_vit = SimpleViT(depth=6, num_heads=8)  # Smaller for demo\nvit_params = sum(p.numel() for p in simple_vit.parameters())\n\nprint(f\"Simple ViT parameters: {vit_params:,}\")\n\n# Test with dummy input\ndummy_input = torch.randn(1, 3, 224, 224)\noutput, attention_maps = simple_vit(dummy_input)\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Number of attention maps: {len(attention_maps)}\")\n\n\nSimple ViT parameters: 44,040,424\nOutput shape: torch.Size([1, 1000])\nNumber of attention maps: 6\n\n\n\n\n0.3.2 Visualizing Attention: What Does the Model Look At?\n\n\nCode\ndef visualize_attention(image, attention_maps, patch_size=16):\n    \"\"\"Visualize what the vision transformer is looking at\"\"\"\n    \n    # Use attention from the last layer, first head\n    attn = attention_maps[-1][0, 0]  # (seq_len, seq_len)\n    \n    # Get attention from class token to all patches\n    cls_attn = attn[0, 1:]  # Exclude class token to class token attention\n    \n    # Reshape to spatial dimensions\n    num_patches_per_side = int(len(cls_attn) ** 0.5)\n    attn_map = cls_attn.reshape(num_patches_per_side, num_patches_per_side)\n    \n    # Resize to image size\n    attn_map = torch.nn.functional.interpolate(\n        attn_map.unsqueeze(0).unsqueeze(0),\n        size=(224, 224),\n        mode='bilinear'\n    ).squeeze()\n    \n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(image.permute(1, 2, 0))\n    plt.title(\"Original Image\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(image.permute(1, 2, 0))\n    plt.imshow(attn_map.detach().numpy(), alpha=0.6, cmap='hot')\n    plt.title(\"Attention Map\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualize attention (you would use a real image)\ndummy_image = torch.randn(3, 224, 224)\nvisualize_attention(dummy_image, attention_maps)"
  },
  {
    "objectID": "posts/series/cv-foundations/08-modern-vision-models.html#foundation-models-the-dinov2-revolution",
    "href": "posts/series/cv-foundations/08-modern-vision-models.html#foundation-models-the-dinov2-revolution",
    "title": "Modern Vision Models: CNNs, Vision Transformers, and DINOv2",
    "section": "0.4 Foundation Models: The DINOv2 Revolution",
    "text": "0.4 Foundation Models: The DINOv2 Revolution\nNow we reach the cutting edge: Foundation Models. DINOv2 (Distillation with No Labels v2) represents a paradigm shift:\n\nSelf-supervised learning: No labels needed!\nUniversal features: Works for any vision task\nIncredible performance: Often beats supervised methods\n\n\n0.4.1 Using DINOv2 with HuggingFace\n\n\nCode\n# Install required packages\n# !pip install transformers torch torchvision\n\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport requests\n\n# Load DINOv2 model and processor\nmodel_name = \"facebook/dinov2-base\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\nprint(f\"Loaded DINOv2 model: {model_name}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\ndef extract_dinov2_features(image_path_or_url):\n    \"\"\"Extract features using DINOv2\"\"\"\n    \n    # Load image\n    if image_path_or_url.startswith('http'):\n        image = Image.open(requests.get(image_path_or_url, stream=True).raw)\n    else:\n        image = Image.open(image_path_or_url)\n    \n    # Process image\n    inputs = processor(images=image, return_tensors=\"pt\")\n    \n    # Extract features\n    with torch.no_grad():\n        outputs = model(**inputs)\n        features = outputs.last_hidden_state\n        \n        # Get CLS token (global image representation)\n        cls_features = features[:, 0]  # Shape: (1, 768)\n        \n        # Get patch features (local representations)\n        patch_features = features[:, 1:]  # Shape: (1, num_patches, 768)\n    \n    return {\n        'cls_features': cls_features,\n        'patch_features': patch_features,\n        'image': image\n    }\n\n# Example usage\ndef demo_dinov2_features():\n    \"\"\"Demonstrate DINOv2 feature extraction\"\"\"\n    \n    # Create dummy image for demo (you would use real images)\n    dummy_image = Image.new('RGB', (224, 224), color='red')\n    \n    # Save temporarily\n    dummy_image.save('temp_image.jpg')\n    \n    # Extract features\n    result = extract_dinov2_features('temp_image.jpg')\n    \n    print(\"DINOv2 Feature Extraction Results:\")\n    print(f\"Global features shape: {result['cls_features'].shape}\")\n    print(f\"Patch features shape: {result['patch_features'].shape}\")\n    \n    # Visualize features\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    plt.imshow(result['image'])\n    plt.title(\"Input Image\")\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(result['cls_features'].squeeze().numpy())\n    plt.title(\"Global Features (768 dimensions)\")\n    plt.xlabel(\"Dimension\")\n    plt.ylabel(\"Value\")\n    \n    plt.subplot(1, 3, 3)\n    # Visualize patch features as heatmap\n    patch_norms = torch.norm(result['patch_features'].squeeze(), dim=1)\n    patch_size = int(len(patch_norms) ** 0.5)\n    patch_map = patch_norms.reshape(patch_size, patch_size)\n    plt.imshow(patch_map.numpy(), cmap='viridis')\n    plt.title(\"Patch Feature Magnitudes\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return result\n\ndemo_result = demo_dinov2_features()\n\n\nLoaded DINOv2 model: facebook/dinov2-base\nModel parameters: 86,580,480\nDINOv2 Feature Extraction Results:\nGlobal features shape: torch.Size([1, 768])\nPatch features shape: torch.Size([1, 256, 768])\n\n\n\n\n\n\n\n0.4.2 Building a DINOv2-Powered Image Similarity Engine\n\n\nCode\nclass DINOv2SimilarityEngine:\n    def __init__(self):\n        self.processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n        self.model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        self.model.eval()\n        self.image_database = {}\n    \n    def extract_features(self, image):\n        \"\"\"Extract DINOv2 features from an image\"\"\"\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            # Use CLS token as global image representation\n            features = outputs.last_hidden_state[:, 0]\n        \n        return features\n    \n    def add_image(self, image_id, image):\n        \"\"\"Add an image to the database\"\"\"\n        features = self.extract_features(image)\n        self.image_database[image_id] = {\n            'features': features,\n            'image': image\n        }\n        print(f\"Added image '{image_id}' to database\")\n    \n    def find_similar_images(self, query_image, top_k=5):\n        \"\"\"Find most similar images in the database\"\"\"\n        query_features = self.extract_features(query_image)\n        \n        similarities = {}\n        \n        for image_id, data in self.image_database.items():\n            # Compute cosine similarity\n            similarity = torch.cosine_similarity(\n                query_features, data['features'], dim=1\n            ).item()\n            similarities[image_id] = similarity\n        \n        # Sort by similarity\n        sorted_similarities = sorted(\n            similarities.items(), key=lambda x: x[1], reverse=True\n        )\n        \n        return sorted_similarities[:top_k]\n    \n    def visualize_results(self, query_image, similar_images):\n        \"\"\"Visualize similarity search results\"\"\"\n        plt.figure(figsize=(15, 8))\n        \n        # Query image\n        plt.subplot(2, len(similar_images) + 1, 1)\n        plt.imshow(query_image)\n        plt.title(\"Query Image\")\n        plt.axis('off')\n        \n        # Similar images\n        for i, (image_id, similarity) in enumerate(similar_images):\n            plt.subplot(2, len(similar_images) + 1, i + 2)\n            plt.imshow(self.image_database[image_id]['image'])\n            plt.title(f\"{image_id}\\nSimilarity: {similarity:.3f}\")\n            plt.axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n\n# Create similarity engine\nsimilarity_engine = DINOv2SimilarityEngine()\n\n# Demo with dummy images (you would use real images)\ndef demo_similarity_engine():\n    \"\"\"Demonstrate the similarity engine\"\"\"\n    \n    # Create some dummy images with different colors\n    colors = ['red', 'blue', 'green', 'yellow', 'purple']\n    \n    for color in colors:\n        dummy_img = Image.new('RGB', (224, 224), color=color)\n        similarity_engine.add_image(f\"{color}_image\", dummy_img)\n    \n    # Query with a red image\n    query_img = Image.new('RGB', (224, 224), color='red')\n    \n    # Find similar images\n    similar = similarity_engine.find_similar_images(query_img, top_k=3)\n    \n    print(\"Most similar images:\")\n    for image_id, similarity in similar:\n        print(f\"  {image_id}: {similarity:.3f}\")\n    \n    # Visualize results\n    similarity_engine.visualize_results(query_img, similar)\n\ndemo_similarity_engine()\n\n\nAdded image 'red_image' to database\nAdded image 'blue_image' to database\nAdded image 'green_image' to database\nAdded image 'yellow_image' to database\nAdded image 'purple_image' to database\nMost similar images:\n  red_image: 1.000\n  yellow_image: 0.871\n  green_image: 0.858"
  },
  {
    "objectID": "posts/series/cv-foundations/08-modern-vision-models.html#comparing-all-approaches-the-ultimate-showdown",
    "href": "posts/series/cv-foundations/08-modern-vision-models.html#comparing-all-approaches-the-ultimate-showdown",
    "title": "Modern Vision Models: CNNs, Vision Transformers, and DINOv2",
    "section": "0.5 Comparing All Approaches: The Ultimate Showdown",
    "text": "0.5 Comparing All Approaches: The Ultimate Showdown\nLet‚Äôs compare all the approaches we‚Äôve learned:\n\n\nCode\nclass VisionModelComparison:\n    def __init__(self):\n        self.models = {\n            'ResNet-18': models.resnet18(pretrained=True),\n            'ResNet-50': models.resnet50(pretrained=True),\n            'EfficientNet-B0': efficientnet_b0(pretrained=True),\n            'ViT-Base': None,  # Would load from transformers\n            'DINOv2-Base': None  # Already loaded above\n        }\n    \n    def compare_models(self):\n        \"\"\"Compare different vision models\"\"\"\n        comparison_data = []\n        \n        for name, model in self.models.items():\n            if model is not None:\n                params = sum(p.numel() for p in model.parameters())\n                size_mb = params * 4 / (1024 * 1024)\n                \n                comparison_data.append({\n                    'Model': name,\n                    'Parameters (M)': f\"{params / 1e6:.1f}\",\n                    'Size (MB)': f\"{size_mb:.1f}\",\n                    'Year': self.get_year(name),\n                    'Type': self.get_type(name)\n                })\n        \n        return comparison_data\n    \n    def get_year(self, name):\n        year_map = {\n            'ResNet-18': 2015,\n            'ResNet-50': 2015,\n            'EfficientNet-B0': 2019,\n            'ViT-Base': 2020,\n            'DINOv2-Base': 2023\n        }\n        return year_map.get(name, 'Unknown')\n    \n    def get_type(self, name):\n        if 'ResNet' in name or 'EfficientNet' in name:\n            return 'CNN'\n        elif 'ViT' in name:\n            return 'Transformer'\n        elif 'DINOv2' in name:\n            return 'Foundation Model'\n        return 'Unknown'\n    \n    def visualize_comparison(self, data):\n        \"\"\"Visualize model comparison\"\"\"\n        import pandas as pd\n        \n        df = pd.DataFrame(data)\n        \n        plt.figure(figsize=(15, 10))\n        \n        # Parameters vs Year\n        plt.subplot(2, 2, 1)\n        for model_type in df['Type'].unique():\n            subset = df[df['Type'] == model_type]\n            plt.scatter(subset['Year'], subset['Parameters (M)'].astype(float), \n                       label=model_type, s=100)\n        \n        plt.xlabel('Year')\n        plt.ylabel('Parameters (Millions)')\n        plt.title('Model Size Evolution')\n        plt.legend()\n        plt.grid(True)\n        \n        # Model types distribution\n        plt.subplot(2, 2, 2)\n        type_counts = df['Type'].value_counts()\n        plt.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n        plt.title('Model Types Distribution')\n        \n        # Size comparison\n        plt.subplot(2, 2, 3)\n        plt.bar(df['Model'], df['Size (MB)'].astype(float))\n        plt.xticks(rotation=45)\n        plt.ylabel('Size (MB)')\n        plt.title('Model Size Comparison')\n        \n        # Timeline\n        plt.subplot(2, 2, 4)\n        timeline_data = df.sort_values('Year')\n        plt.plot(timeline_data['Year'], range(len(timeline_data)), 'o-')\n        for i, (idx, row) in enumerate(timeline_data.iterrows()):\n            plt.annotate(row['Model'], (row['Year'], i), \n                        xytext=(5, 0), textcoords='offset points')\n        plt.xlabel('Year')\n        plt.ylabel('Model Index')\n        plt.title('Vision Models Timeline')\n        plt.grid(True)\n        \n        plt.tight_layout()\n        plt.show()\n\n# Run comparison\ncomparison = VisionModelComparison()\ncomparison_data = comparison.compare_models()\n\nprint(\"Vision Models Comparison:\")\nprint(\"-\" * 80)\nfor data in comparison_data:\n    print(f\"{data['Model']:15} | {data['Year']} | {data['Type']:15} | \"\n          f\"{data['Parameters (M)']:&gt;8} M | {data['Size (MB)']:&gt;8} MB\")\n\ncomparison.visualize_comparison(comparison_data)\n\n\nVision Models Comparison:\n--------------------------------------------------------------------------------\nResNet-18       | 2015 | CNN             |     11.7 M |     44.6 MB\nResNet-50       | 2015 | CNN             |     25.6 M |     97.5 MB\nEfficientNet-B0 | 2019 | CNN             |      5.3 M |     20.2 MB"
  },
  {
    "objectID": "posts/series/cv-foundations/08-modern-vision-models.html#the-future-whats-next",
    "href": "posts/series/cv-foundations/08-modern-vision-models.html#the-future-whats-next",
    "title": "Modern Vision Models: CNNs, Vision Transformers, and DINOv2",
    "section": "0.6 The Future: What‚Äôs Next?",
    "text": "0.6 The Future: What‚Äôs Next?\nAs we look ahead, several trends are shaping the future of computer vision:\n\n0.6.1 1. Multimodal Foundation Models\n# Future: Models that understand both vision and language\n# Example: CLIP, GPT-4V, LLaVA\n\n\n0.6.2 2. Efficient Architectures\n# Trend: Smaller, faster models for mobile devices\n# Example: MobileViT, EfficientViT\n\n\n0.6.3 3. Self-Supervised Learning\n# Growing trend: Learning without labels\n# Example: MAE, SimCLR, DINOv2"
  },
  {
    "objectID": "posts/series/cv-foundations/08-modern-vision-models.html#your-challenge-build-a-modern-vision-pipeline",
    "href": "posts/series/cv-foundations/08-modern-vision-models.html#your-challenge-build-a-modern-vision-pipeline",
    "title": "Modern Vision Models: CNNs, Vision Transformers, and DINOv2",
    "section": "0.7 Your Challenge: Build a Modern Vision Pipeline",
    "text": "0.7 Your Challenge: Build a Modern Vision Pipeline\nNow it‚Äôs your turn to build a complete modern vision system:\n\n\nCode\nclass ModernVisionPipeline:\n    def __init__(self):\n        # Load multiple models for different tasks\n        self.classification_model = models.resnet50(pretrained=True)\n        self.feature_extractor = None  # DINOv2 model\n        self.similarity_engine = DINOv2SimilarityEngine()\n    \n    def classify_image(self, image):\n        \"\"\"Classify image using ResNet\"\"\"\n        # Your implementation here\n        pass\n    \n    def extract_features(self, image):\n        \"\"\"Extract features using DINOv2\"\"\"\n        # Your implementation here\n        pass\n    \n    def find_similar_images(self, query_image, database):\n        \"\"\"Find similar images using DINOv2 features\"\"\"\n        # Your implementation here\n        pass\n    \n    def analyze_image(self, image):\n        \"\"\"Complete image analysis pipeline\"\"\"\n        results = {\n            'classification': self.classify_image(image),\n            'features': self.extract_features(image),\n            'similar_images': self.find_similar_images(image, self.image_database)\n        }\n        return results\n\n# Build your pipeline!\npipeline = ModernVisionPipeline()"
  },
  {
    "objectID": "posts/series/cv-foundations/08-modern-vision-models.html#whats-coming-next",
    "href": "posts/series/cv-foundations/08-modern-vision-models.html#whats-coming-next",
    "title": "Modern Vision Models: CNNs, Vision Transformers, and DINOv2",
    "section": "0.8 What‚Äôs Coming Next?",
    "text": "0.8 What‚Äôs Coming Next?\nIn our next post, ‚ÄúYour First CV Project: Putting It All Together‚Äù, we‚Äôll:\n\nBuild a complete computer vision application\nCombine classical and modern techniques\nDeploy your model for real-world use\nCreate an interactive demo\n\nYou‚Äôve just learned about the most advanced vision models ever created‚Äînext, we‚Äôll put everything together into a real project!"
  },
  {
    "objectID": "posts/series/cv-foundations/08-modern-vision-models.html#key-takeaways",
    "href": "posts/series/cv-foundations/08-modern-vision-models.html#key-takeaways",
    "title": "Modern Vision Models: CNNs, Vision Transformers, and DINOv2",
    "section": "0.9 Key Takeaways",
    "text": "0.9 Key Takeaways\n\nCNNs dominated computer vision for a decade\nVision Transformers brought attention mechanisms to vision\nFoundation models like DINOv2 learn universal representations\nSelf-supervised learning eliminates the need for labels\nModern pipelines combine multiple approaches\nThe field evolves rapidly‚Äîstay curious and keep learning!\n\n\n\n\n\n\n\nHands-On Lab\n\n\n\nReady to experiment with cutting-edge vision models? Try the complete interactive notebook: Modern Vision Models Lab\nCompare CNNs, Vision Transformers, and DINOv2 on your own images!\n\n\n\n\n\n\n\n\nSeries Navigation\n\n\n\n\nPrevious: Why Deep Learning? When Classical Methods Hit the Wall\nNext: Your First CV Project: Putting It All Together\nSeries Home: Computer Vision Foundations\n\n\n\n\nYou‚Äôve just explored the cutting edge of computer vision! From ResNet‚Äôs skip connections to DINOv2‚Äôs self-supervised learning‚Äîyou now understand the models that power today‚Äôs AI applications. Next, we‚Äôll build something amazing with all this knowledge!"
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html",
    "href": "posts/series/cv-foundations/06-feature-magic.html",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "",
    "text": "Feature detection visualization - geometric shapes with clear keypoints for detection"
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#the-puzzle-piece-problem",
    "href": "posts/series/cv-foundations/06-feature-magic.html#the-puzzle-piece-problem",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.1 The Puzzle Piece Problem",
    "text": "0.1 The Puzzle Piece Problem\nImagine you‚Äôre doing a 1000-piece jigsaw puzzle. How do you know which pieces fit together? You look for unique features‚Äîdistinctive colors, patterns, corners, and edges that help you match pieces.\nComputer vision faces the same challenge: How do we find the same object in different photos? The answer lies in feature detection‚Äîfinding unique, recognizable points that remain consistent even when the image changes.\nToday, we‚Äôll unlock this superpower and teach computers to recognize objects across different photos, lighting conditions, and viewing angles!\n\n\n\n\n\n\nTip\n\n\n\nTry it yourself! Open this interactive Colab notebook to experiment with feature detection and matching as we build this tutorial."
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#what-are-features",
    "href": "posts/series/cv-foundations/06-feature-magic.html#what-are-features",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.2 What Are Features?",
    "text": "0.2 What Are Features?\nFeatures are distinctive points in an image that are: - Unique: Stand out from their surroundings - Repeatable: Can be found again in different images - Stable: Don‚Äôt change much with lighting or viewpoint - Informative: Carry enough information for matching\nThink of features as the ‚Äúfingerprints‚Äù of an image!\n\n\n\nArchitectural features perfect for detection\n\n\nIn this architectural image, features would be detected at: - Window corners where frames meet - Building edges and wall boundaries\n- Grid patterns of repeated windows - High-contrast regions between frames and glass"
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#your-first-feature-detector-sift",
    "href": "posts/series/cv-foundations/06-feature-magic.html#your-first-feature-detector-sift",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.3 Your First Feature Detector: SIFT",
    "text": "0.3 Your First Feature Detector: SIFT\nSIFT (Scale-Invariant Feature Transform) is like having a super-detective that can find the same clues even if they‚Äôre rotated, scaled, or slightly changed:\n\n\nCode\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\n\n# Load a sample image with clear features (architectural scene)\ndef load_image_from_url(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n\n# Load our synthetic architectural image with clear features\nimg = cv2.imread('images/architecture-features.jpg')\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Create SIFT detector\nsift = cv2.SIFT_create()\n\n# Find keypoints and descriptors\nkeypoints, descriptors = sift.detectAndCompute(gray, None)\n\nprint(f\"SIFT found {len(keypoints)} keypoints\")\nprint(f\"Each keypoint has a {descriptors.shape[1]}-dimensional descriptor\")\n\n# Draw keypoints\nimg_with_keypoints = cv2.drawKeypoints(\n    img_rgb, keypoints, None, \n    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n)\n\n# Display results\nplt.figure(figsize=(15, 8))\n\nplt.subplot(1, 2, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(img_with_keypoints)\nplt.title(f\"SIFT Keypoints: {len(keypoints)} detected\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Show some keypoint properties\nprint(\"\\nFirst 5 keypoints properties:\")\nfor i, kp in enumerate(keypoints[:5]):\n    print(f\"Keypoint {i+1}: position=({kp.pt[0]:.1f}, {kp.pt[1]:.1f}), \"\n          f\"size={kp.size:.1f}, angle={kp.angle:.1f}¬∞\")\n\n\nSIFT found 908 keypoints\nEach keypoint has a 128-dimensional descriptor\n\nFirst 5 keypoints properties:\nKeypoint 1: position=(81.9, 150.3), size=5.4, angle=101.4¬∞\nKeypoint 2: position=(81.9, 150.3), size=5.4, angle=257.9¬∞\nKeypoint 3: position=(99.7, 153.4), size=21.1, angle=174.5¬∞\nKeypoint 4: position=(99.7, 153.4), size=21.1, angle=278.9¬∞\nKeypoint 5: position=(100.9, 548.0), size=18.1, angle=175.7¬∞\n\n\n\n\n\nüéØ Amazing! Each green circle represents a detected feature. The size shows the scale, and the line shows the orientation."
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#the-faster-alternative-orb",
    "href": "posts/series/cv-foundations/06-feature-magic.html#the-faster-alternative-orb",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.4 The Faster Alternative: ORB",
    "text": "0.4 The Faster Alternative: ORB\nORB (Oriented FAST and Rotated BRIEF) is like SIFT‚Äôs speedy cousin‚Äîfaster and free to use in commercial applications:\n\n\nCode\n# Create ORB detector\norb = cv2.ORB_create()\n\n# Find keypoints and descriptors\norb_kp, orb_desc = orb.detectAndCompute(gray, None)\n\nprint(f\"ORB found {len(orb_kp)} keypoints\")\nprint(f\"Each keypoint has a {orb_desc.shape[1]*8}-bit binary descriptor\")\n\n# Draw ORB keypoints\nimg_orb = cv2.drawKeypoints(img_rgb, orb_kp, None, color=(0, 255, 0), flags=0)\n\n# Compare SIFT vs ORB\nplt.figure(figsize=(15, 10))\n\nplt.subplot(2, 2, 1)\nplt.imshow(img_with_keypoints)\nplt.title(f\"SIFT: {len(keypoints)} keypoints\")\nplt.axis('off')\n\nplt.subplot(2, 2, 2)\nplt.imshow(img_orb)\nplt.title(f\"ORB: {len(orb_kp)} keypoints\")\nplt.axis('off')\n\n# Show descriptor differences\nplt.subplot(2, 2, 3)\nplt.plot(descriptors[0])\nplt.title(\"SIFT Descriptor (128 values)\")\nplt.xlabel(\"Dimension\")\nplt.ylabel(\"Value\")\n\nplt.subplot(2, 2, 4)\nplt.plot(orb_desc[0])\nplt.title(\"ORB Descriptor (32 bytes)\")\nplt.xlabel(\"Byte\")\nplt.ylabel(\"Value\")\n\nplt.tight_layout()\nplt.show()\n\n\nORB found 500 keypoints\nEach keypoint has a 256-bit binary descriptor\n\n\n\n\n\nKey Differences: - SIFT: More accurate, 128-dimensional float descriptors - ORB: Faster, 256-bit binary descriptors, patent-free"
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#feature-matching-finding-connections",
    "href": "posts/series/cv-foundations/06-feature-magic.html#feature-matching-finding-connections",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.5 Feature Matching: Finding Connections",
    "text": "0.5 Feature Matching: Finding Connections\nNow comes the exciting part‚Äîmatching features between images to find the same objects or scenes:\n\n\nCode\n# Load a second image (different architectural pattern for matching)\nimg2 = cv2.imread('images/architecture-features-2.jpg')\nimg2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\ngray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n\n# Detect features in both images\nkp1, desc1 = sift.detectAndCompute(gray, None)\nkp2, desc2 = sift.detectAndCompute(gray2, None)\n\ndef match_features(desc1, desc2, ratio_threshold=0.7):\n    \"\"\"Match features using Lowe's ratio test\"\"\"\n    \n    # Brute Force matcher\n    bf = cv2.BFMatcher()\n    matches = bf.knnMatch(desc1, desc2, k=2)\n    \n    # Apply Lowe's ratio test to filter good matches\n    good_matches = []\n    for match_pair in matches:\n        if len(match_pair) == 2:\n            m, n = match_pair\n            if m.distance &lt; ratio_threshold * n.distance:\n                good_matches.append(m)\n    \n    return good_matches\n\n# Match SIFT features\ngood_matches = match_features(desc1, desc2)\nprint(f\"Found {len(good_matches)} good matches out of {len(kp1)} and {len(kp2)} keypoints\")\n\n# Draw matches\nmatched_img = cv2.drawMatches(\n    img_rgb, kp1,\n    img2_rgb, kp2,\n    good_matches[:50], None,  # Show top 50 matches\n    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n)\n\nplt.figure(figsize=(20, 10))\nplt.imshow(matched_img)\nplt.title(f\"Feature Matching: {len(good_matches)} matches found\")\nplt.axis('off')\nplt.show()\n\n# Show match quality statistics\ndistances = [m.distance for m in good_matches]\nprint(f\"\\nMatch quality statistics:\")\nprint(f\"Average distance: {np.mean(distances):.2f}\")\n#print(f\"Min distance: {np.min(distances):.2f}\")\n#print(f\"Max distance: {np.max(distances):.2f}\")\n\n\nFound 0 good matches out of 908 and 466 keypoints\n\nMatch quality statistics:\nAverage distance: nan\n\n\n\n\n\nüî• Incredible! Each line connects matching features between the two images. The shorter the line, the better the match!"
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#understanding-feature-descriptors",
    "href": "posts/series/cv-foundations/06-feature-magic.html#understanding-feature-descriptors",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.6 Understanding Feature Descriptors",
    "text": "0.6 Understanding Feature Descriptors\nEach keypoint comes with a descriptor‚Äîa numerical ‚Äúfingerprint‚Äù that describes the local area around that point:\n\n\nCode\ndef visualize_feature_patches(image, keypoints, num_features=6):\n    \"\"\"Visualize patches around keypoints\"\"\"\n    \n    plt.figure(figsize=(15, 10))\n    \n    for i in range(min(num_features, len(keypoints))):\n        kp = keypoints[i]\n        \n        # Extract patch around keypoint\n        x, y = int(kp.pt[0]), int(kp.pt[1])\n        size = max(int(kp.size), 20)  # Minimum patch size\n        \n        # Make sure we don't go out of bounds\n        x1 = max(0, x - size//2)\n        y1 = max(0, y - size//2)\n        x2 = min(image.shape[1], x + size//2)\n        y2 = min(image.shape[0], y + size//2)\n        \n        if x2 &gt; x1 and y2 &gt; y1:  # Valid patch\n            patch = image[y1:y2, x1:x2]\n            \n            # Plot patch\n            plt.subplot(2, 3, i + 1)\n            if len(patch.shape) == 3:\n                plt.imshow(patch)\n            else:\n                plt.imshow(patch, cmap='gray')\n            plt.title(f\"Keypoint {i+1}\\nSize: {kp.size:.1f}, Angle: {kp.angle:.1f}¬∞\")\n            plt.axis('off')\n            \n            # Draw keypoint center\n            center_x = (x - x1)\n            center_y = (y - y1)\n            plt.plot(center_x, center_y, 'r+', markersize=10, markeredgewidth=2)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualize some feature patches\nprint(\"Here's what SIFT 'sees' around each keypoint:\")\nvisualize_feature_patches(img_rgb, keypoints)\n\n\nHere's what SIFT 'sees' around each keypoint:\n\n\n\n\n\nüí° Insight: Each red cross marks the exact keypoint location. SIFT analyzes the gradient patterns in these patches to create unique descriptors."
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#real-world-application-object-recognition",
    "href": "posts/series/cv-foundations/06-feature-magic.html#real-world-application-object-recognition",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.7 Real-World Application: Object Recognition",
    "text": "0.7 Real-World Application: Object Recognition\nLet‚Äôs build a simple object recognition system using feature matching:\n\n\nCode\nclass SimpleObjectRecognizer:\n    def __init__(self, detector_type='ORB'):\n        if detector_type == 'ORB':\n            self.detector = cv2.ORB_create()\n            self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n        else:  # SIFT\n            self.detector = cv2.SIFT_create()\n            self.matcher = cv2.BFMatcher()\n        \n        self.detector_type = detector_type\n        self.reference_objects = {}\n    \n    def add_reference_object(self, name, image):\n        \"\"\"Add a reference object to recognize\"\"\"\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        else:\n            gray = image\n            \n        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\n        \n        if descriptors is not None:\n            self.reference_objects[name] = {\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'image': image\n            }\n            print(f\"Added '{name}' with {len(keypoints)} keypoints\")\n        else:\n            print(f\"No features found in '{name}'\")\n    \n    def recognize_objects(self, scene_image, min_matches=10):\n        \"\"\"Find reference objects in a scene\"\"\"\n        if len(scene_image.shape) == 3:\n            scene_gray = cv2.cvtColor(scene_image, cv2.COLOR_RGB2GRAY)\n        else:\n            scene_gray = scene_image\n            \n        scene_kp, scene_desc = self.detector.detectAndCompute(scene_gray, None)\n        \n        if scene_desc is None:\n            return []\n        \n        results = []\n        \n        for obj_name, obj_data in self.reference_objects.items():\n            # Match features\n            if self.detector_type == 'ORB':\n                matches = self.matcher.match(obj_data['descriptors'], scene_desc)\n                matches = sorted(matches, key=lambda x: x.distance)\n                good_matches = matches[:min(50, len(matches))]\n            else:  # SIFT with ratio test\n                matches = self.matcher.knnMatch(obj_data['descriptors'], scene_desc, k=2)\n                good_matches = []\n                for match_pair in matches:\n                    if len(match_pair) == 2:\n                        m, n = match_pair\n                        if m.distance &lt; 0.7 * n.distance:\n                            good_matches.append(m)\n            \n            if len(good_matches) &gt;= min_matches:\n                confidence = min_matches / len(good_matches) if len(good_matches) &gt; 0 else 0\n                results.append({\n                    'name': obj_name,\n                    'matches': len(good_matches),\n                    'confidence': confidence,\n                    'good_matches': good_matches,\n                    'scene_keypoints': scene_kp,\n                    'obj_keypoints': obj_data['keypoints']\n                })\n        \n        return sorted(results, key=lambda x: x['matches'], reverse=True)\n\n# Example usage with architectural features\nrecognizer = SimpleObjectRecognizer('SIFT')\n\n# Add reference objects (crop distinctive parts of images)\n# For demo, we'll use the same image as reference\nreference_crop = img_rgb[100:400, 200:500]  # Crop a distinctive building section\nrecognizer.add_reference_object(\"Building Section\", reference_crop)\n\n# Try to find it in a scene\nrecognition_results = recognizer.recognize_objects(img2_rgb, min_matches=5)\n\nif recognition_results:\n    result = recognition_results[0]\n    \n    plt.figure(figsize=(15, 8))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(reference_crop)\n    plt.title(\"Reference Object\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(img2_rgb)\n    plt.title(f\"Scene - Found {result['matches']} matches\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"Recognition Results:\")\n    for result in recognition_results:\n        print(f\"- Found '{result['name']}' with {result['matches']} matches\")\nelse:\n    print(\"No objects recognized in the scene\")\n\n\nAdded 'Building Section' with 295 keypoints\nNo objects recognized in the scene"
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#feature-detection-comparison",
    "href": "posts/series/cv-foundations/06-feature-magic.html#feature-detection-comparison",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.8 Feature Detection Comparison",
    "text": "0.8 Feature Detection Comparison\nLet‚Äôs compare different feature detectors to understand their strengths:\n\n\nCode\ndef compare_feature_detectors(image):\n    \"\"\"Compare different feature detection algorithms\"\"\"\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = image\n    \n    # Different detectors\n    detectors = {\n        'SIFT': cv2.SIFT_create(),\n        'ORB': cv2.ORB_create(),\n        'FAST': cv2.FastFeatureDetector_create(),\n        'BRISK': cv2.BRISK_create()\n    }\n    \n    results = {}\n    \n    plt.figure(figsize=(20, 15))\n    \n    for i, (name, detector) in enumerate(detectors.items()):\n        # Detect keypoints\n        if name in ['SIFT', 'ORB', 'BRISK']:\n            keypoints, descriptors = detector.detectAndCompute(gray, None)\n        else:  # FAST doesn't compute descriptors\n            keypoints = detector.detect(gray, None)\n            descriptors = None\n        \n        # Draw keypoints\n        img_with_kp = cv2.drawKeypoints(image, keypoints, None, color=(0, 255, 0))\n        \n        # Store results\n        results[name] = {\n            'keypoints': len(keypoints),\n            'has_descriptors': descriptors is not None,\n            'speed': 'Fast' if name in ['FAST', 'ORB'] else 'Slow'\n        }\n        \n        # Plot\n        plt.subplot(2, 2, i + 1)\n        plt.imshow(img_with_kp)\n        plt.title(f\"{name}: {len(keypoints)} keypoints\")\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print comparison table\n    print(\"Feature Detector Comparison:\")\n    print(\"-\" * 60)\n    print(f\"{'Detector':&lt;10} | {'Keypoints':&lt;10} | {'Descriptors':&lt;12} | {'Speed':&lt;8}\")\n    print(\"-\" * 60)\n    for name, data in results.items():\n        desc_info = \"‚úì\" if data['has_descriptors'] else \"‚úó\"\n        print(f\"{name:&lt;10} | {data['keypoints']:&lt;10} | {desc_info:&lt;12} | {data['speed']:&lt;8}\")\n    \n    return results\n\n# Compare detectors on architectural image\nprint(\"Comparing feature detectors on architectural scene:\")\ncomparison_results = compare_feature_detectors(img_rgb)\n\n\nComparing feature detectors on architectural scene:\nFeature Detector Comparison:\n------------------------------------------------------------\nDetector   | Keypoints  | Descriptors  | Speed   \n------------------------------------------------------------\nSIFT       | 908        | ‚úì            | Slow    \nORB        | 500        | ‚úì            | Fast    \nFAST       | 587        | ‚úó            | Fast    \nBRISK      | 2007       | ‚úì            | Slow"
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#your-challenge-build-a-panorama-stitcher",
    "href": "posts/series/cv-foundations/06-feature-magic.html#your-challenge-build-a-panorama-stitcher",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.9 Your Challenge: Build a Panorama Stitcher",
    "text": "0.9 Your Challenge: Build a Panorama Stitcher\nNow it‚Äôs your turn! Here‚Äôs a framework for a panorama stitching system:\n\n\nCode\nclass PanoramaStitcher:\n    def __init__(self):\n        self.detector = cv2.SIFT_create()\n        self.matcher = cv2.BFMatcher()\n    \n    def find_homography(self, img1, img2):\n        \"\"\"Find transformation between two images\"\"\"\n        # Convert to grayscale\n        gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY) if len(img1.shape) == 3 else img1\n        gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY) if len(img2.shape) == 3 else img2\n        \n        # Find keypoints and descriptors\n        kp1, desc1 = self.detector.detectAndCompute(gray1, None)\n        kp2, desc2 = self.detector.detectAndCompute(gray2, None)\n        \n        if desc1 is None or desc2 is None:\n            return None, None, 0\n        \n        # Match features\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n        \n        # Filter good matches using Lowe's ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance &lt; 0.7 * n.distance:\n                    good_matches.append(m)\n        \n        if len(good_matches) &lt; 10:\n            return None, None, len(good_matches)\n        \n        # Extract matched points\n        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n        \n        # Find homography using RANSAC\n        homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n        \n        return homography, mask, len(good_matches)\n    \n    def visualize_matches(self, img1, img2):\n        \"\"\"Visualize feature matches between two images\"\"\"\n        # Find features\n        gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY) if len(img1.shape) == 3 else img1\n        gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY) if len(img2.shape) == 3 else img2\n        \n        kp1, desc1 = self.detector.detectAndCompute(gray1, None)\n        kp2, desc2 = self.detector.detectAndCompute(gray2, None)\n        \n        if desc1 is None or desc2 is None:\n            print(\"Could not find features in one or both images\")\n            return\n        \n        # Match and filter\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance &lt; 0.7 * n.distance:\n                    good_matches.append(m)\n        \n        # Draw matches\n        matched_img = cv2.drawMatches(\n            img1, kp1, img2, kp2, good_matches[:30], None,\n            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n        )\n        \n        plt.figure(figsize=(20, 10))\n        plt.imshow(matched_img)\n        plt.title(f\"Feature Matches: {len(good_matches)} good matches found\")\n        plt.axis('off')\n        plt.show()\n        \n        return len(good_matches)\n\n# Test the panorama stitcher\nstitcher = PanoramaStitcher()\n\nprint(\"Analyzing feature matches between architectural images:\")\nnum_matches = stitcher.visualize_matches(img_rgb, img2_rgb)\n\nif num_matches and num_matches &gt; 10:\n    print(f\"‚úÖ Great! Found {num_matches} matches - perfect for panorama stitching!\")\n    print(\"üí° Challenge: Implement the actual image stitching using cv2.warpPerspective\")\nelse:\n    print(\"‚ùå Not enough matches for reliable stitching\")\n    print(\"üí° Try with images that have more overlap or distinctive features\")\n\n\nAnalyzing feature matches between architectural images:\n‚ùå Not enough matches for reliable stitching\nüí° Try with images that have more overlap or distinctive features"
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#the-magic-behind-feature-matching",
    "href": "posts/series/cv-foundations/06-feature-magic.html#the-magic-behind-feature-matching",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.10 The Magic Behind Feature Matching",
    "text": "0.10 The Magic Behind Feature Matching\nUnderstanding what makes feature matching work:\n\n0.10.1 1. Scale Invariance\nFeatures can be detected at different sizes:\n\n\nCode\n# Demonstrate scale invariance\ndef show_scale_invariance():\n    # Create a simple pattern\n    pattern = np.zeros((200, 200), dtype=np.uint8)\n    cv2.rectangle(pattern, (50, 50), (150, 150), 255, 2)\n    cv2.circle(pattern, (100, 100), 30, 255, 2)\n    \n    # Scale it\n    small_pattern = cv2.resize(pattern, (100, 100))\n    large_pattern = cv2.resize(pattern, (400, 400))\n    \n    # Detect features in each\n    sift = cv2.SIFT_create()\n    \n    kp_orig, _ = sift.detectAndCompute(pattern, None)\n    kp_small, _ = sift.detectAndCompute(small_pattern, None)\n    kp_large, _ = sift.detectAndCompute(large_pattern, None)\n    \n    # Visualize\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    img_small = cv2.drawKeypoints(cv2.cvtColor(small_pattern, cv2.COLOR_GRAY2RGB), \n                                  kp_small, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    plt.imshow(img_small)\n    plt.title(f\"Small (100x100): {len(kp_small)} keypoints\")\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    img_orig = cv2.drawKeypoints(cv2.cvtColor(pattern, cv2.COLOR_GRAY2RGB), \n                                 kp_orig, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    plt.imshow(img_orig)\n    plt.title(f\"Original (200x200): {len(kp_orig)} keypoints\")\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 3)\n    img_large = cv2.drawKeypoints(cv2.cvtColor(large_pattern, cv2.COLOR_GRAY2RGB), \n                                  kp_large, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    plt.imshow(img_large)\n    plt.title(f\"Large (400x400): {len(kp_large)} keypoints\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"‚ú® Scale Invariance: SIFT detects similar features at different scales!\")\n\nshow_scale_invariance()\n\n\n\n\n\n‚ú® Scale Invariance: SIFT detects similar features at different scales!\n\n\n\n\n0.10.2 2. Rotation Invariance\nFeatures remain detectable when rotated:\n\n\nCode\n# Demonstrate rotation invariance\ndef show_rotation_invariance():\n    # Use a small crop with clear features\n    if 'img_rgb' in globals():\n        test_img = img_rgb[100:300, 200:400]\n    else:\n        # Create a test pattern if main image not available\n        test_img = np.zeros((200, 200, 3), dtype=np.uint8)\n        cv2.rectangle(test_img, (50, 50), (150, 150), (255, 255, 255), 2)\n        cv2.circle(test_img, (100, 100), 30, (255, 255, 255), 2)\n    \n    # Rotate the image\n    center = (test_img.shape[1]//2, test_img.shape[0]//2)\n    rotation_matrix = cv2.getRotationMatrix2D(center, 45, 1.0)\n    rotated_img = cv2.warpAffine(test_img, rotation_matrix, (test_img.shape[1], test_img.shape[0]))\n    \n    # Detect features\n    sift = cv2.SIFT_create()\n    \n    gray_orig = cv2.cvtColor(test_img, cv2.COLOR_RGB2GRAY)\n    gray_rot = cv2.cvtColor(rotated_img, cv2.COLOR_RGB2GRAY)\n    \n    kp_orig, desc_orig = sift.detectAndCompute(gray_orig, None)\n    kp_rot, desc_rot = sift.detectAndCompute(gray_rot, None)\n    \n    # Match features\n    if desc_orig is not None and desc_rot is not None:\n        bf = cv2.BFMatcher()\n        matches = bf.knnMatch(desc_orig, desc_rot, k=2)\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance &lt; 0.7 * n.distance:\n                    good_matches.append(m)\n        \n        # Visualize\n        matched_img = cv2.drawMatches(\n            test_img, kp_orig, rotated_img, kp_rot, good_matches, None,\n            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n        )\n        \n        plt.figure(figsize=(15, 8))\n        plt.imshow(matched_img)\n        plt.title(f\"Rotation Invariance: {len(good_matches)} matches between original and 45¬∞ rotated image\")\n        plt.axis('off')\n        plt.show()\n        \n        print(f\"üîÑ Rotation Invariance: Found {len(good_matches)} matching features despite 45¬∞ rotation!\")\n    else:\n        print(\"Could not detect enough features for matching\")\n\nshow_rotation_invariance()\n\n\n\n\n\nüîÑ Rotation Invariance: Found 19 matching features despite 45¬∞ rotation!"
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#key-takeaways",
    "href": "posts/series/cv-foundations/06-feature-magic.html#key-takeaways",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.11 Key Takeaways",
    "text": "0.11 Key Takeaways\n\nFeatures are the ‚Äúfingerprints‚Äù of images - unique, repeatable points\nSIFT is more accurate but slower; ORB is faster but less precise\nFeature matching enables object recognition, panorama stitching, and 3D reconstruction\nLowe‚Äôs ratio test filters out ambiguous matches for better accuracy\nScale and rotation invariance make features robust to transformations\nReal-world applications include Google Photos, autonomous vehicles, and AR/VR"
  },
  {
    "objectID": "posts/series/cv-foundations/06-feature-magic.html#whats-coming-next",
    "href": "posts/series/cv-foundations/06-feature-magic.html#whats-coming-next",
    "title": "Feature Magic: What Makes Images Unique",
    "section": "0.12 What‚Äôs Coming Next?",
    "text": "0.12 What‚Äôs Coming Next?\nIn our next post, ‚ÄúWhy Deep Learning? When Classical Methods Hit the Wall‚Äù, we‚Äôll discover:\n\nThe limitations of classical computer vision methods\nWhy neural networks revolutionized image understanding\nYour first deep learning model for image classification\nTransfer learning - standing on the shoulders of giants\n\nYou‚Äôve mastered the art of finding and matching features‚Äînext, we‚Äôll explore how deep learning took computer vision to the next level!\n\n\n\n\n\n\nHands-On Lab\n\n\n\nReady to extract and match features in your own images? Try the complete interactive notebook: Feature Magic Lab\nBuild panoramas, recognize objects, and explore the magic of feature detection!\n\n\n\n\n\n\n\n\nSeries Navigation\n\n\n\n\nPrevious: Image Segmentation: Dividing and Conquering\nNext: Why Deep Learning? When Classical Methods Hit the Wall\nSeries Home: Computer Vision Foundations\n\n\n\n\nYou‚Äôve just learned one of the most powerful techniques in computer vision! Feature matching is used in everything from Google Photos to archaeological site reconstruction. Next, we‚Äôll see why deep learning became necessary and how it builds on these foundations."
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html",
    "href": "posts/series/cv-foundations/09-first-cv-project.html",
    "title": "Your First CV Project: Putting It All Together",
    "section": "",
    "text": "You‚Äôve learned about pixels, mastered OpenCV, detected patterns, matched features, and explored cutting-edge deep learning models. Now comes the exciting part: building a complete computer vision application that showcases everything you‚Äôve learned!\nToday, we‚Äôre going to create the ‚ÄúSmart Photo Analyzer‚Äù‚Äîan interactive web app that can: - üîç Analyze any image you upload - üè∑Ô∏è Classify objects using deep learning - üé® Apply artistic filters using classical CV - üîó Find similar images using DINOv2 features - üìä Extract detailed statistics about the image\n\n\n\nSmart Photo Analyzer Demo\n\n\nBy the end of this post, you‚Äôll have a portfolio-worthy project that demonstrates your computer vision skills!"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#the-moment-of-truth-building-something-real",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#the-moment-of-truth-building-something-real",
    "title": "Your First CV Project: Putting It All Together",
    "section": "",
    "text": "You‚Äôve learned about pixels, mastered OpenCV, detected patterns, matched features, and explored cutting-edge deep learning models. Now comes the exciting part: building a complete computer vision application that showcases everything you‚Äôve learned!\nToday, we‚Äôre going to create the ‚ÄúSmart Photo Analyzer‚Äù‚Äîan interactive web app that can: - üîç Analyze any image you upload - üè∑Ô∏è Classify objects using deep learning - üé® Apply artistic filters using classical CV - üîó Find similar images using DINOv2 features - üìä Extract detailed statistics about the image\n\n\n\nSmart Photo Analyzer Demo\n\n\nBy the end of this post, you‚Äôll have a portfolio-worthy project that demonstrates your computer vision skills!"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#project-overview-smart-photo-analyzer",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#project-overview-smart-photo-analyzer",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.2 Project Overview: Smart Photo Analyzer",
    "text": "0.2 Project Overview: Smart Photo Analyzer\n\n0.2.1 What We‚Äôre Building\nOur Smart Photo Analyzer will have these features:\n\nImage Upload & Display\nBasic Image Analysis (size, colors, etc.)\nObject Classification (using pre-trained models)\nArtistic Filters (using OpenCV)\nFeature Extraction (using DINOv2)\nSimilar Image Search\nInteractive Web Interface\n\n\n\n0.2.2 The Tech Stack\n\nBackend: Python with OpenCV, PyTorch, Transformers\nFrontend: Streamlit (for quick deployment) or Gradio\nModels: ResNet for classification, DINOv2 for features\nDeployment: Local first, then optional cloud deployment\n\n\n\n0.2.3 üé¨ Live Demo Preview\nHere‚Äôs what our finished Smart Photo Analyzer looks like in action:\n\n\n\nSmart Photo Analyzer Demo - Step by step walkthrough\n\n\nThe demo shows the complete workflow: uploading images, AI analysis, applying filters, and viewing results in an intuitive interface."
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#setting-up-the-project-structure",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#setting-up-the-project-structure",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.3 Setting Up the Project Structure",
    "text": "0.3 Setting Up the Project Structure\nLet‚Äôs start by organizing our project:\n\n\nCode\n# Project structure\n\"\"\"\nsmart_photo_analyzer/\n‚îú‚îÄ‚îÄ app.py                 # Main Streamlit app\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ image_analyzer.py  # Core analysis functions\n‚îÇ   ‚îú‚îÄ‚îÄ filters.py         # OpenCV filters\n‚îÇ   ‚îú‚îÄ‚îÄ classifier.py      # Deep learning classification\n‚îÇ   ‚îî‚îÄ‚îÄ feature_extractor.py  # DINOv2 features\n‚îú‚îÄ‚îÄ models/                # Saved models (if any)\n‚îú‚îÄ‚îÄ sample_images/         # Test images\n‚îú‚îÄ‚îÄ requirements.txt       # Dependencies\n‚îî‚îÄ‚îÄ README.md             # Project documentation\n\"\"\"\n\n# Let's create the core modules\nimport os\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport streamlit as st\n\n\nüéØ Try it yourself! Open in Colab"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#building-the-core-components",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#building-the-core-components",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.4 Building the Core Components",
    "text": "0.4 Building the Core Components\n\n0.4.1 1. Image Analyzer Class\n\n\nCode\nclass SmartImageAnalyzer:\n    def __init__(self):\n        self.setup_models()\n        self.setup_transforms()\n    \n    def setup_models(self):\n        \"\"\"Initialize all models\"\"\"\n        print(\"Loading models...\")\n        \n        # Classification model\n        self.classifier = models.resnet50(pretrained=True)\n        self.classifier.eval()\n        \n        # DINOv2 for feature extraction\n        self.dinov2_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n        self.dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        self.dinov2_model.eval()\n        \n        # ImageNet class labels\n        self.load_imagenet_labels()\n        \n        print(\"Models loaded successfully!\")\n    \n    def setup_transforms(self):\n        \"\"\"Setup image transforms\"\"\"\n        self.classification_transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n    \n    def load_imagenet_labels(self):\n        \"\"\"Load ImageNet class labels\"\"\"\n        # Simplified version - you would load the full ImageNet labels\n        self.imagenet_labels = [\n            \"tench\", \"goldfish\", \"great white shark\", \"tiger shark\",\n            \"hammerhead\", \"electric ray\", \"stingray\", \"cock\", \"hen\", \"ostrich\"\n            # ... (1000 total classes)\n        ]\n    \n    def analyze_basic_properties(self, image):\n        \"\"\"Analyze basic image properties\"\"\"\n        if isinstance(image, PIL.Image.Image):\n            image_array = np.array(image)\n        else:\n            image_array = image\n        \n        height, width = image_array.shape[:2]\n        channels = image_array.shape[2] if len(image_array.shape) == 3 else 1\n        \n        # Color analysis\n        if channels == 3:\n            # Convert to different color spaces\n            hsv = cv2.cvtColor(image_array, cv2.COLOR_RGB2HSV)\n            lab = cv2.cvtColor(image_array, cv2.COLOR_RGB2LAB)\n            \n            # Dominant colors (simplified)\n            dominant_colors = self.extract_dominant_colors(image_array)\n            \n            # Brightness and contrast\n            gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n            brightness = np.mean(gray)\n            contrast = np.std(gray)\n        else:\n            brightness = np.mean(image_array)\n            contrast = np.std(image_array)\n            dominant_colors = None\n        \n        return {\n            'dimensions': f\"{width} x {height}\",\n            'channels': channels,\n            'file_size': f\"{width * height * channels * 4 / 1024:.1f} KB\",\n            'brightness': f\"{brightness:.1f}\",\n            'contrast': f\"{contrast:.1f}\",\n            'dominant_colors': dominant_colors\n        }\n    \n    def extract_dominant_colors(self, image, k=5):\n        \"\"\"Extract dominant colors using K-means\"\"\"\n        # Reshape image to be a list of pixels\n        pixels = image.reshape(-1, 3)\n        pixels = np.float32(pixels)\n        \n        # K-means clustering\n        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n        _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n        \n        # Convert back to uint8\n        centers = np.uint8(centers)\n        \n        return centers.tolist()\n    \n    def classify_image(self, image):\n        \"\"\"Classify image using ResNet\"\"\"\n        # Preprocess image\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n        \n        input_tensor = self.classification_transform(image).unsqueeze(0)\n        \n        # Make prediction\n        with torch.no_grad():\n            outputs = self.classifier(input_tensor)\n            probabilities = torch.softmax(outputs, dim=1)\n            top5_prob, top5_indices = torch.topk(probabilities, 5)\n        \n        # Format results\n        results = []\n        for i in range(5):\n            class_idx = top5_indices[0][i].item()\n            prob = top5_prob[0][i].item()\n            class_name = self.imagenet_labels[class_idx] if class_idx &lt; len(self.imagenet_labels) else f\"Class_{class_idx}\"\n            results.append({\n                'class': class_name,\n                'confidence': prob\n            })\n        \n        return results\n    \n    def extract_features(self, image):\n        \"\"\"Extract DINOv2 features\"\"\"\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n        \n        # Process image\n        inputs = self.dinov2_processor(images=image, return_tensors=\"pt\")\n        \n        # Extract features\n        with torch.no_grad():\n            outputs = self.dinov2_model(**inputs)\n            cls_features = outputs.last_hidden_state[:, 0]  # Global features\n            patch_features = outputs.last_hidden_state[:, 1:]  # Local features\n        \n        return {\n            'global_features': cls_features.numpy(),\n            'patch_features': patch_features.numpy(),\n            'feature_dimension': cls_features.shape[1]\n        }\n\n# Initialize the analyzer\nanalyzer = SmartImageAnalyzer()\n\n\n\n\n0.4.2 2. Image Filters Module\n\n\nCode\nclass ImageFilters:\n    @staticmethod\n    def apply_blur(image, kernel_size=15):\n        \"\"\"Apply Gaussian blur\"\"\"\n        return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n    \n    @staticmethod\n    def apply_sharpen(image):\n        \"\"\"Apply sharpening filter\"\"\"\n        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n        return cv2.filter2D(image, -1, kernel)\n    \n    @staticmethod\n    def apply_edge_detection(image):\n        \"\"\"Apply Canny edge detection\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n        return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n    \n    @staticmethod\n    def apply_vintage(image):\n        \"\"\"Apply vintage filter\"\"\"\n        # Convert to float for calculations\n        img_float = image.astype(np.float32) / 255.0\n        \n        # Apply sepia effect\n        sepia_filter = np.array([\n            [0.393, 0.769, 0.189],\n            [0.349, 0.686, 0.168],\n            [0.272, 0.534, 0.131]\n        ])\n        \n        sepia_img = img_float @ sepia_filter.T\n        sepia_img = np.clip(sepia_img, 0, 1)\n        \n        # Add vignette effect\n        h, w = image.shape[:2]\n        X, Y = np.meshgrid(np.arange(w), np.arange(h))\n        center_x, center_y = w // 2, h // 2\n        \n        # Calculate distance from center\n        distance = np.sqrt((X - center_x)**2 + (Y - center_y)**2)\n        max_distance = np.sqrt(center_x**2 + center_y**2)\n        \n        # Create vignette mask\n        vignette = 1 - (distance / max_distance) * 0.5\n        vignette = np.clip(vignette, 0.3, 1)\n        \n        # Apply vignette\n        for i in range(3):\n            sepia_img[:, :, i] *= vignette\n        \n        return (sepia_img * 255).astype(np.uint8)\n    \n    @staticmethod\n    def apply_cartoon(image):\n        \"\"\"Apply cartoon effect\"\"\"\n        # Bilateral filter for smooth color regions\n        bilateral = cv2.bilateralFilter(image, 15, 200, 200)\n        \n        # Edge detection\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n                                     cv2.THRESH_BINARY, 7, 7)\n        edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n        \n        # Combine bilateral filter and edges\n        cartoon = cv2.bitwise_and(bilateral, edges)\n        \n        return cartoon\n    \n    @staticmethod\n    def apply_oil_painting(image, size=7, dynRatio=1):\n        \"\"\"Apply oil painting effect\"\"\"\n        return cv2.xphoto.oilPainting(image, size, dynRatio)\n    \n    @staticmethod\n    def get_available_filters():\n        \"\"\"Get list of available filters\"\"\"\n        return {\n            'blur': 'Gaussian Blur',\n            'sharpen': 'Sharpen',\n            'edges': 'Edge Detection',\n            'vintage': 'Vintage',\n            'cartoon': 'Cartoon',\n            'oil': 'Oil Painting'\n        }\n\nfilters = ImageFilters()\n\n\n\n\n0.4.3 3. Similarity Search Engine\n\n\nCode\nclass SimilaritySearchEngine:\n    def __init__(self, analyzer):\n        self.analyzer = analyzer\n        self.image_database = {}\n    \n    def add_image_to_database(self, image_id, image):\n        \"\"\"Add image to search database\"\"\"\n        features = self.analyzer.extract_features(image)\n        self.image_database[image_id] = {\n            'features': features['global_features'],\n            'image': image\n        }\n    \n    def find_similar_images(self, query_image, top_k=5):\n        \"\"\"Find similar images in database\"\"\"\n        query_features = self.analyzer.extract_features(query_image)['global_features']\n        \n        similarities = []\n        \n        for image_id, data in self.image_database.items():\n            # Compute cosine similarity\n            similarity = self.cosine_similarity(query_features, data['features'])\n            similarities.append((image_id, similarity, data['image']))\n        \n        # Sort by similarity\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        return similarities[:top_k]\n    \n    @staticmethod\n    def cosine_similarity(a, b):\n        \"\"\"Compute cosine similarity between two vectors\"\"\"\n        return np.dot(a.flatten(), b.flatten()) / (\n            np.linalg.norm(a.flatten()) * np.linalg.norm(b.flatten())\n        )\n\nsimilarity_engine = SimilaritySearchEngine(analyzer)"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#building-the-web-interface-with-streamlit",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#building-the-web-interface-with-streamlit",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.5 Building the Web Interface with Streamlit",
    "text": "0.5 Building the Web Interface with Streamlit\nNow let‚Äôs create an interactive web interface:\n\n\nCode\n# app.py - Main Streamlit application\nimport streamlit as st\nimport plotly.express as px\nimport plotly.graph_objects as go\n\ndef main():\n    st.set_page_config(\n        page_title=\"Smart Photo Analyzer\",\n        page_icon=\"üì∏\",\n        layout=\"wide\"\n    )\n    \n    st.title(\"üì∏ Smart Photo Analyzer\")\n    st.markdown(\"Upload an image and discover its secrets using computer vision!\")\n    \n    # Sidebar for navigation\n    st.sidebar.title(\"Navigation\")\n    page = st.sidebar.selectbox(\n        \"Choose a feature:\",\n        [\"Image Analysis\", \"Apply Filters\", \"Similar Images\", \"About\"]\n    )\n    \n    if page == \"Image Analysis\":\n        image_analysis_page()\n    elif page == \"Apply Filters\":\n        filters_page()\n    elif page == \"Similar Images\":\n        similarity_page()\n    else:\n        about_page()\n\ndef image_analysis_page():\n    st.header(\"üîç Image Analysis\")\n    \n    # File uploader\n    uploaded_file = st.file_uploader(\n        \"Choose an image file\",\n        type=['png', 'jpg', 'jpeg'],\n        help=\"Upload an image to analyze\"\n    )\n    \n    if uploaded_file is not None:\n        # Load and display image\n        image = Image.open(uploaded_file)\n        image_array = np.array(image)\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.subheader(\"Original Image\")\n            st.image(image, use_column_width=True)\n        \n        with col2:\n            st.subheader(\"Analysis Results\")\n            \n            # Basic properties\n            with st.expander(\"üìä Basic Properties\", expanded=True):\n                properties = analyzer.analyze_basic_properties(image_array)\n                \n                for key, value in properties.items():\n                    if key != 'dominant_colors':\n                        st.metric(key.replace('_', ' ').title(), value)\n                \n                # Display dominant colors\n                if properties['dominant_colors']:\n                    st.write(\"**Dominant Colors:**\")\n                    colors_html = \"\"\n                    for color in properties['dominant_colors']:\n                        hex_color = f\"#{color[0]:02x}{color[1]:02x}{color[2]:02x}\"\n                        colors_html += f'&lt;div style=\"display:inline-block; width:30px; height:30px; background-color:{hex_color}; margin:2px; border:1px solid #ccc;\"&gt;&lt;/div&gt;'\n                    st.markdown(colors_html, unsafe_allow_html=True)\n            \n            # Classification results\n            with st.expander(\"üè∑Ô∏è Object Classification\", expanded=True):\n                with st.spinner(\"Classifying image...\"):\n                    classification_results = analyzer.classify_image(image)\n                \n                for i, result in enumerate(classification_results):\n                    confidence_percent = result['confidence'] * 100\n                    st.write(f\"**{i+1}. {result['class'].title()}** - {confidence_percent:.1f}%\")\n                    st.progress(result['confidence'])\n            \n            # Feature visualization\n            with st.expander(\"üß† Deep Learning Features\"):\n                with st.spinner(\"Extracting features...\"):\n                    features = analyzer.extract_features(image)\n                \n                st.write(f\"**Feature Dimension:** {features['feature_dimension']}\")\n                \n                # Plot feature distribution\n                global_features = features['global_features'].flatten()\n                fig = px.histogram(\n                    x=global_features,\n                    title=\"Global Feature Distribution\",\n                    labels={'x': 'Feature Value', 'y': 'Frequency'}\n                )\n                st.plotly_chart(fig, use_container_width=True)\n\ndef filters_page():\n    st.header(\"üé® Apply Artistic Filters\")\n    \n    uploaded_file = st.file_uploader(\n        \"Choose an image file\",\n        type=['png', 'jpg', 'jpeg'],\n        key=\"filters_uploader\"\n    )\n    \n    if uploaded_file is not None:\n        image = Image.open(uploaded_file)\n        image_array = np.array(image)\n        \n        # Filter selection\n        available_filters = filters.get_available_filters()\n        selected_filter = st.selectbox(\n            \"Choose a filter:\",\n            options=list(available_filters.keys()),\n            format_func=lambda x: available_filters[x]\n        )\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.subheader(\"Original\")\n            st.image(image, use_column_width=True)\n        \n        with col2:\n            st.subheader(f\"With {available_filters[selected_filter]} Filter\")\n            \n            with st.spinner(\"Applying filter...\"):\n                if selected_filter == 'blur':\n                    kernel_size = st.slider(\"Blur intensity\", 1, 31, 15, step=2)\n                    filtered_image = filters.apply_blur(image_array, kernel_size)\n                elif selected_filter == 'sharpen':\n                    filtered_image = filters.apply_sharpen(image_array)\n                elif selected_filter == 'edges':\n                    filtered_image = filters.apply_edge_detection(image_array)\n                elif selected_filter == 'vintage':\n                    filtered_image = filters.apply_vintage(image_array)\n                elif selected_filter == 'cartoon':\n                    filtered_image = filters.apply_cartoon(image_array)\n                elif selected_filter == 'oil':\n                    size = st.slider(\"Brush size\", 1, 15, 7)\n                    filtered_image = filters.apply_oil_painting(image_array, size)\n            \n            st.image(filtered_image, use_column_width=True)\n            \n            # Download button\n            filtered_pil = Image.fromarray(filtered_image)\n            buf = io.BytesIO()\n            filtered_pil.save(buf, format='PNG')\n            buf.seek(0)\n            \n            st.download_button(\n                label=\"Download Filtered Image\",\n                data=buf,\n                file_name=f\"filtered_{selected_filter}.png\",\n                mime=\"image/png\"\n            )\n\ndef similarity_page():\n    st.header(\"üîó Find Similar Images\")\n    \n    # Initialize session state for image database\n    if 'database_images' not in st.session_state:\n        st.session_state.database_images = []\n    \n    # Upload images to database\n    st.subheader(\"Step 1: Build Image Database\")\n    uploaded_files = st.file_uploader(\n        \"Upload multiple images to build a database\",\n        type=['png', 'jpg', 'jpeg'],\n        accept_multiple_files=True,\n        key=\"database_uploader\"\n    )\n    \n    if uploaded_files:\n        for i, file in enumerate(uploaded_files):\n            image = Image.open(file)\n            similarity_engine.add_image_to_database(f\"image_{i}\", image)\n            st.session_state.database_images.append((f\"image_{i}\", image))\n        \n        st.success(f\"Added {len(uploaded_files)} images to database!\")\n    \n    # Query image\n    st.subheader(\"Step 2: Search for Similar Images\")\n    query_file = st.file_uploader(\n        \"Upload a query image\",\n        type=['png', 'jpg', 'jpeg'],\n        key=\"query_uploader\"\n    )\n    \n    if query_file and st.session_state.database_images:\n        query_image = Image.open(query_file)\n        \n        col1, col2 = st.columns([1, 2])\n        \n        with col1:\n            st.subheader(\"Query Image\")\n            st.image(query_image, use_column_width=True)\n        \n        with col2:\n            st.subheader(\"Similar Images\")\n            \n            with st.spinner(\"Searching for similar images...\"):\n                similar_images = similarity_engine.find_similar_images(query_image, top_k=3)\n            \n            for i, (image_id, similarity, similar_image) in enumerate(similar_images):\n                st.write(f\"**Rank {i+1}** - Similarity: {similarity:.3f}\")\n                st.image(similar_image, width=200)\n\ndef about_page():\n    st.header(\"About Smart Photo Analyzer\")\n    \n    st.markdown(\"\"\"\n    ## üéØ What This App Does\n    \n    The Smart Photo Analyzer demonstrates the power of computer vision by combining:\n    \n    - **Classical Computer Vision** (OpenCV filters and image processing)\n    - **Deep Learning** (ResNet for classification)\n    - **Foundation Models** (DINOv2 for feature extraction)\n    \n    ## üõ†Ô∏è Technologies Used\n    \n    - **OpenCV**: Image processing and filters\n    - **PyTorch**: Deep learning framework\n    - **Transformers**: HuggingFace models\n    - **Streamlit**: Web interface\n    - **DINOv2**: Self-supervised feature extraction\n    \n    ## üìö What You've Learned\n    \n    By building this app, you've mastered:\n    \n    1. **Image basics** - Understanding pixels and arrays\n    2. **OpenCV operations** - Resize, crop, filter, detect edges\n    3. **Pattern recognition** - Finding shapes and contours\n    4. **Feature matching** - Keypoints and descriptors\n    5. **Deep learning** - CNNs and classification\n    6. **Modern models** - Vision Transformers and foundation models\n    7. **Application development** - Building real-world projects\n    \n    ## üöÄ Next Steps\n    \n    - Deploy this app to the cloud (Heroku, Streamlit Cloud)\n    - Add more advanced features (object detection, segmentation)\n    - Experiment with other foundation models\n    - Build your own computer vision startup! üéâ\n    \"\"\")\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#deployment-options",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#deployment-options",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.6 Deployment Options",
    "text": "0.6 Deployment Options\n\n0.6.1 Option 1: Local Deployment\n# Create requirements.txt\npip freeze &gt; requirements.txt\n\n# Run the app\nstreamlit run app.py\n\n\n0.6.2 Option 2: Streamlit Cloud\n# Create a GitHub repository with your code\n# Connect to Streamlit Cloud\n# Deploy with one click!\n\n\n0.6.3 Option 3: Docker Deployment\n# Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8501\n\nCMD [\"streamlit\", \"run\", \"app.py\"]"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#advanced-features-to-add",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#advanced-features-to-add",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.7 Advanced Features to Add",
    "text": "0.7 Advanced Features to Add\n\n0.7.1 1. Batch Processing\n\n\nCode\ndef process_image_batch(images):\n    \"\"\"Process multiple images at once\"\"\"\n    results = []\n    \n    for image in images:\n        result = {\n            'properties': analyzer.analyze_basic_properties(image),\n            'classification': analyzer.classify_image(image),\n            'features': analyzer.extract_features(image)\n        }\n        results.append(result)\n    \n    return results\n\n\n\n\n0.7.2 2. Custom Model Training\n\n\nCode\nclass CustomClassifier:\n    def __init__(self):\n        self.model = models.resnet18(pretrained=True)\n        # Modify final layer for your specific classes\n        self.model.fc = nn.Linear(self.model.fc.in_features, num_custom_classes)\n    \n    def train(self, train_loader, val_loader, epochs=10):\n        \"\"\"Train the model on custom data\"\"\"\n        # Training loop implementation\n        pass\n    \n    def save_model(self, path):\n        \"\"\"Save trained model\"\"\"\n        torch.save(self.model.state_dict(), path)\n\n\n\n\n0.7.3 3. Real-time Video Processing\n\n\nCode\ndef process_video_stream():\n    \"\"\"Process video stream in real-time\"\"\"\n    cap = cv2.VideoCapture(0)  # Webcam\n    \n    while True:\n        ret, frame = cap.read()\n        if ret:\n            # Apply analysis to each frame\n            processed_frame = analyzer.classify_image(frame)\n            cv2.imshow('Smart Video Analyzer', processed_frame)\n        \n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \n    cap.release()\n    cv2.destroyAllWindows()"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#testing-your-application",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#testing-your-application",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.8 Testing Your Application",
    "text": "0.8 Testing Your Application\n\n\nCode\nimport unittest\n\nclass TestSmartPhotoAnalyzer(unittest.TestCase):\n    def setUp(self):\n        self.analyzer = SmartImageAnalyzer()\n        self.test_image = Image.new('RGB', (224, 224), color='red')\n    \n    def test_basic_analysis(self):\n        \"\"\"Test basic image analysis\"\"\"\n        properties = self.analyzer.analyze_basic_properties(self.test_image)\n        self.assertIn('dimensions', properties)\n        self.assertIn('channels', properties)\n    \n    def test_classification(self):\n        \"\"\"Test image classification\"\"\"\n        results = self.analyzer.classify_image(self.test_image)\n        self.assertEqual(len(results), 5)  # Top 5 predictions\n        self.assertTrue(all('class' in r and 'confidence' in r for r in results))\n    \n    def test_feature_extraction(self):\n        \"\"\"Test feature extraction\"\"\"\n        features = self.analyzer.extract_features(self.test_image)\n        self.assertIn('global_features', features)\n        self.assertIn('feature_dimension', features)\n\nif __name__ == '__main__':\n    unittest.main()"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#performance-optimization",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#performance-optimization",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.9 Performance Optimization",
    "text": "0.9 Performance Optimization\n\n0.9.1 1. Model Caching\n\n\nCode\n@st.cache_resource\ndef load_models():\n    \"\"\"Cache models to avoid reloading\"\"\"\n    return SmartImageAnalyzer()\n\n# Use cached models\nanalyzer = load_models()\n\n\n\n\n0.9.2 2. Image Preprocessing Optimization\n\n\nCode\ndef optimize_image_size(image, max_size=1024):\n    \"\"\"Resize large images for faster processing\"\"\"\n    width, height = image.size\n    \n    if max(width, height) &gt; max_size:\n        ratio = max_size / max(width, height)\n        new_width = int(width * ratio)\n        new_height = int(height * ratio)\n        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n    \n    return image"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#your-portfolio-showcase",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#your-portfolio-showcase",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.10 Your Portfolio Showcase",
    "text": "0.10 Your Portfolio Showcase\n\n0.10.1 Creating a Professional README\n# Smart Photo Analyzer\n\nA comprehensive computer vision application that analyzes images using classical and modern techniques.\n\n## Features\n\n- üîç **Image Analysis**: Extract detailed properties and statistics\n- üè∑Ô∏è **Object Classification**: Identify objects using deep learning\n- üé® **Artistic Filters**: Apply creative effects using OpenCV\n- üîó **Similarity Search**: Find similar images using DINOv2 features\n\n## Demo\n\n![Smart Photo Analyzer Demo](images/smart-photo-analyzer-demo.gif)\n\n## Installation\n\n```bash\npip install -r requirements.txt\nstreamlit run app.py"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#technologies",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#technologies",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.11 Technologies",
    "text": "0.11 Technologies\n\nOpenCV for image processing\nPyTorch for deep learning\nDINOv2 for feature extraction\nStreamlit for web interface"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#what-i-learned",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#what-i-learned",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.12 What I Learned",
    "text": "0.12 What I Learned\nThis project demonstrates my understanding of: - Classical computer vision techniques - Deep learning for image classification - Foundation models and feature extraction - Full-stack application development"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#whats-coming-next",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#whats-coming-next",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.13 What‚Äôs Coming Next?",
    "text": "0.13 What‚Äôs Coming Next?\nIn our final post, ‚ÄúWhere to Go Next: Your Computer Vision Journey‚Äù, we‚Äôll explore:\n\nAdvanced topics to study next\nCareer paths in computer vision\nOpen source projects to contribute to\nResources for continued learning\nBuilding your portfolio and landing your first CV job\n\nYou‚Äôve just built a complete computer vision application‚Äîcongratulations! üéâ"
  },
  {
    "objectID": "posts/series/cv-foundations/09-first-cv-project.html#key-takeaways",
    "href": "posts/series/cv-foundations/09-first-cv-project.html#key-takeaways",
    "title": "Your First CV Project: Putting It All Together",
    "section": "0.14 Key Takeaways",
    "text": "0.14 Key Takeaways\n\nIntegration is key: Combining classical and modern techniques\nUser experience matters: Good interfaces make CV accessible\nTesting is crucial: Ensure your application works reliably\nPerformance optimization: Make your app fast and responsive\nPortfolio value: This project showcases your CV skills\nReal-world application: You‚Äôve built something genuinely useful!\n\n\n\n\n\n\n\nComplete Project\n\n\n\nReady to build your own Smart Photo Analyzer? Get the complete code and deploy your own version: Smart Photo Analyzer - Complete Project\nMake it your own and add it to your portfolio!\n\n\n\n\n\n\n\n\nSeries Navigation\n\n\n\n\nPrevious: Modern Vision Models: CNNs, Vision Transformers, and DINOv2\nNext: Where to Go Next: Your Computer Vision Journey\nSeries Home: Computer Vision Foundations\n\n\n\n\nYou‚Äôve just built a complete computer vision application that showcases everything you‚Äôve learned! From pixels to deep learning to deployment‚Äîyou‚Äôre now ready to tackle real-world computer vision challenges. In our final post, we‚Äôll chart your path forward in this exciting field."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "",
    "text": "Robot eye representing computer vision"
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#the-magic-moment-when-machines-learn-to-see",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#the-magic-moment-when-machines-learn-to-see",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.1 The Magic Moment: When Machines Learn to See",
    "text": "0.1 The Magic Moment: When Machines Learn to See\nImagine you‚Äôre teaching a friend to recognize your cat in photos. You‚Äôd point out the whiskers, the pointed ears, maybe that distinctive patch of white fur. Now imagine doing the same thing with a computer‚Äîexcept the computer doesn‚Äôt understand ‚Äúwhiskers‚Äù or ‚Äúcute.‚Äù It only sees numbers.\nThat‚Äôs exactly what computer vision is: teaching machines to understand and interpret visual information the way humans do.\n\n\n\n\n\n\nTip\n\n\n\nTry it yourself! Open this interactive Colab notebook to see computer vision in action as we build this tutorial series."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#the-story-that-started-it-all",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#the-story-that-started-it-all",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.2 The Story That Started It All",
    "text": "0.2 The Story That Started It All\nPicture this: You‚Äôre showing your grandmother a photo on your phone. Within milliseconds, she says, ‚ÄúOh, that‚Äôs your cat Whiskers sitting on the windowsill!‚Äù But if you asked a computer the same question just 15 years ago, it would have been completely stumped.\nToday? Your phone can not only recognize your cat but also tell you the breed, suggest similar photos, and even create a cute slideshow. How did we get here?\nThat‚Äôs the magic of computer vision ‚Äì teaching machines to see and understand the world like we do."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#what-is-computer-vision-really",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#what-is-computer-vision-really",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.3 What Is Computer Vision, Really?",
    "text": "0.3 What Is Computer Vision, Really?\nComputer vision is like giving a computer a pair of eyes and teaching it to understand what it sees. But here‚Äôs the fascinating part: while you can instantly recognize a cat, a computer sees this:\n\n\n\nA dog is trying to eat chocolate\n\n\nIn less than a second, your brain processed: - There‚Äôs a dog - He is trying to eat a chocolate\nBut to a computer, this image is just a grid of numbers. Our job is to bridge that gap."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#your-first-taste-what-does-a-computer-see",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#your-first-taste-what-does-a-computer-see",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.4 Your First Taste: What Does a Computer See?",
    "text": "0.4 Your First Taste: What Does a Computer See?\nLet‚Äôs peek behind the curtain. Here‚Äôs what an image looks like to a computer:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a simple 5x5 \"image\" (just numbers!)\nsimple_image = np.array([\n    [255, 255, 255, 255, 255],\n    [255,   0,   0,   0, 255],\n    [255,   0, 128,   0, 255],\n    [255,   0,   0,   0, 255],\n    [255, 255, 255, 255, 255]\n])\n\n# Show it as an image\nplt.figure(figsize=(8, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(simple_image, cmap='gray')\nplt.title(\"What You See\")\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(simple_image, cmap='gray')\nplt.title(\"What the Computer Sees\")\nfor i in range(5):\n    for j in range(5):\n        plt.text(j, i, str(simple_image[i, j]), \n                ha='center', va='center', color='red', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"The computer sees this as a 5x5 grid of numbers:\")\nprint(simple_image)\n\n\n\n\n\nThe computer sees this as a 5x5 grid of numbers:\n[[255 255 255 255 255]\n [255   0   0   0 255]\n [255   0 128   0 255]\n [255   0   0   0 255]\n [255 255 255 255 255]]\n\n\nMind = Blown! ü§Ø The computer doesn‚Äôt see a ‚Äúsmiley face‚Äù‚Äîit sees a pattern of numbers. But with the right algorithms, those numbers become meaningful."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#the-aha-moment-why-this-matters",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#the-aha-moment-why-this-matters",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.5 The ‚ÄúAha!‚Äù Moment: Why This Matters",
    "text": "0.5 The ‚ÄúAha!‚Äù Moment: Why This Matters\nComputer vision isn‚Äôt just academic curiosity. It‚Äôs changing the world right now:\n\n0.5.1 üöó Self-Driving Cars\n\n\n\nSelf-driving car\n\n\nTesla‚Äôs cars process 36 images per second from 8 cameras to navigate roads, recognize traffic signs, and avoid pedestrians.\n\n\n0.5.2 üè• Medical Diagnosis\n\n\n\nMedical scan\n\n\nComputer vision can detect cancer in medical scans with higher accuracy than human doctors. Google‚Äôs DeepMind can diagnose over 50 eye diseases just from retinal photos.\n\n\n0.5.3 üì± Your Daily Life\n\nUnlocking your phone with Face ID\nGoogle Photos organizing your pictures\nInstagram filters that put bunny ears on your head\n\n\n\n0.5.4 üåæ Agriculture Revolution\n\n\n\nAgricultural drone monitoring crops\n\n\nDrones equipped with computer vision can identify diseased crops, count plants, and optimize irrigation‚Äîincreasing yields by 20-30%."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#the-beautiful-challenge",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#the-beautiful-challenge",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.6 The Beautiful Challenge",
    "text": "0.6 The Beautiful Challenge\nHere‚Äôs what makes computer vision fascinating: It‚Äôs both incredibly simple and mind-bogglingly complex.\nSimple because the goal is clear: help computers understand images.\nComplex because human vision is the result of millions of years of evolution, and we‚Äôre trying to replicate it with math and code."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#the-journey-were-taking-together",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#the-journey-were-taking-together",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.7 The Journey We‚Äôre Taking Together",
    "text": "0.7 The Journey We‚Äôre Taking Together\nIn this series, we‚Äôll follow Jeremy Howard‚Äôs fastai approach: start with working code, build intuition, then dive deeper.\n\n\n\nLearning path visualization\n\n\n\n0.7.1 Phase 1: Understanding the Basics (Posts 1-2)\n\nHow images are stored as numbers\nBasic operations (resize, crop, rotate)\nYour first ‚ÄúHello, Computer Vision!‚Äù program\n\n\n\n0.7.2 Phase 2: Classical Techniques (Posts 3-5)\n\nFinding edges and shapes\nDetecting objects the ‚Äúold school‚Äù way\nFeature extraction and matching\n\n\n\n0.7.3 Phase 3: Deep Learning Revolution (Posts 6-7)\n\nWhy neural networks changed everything\nUsing pre-trained models (the smart shortcut)\nModern foundation models like DINOv2\n\n\n\n0.7.4 Phase 4: Building Your Future (Posts 8-9)\n\nYour complete computer vision project\nCareer paths and next steps"
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#the-pareto-principle-in-action",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#the-pareto-principle-in-action",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.8 The Pareto Principle in Action",
    "text": "0.8 The Pareto Principle in Action\nHere‚Äôs our secret weapon: The 80/20 rule.\n\n80% of computer vision tasks use just 20% of the available techniques\n80% of your results will come from 20% of your effort\n80% of real-world applications use pre-trained models (not custom training)\n\nWe‚Äôll focus on that crucial 20% that gives you maximum impact."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#what-youll-build",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#what-youll-build",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.9 What You‚Äôll Build",
    "text": "0.9 What You‚Äôll Build\nBy the end of this series, you‚Äôll have: - ‚úÖ A complete understanding of how images work as data - ‚úÖ Hands-on experience with OpenCV and modern deep learning - ‚úÖ A working web app that combines classical and modern CV techniques - ‚úÖ The foundation to build your own computer vision projects"
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#the-tools-of-the-trade",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#the-tools-of-the-trade",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.10 The Tools of the Trade",
    "text": "0.10 The Tools of the Trade\nWe‚Äôll use the same tools that professionals use:\n\nPython: The language of choice for computer vision\nOpenCV: The Swiss Army knife of image processing\nPyTorch: For deep learning magic\nGoogle Colab: Free GPU power for everyone\nHuggingFace: Pre-trained models made easy\n\nDon‚Äôt worry if these names sound scary ‚Äì we‚Äôll introduce each one gently."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#a-personal-promise",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#a-personal-promise",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.11 A Personal Promise",
    "text": "0.11 A Personal Promise\nI promise you this: Every concept will be explained with simple analogies.\n\nNeural networks? Think of them as very picky art critics.\nConvolutions? Imagine sliding a magnifying glass over a photo.\nTransfer learning? Like learning to drive a truck after you know how to drive a car."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#your-first-assignment-optional-but-fun",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#your-first-assignment-optional-but-fun",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.12 Your First Assignment (Optional but Fun!)",
    "text": "0.12 Your First Assignment (Optional but Fun!)\nBefore we dive into code, try this thought experiment:\n\nTake a photo with your phone\nLook at it for 5 seconds\nWrite down everything you can see\nNow imagine explaining each item to someone who has never seen the world\n\nThat list you just made? That‚Äôs what we‚Äôre teaching computers to do."
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#key-takeaways",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#key-takeaways",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.13 Key Takeaways",
    "text": "0.13 Key Takeaways\n\nComputer vision = teaching machines to understand images\nImages are just arrays of numbers (but meaningful ones!)\nStart with working code, build intuition, then understand theory\nReal applications are already changing the world\nYou can build amazing things with surprisingly little code"
  },
  {
    "objectID": "posts/series/cv-foundations/01-why-computer-vision.html#whats-next",
    "href": "posts/series/cv-foundations/01-why-computer-vision.html#whats-next",
    "title": "Why Computer Vision? Teaching a Robot to See",
    "section": "0.14 What‚Äôs Next?",
    "text": "0.14 What‚Äôs Next?\nIn our next post, ‚ÄúImages as Data: The Digital Lego Blocks‚Äù, we‚Äôll:\n\nLoad our first image with Python\nExplore how colors become numbers\n\nPlay with pixels like digital Lego blocks\nCreate our first simple image filter\n\n\n\n\n\n\n\nNote\n\n\n\nReady to dive deeper? Click here for the interactive Colab notebook where you can run all the code examples and experiment with your own images!\n\n\n\nNext up: Images as Data: The Digital Lego Blocks ‚Üí\nSeries Navigation: ‚Üê Back to CV Foundations Overview"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html",
    "href": "posts/series/cv-foundations/02-images-as-data.html",
    "title": "Images as Data: How Computers See the World",
    "section": "",
    "text": "Remember when you first learned that music could be stored as numbers? Or that your favorite movie is just a sequence of 1s and 0s? Well, prepare for another ‚Äúmind-blown‚Äù moment: every image you‚Äôve ever seen is just a big table of numbers.\nThat sunset photo from your vacation? Numbers. Your profile picture? Numbers. The Mona Lisa? You guessed it‚Äînumbers!\nLet‚Äôs dive into this digital rabbit hole and discover how computers really see the world."
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#the-great-revelation-everything-is-numbers",
    "href": "posts/series/cv-foundations/02-images-as-data.html#the-great-revelation-everything-is-numbers",
    "title": "Images as Data: How Computers See the World",
    "section": "",
    "text": "Remember when you first learned that music could be stored as numbers? Or that your favorite movie is just a sequence of 1s and 0s? Well, prepare for another ‚Äúmind-blown‚Äù moment: every image you‚Äôve ever seen is just a big table of numbers.\nThat sunset photo from your vacation? Numbers. Your profile picture? Numbers. The Mona Lisa? You guessed it‚Äînumbers!\nLet‚Äôs dive into this digital rabbit hole and discover how computers really see the world."
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#the-pixel-story-meet-the-building-blocks",
    "href": "posts/series/cv-foundations/02-images-as-data.html#the-pixel-story-meet-the-building-blocks",
    "title": "Images as Data: How Computers See the World",
    "section": "0.2 The Pixel Story: Meet the Building Blocks",
    "text": "0.2 The Pixel Story: Meet the Building Blocks\nImagine you‚Äôre creating a mosaic with tiny colored tiles. Each tile is a single color, and when you step back, all these tiles together form a beautiful picture. In the digital world, these tiles are called pixels (short for ‚Äúpicture elements‚Äù).\n\n0.2.1 Your First Digital Image\nLet‚Äôs create the world‚Äôs tiniest image‚Äîjust 3√ó3 pixels:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a tiny 3x3 grayscale image\ntiny_image = np.array([\n    [0,   128, 255],\n    [64,  192, 128], \n    [255, 64,  0  ]\n])\n\nprint(\"Our tiny image as numbers:\")\nprint(tiny_image)\nprint(f\"\\nImage shape: {tiny_image.shape}\")\nprint(f\"Image data type: {tiny_image.dtype}\")\n\n\nOur tiny image as numbers:\n[[  0 128 255]\n [ 64 192 128]\n [255  64   0]]\n\nImage shape: (3, 3)\nImage data type: int64\n\n\nNow let‚Äôs see what this looks like as an actual image:\n\n\nCode\nplt.figure(figsize=(8, 4))\n\n# Show the numbers\nplt.subplot(1, 2, 1)\nplt.imshow(tiny_image, cmap='gray', interpolation='nearest')\nplt.title(\"What You See\")\nfor i in range(3):\n    for j in range(3):\n        plt.text(j, i, str(tiny_image[i, j]), \n                ha='center', va='center', color='red', fontweight='bold')\nplt.colorbar()\n\n# Show as image\nplt.subplot(1, 2, 2)\nplt.imshow(tiny_image, cmap='gray', interpolation='nearest')\nplt.title(\"Just the Image\")\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nüéØ Try this! Open in Colab"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#the-color-mystery-rgb-revealed",
    "href": "posts/series/cv-foundations/02-images-as-data.html#the-color-mystery-rgb-revealed",
    "title": "Images as Data: How Computers See the World",
    "section": "0.3 The Color Mystery: RGB Revealed",
    "text": "0.3 The Color Mystery: RGB Revealed\nBut wait‚Äîwhat about color images? Here‚Äôs where it gets interesting. Color images are like having three grayscale images stacked on top of each other:\n\nRed layer: How much red is in each pixel\nGreen layer: How much green is in each pixel\n\nBlue layer: How much blue is in each pixel\n\nThink of it like making a sandwich with three transparent colored sheets!\n\n\nCode\n# Create a simple 2x2 color image\ncolor_image = np.array([\n    [[255, 0, 0], [0, 255, 0]],      # Red pixel, Green pixel\n    [[0, 0, 255], [255, 255, 0]]     # Blue pixel, Yellow pixel\n])\n\nprint(\"Color image shape:\", color_image.shape)\nprint(\"This means: 2 rows, 2 columns, 3 color channels\")\n\nplt.figure(figsize=(12, 4))\n\n# Show each color channel\nchannels = ['Red', 'Green', 'Blue']\ncolors = ['Reds', 'Greens', 'Blues']\n\nfor i in range(3):\n    plt.subplot(1, 4, i+1)\n    plt.imshow(color_image[:, :, i], cmap=colors[i])\n    plt.title(f\"{channels[i]} Channel\")\n    plt.axis('off')\n\n# Show the combined image\nplt.subplot(1, 4, 4)\nplt.imshow(color_image)\nplt.title(\"Combined RGB\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nColor image shape: (2, 2, 3)\nThis means: 2 rows, 2 columns, 3 color channels"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#loading-your-first-real-image",
    "href": "posts/series/cv-foundations/02-images-as-data.html#loading-your-first-real-image",
    "title": "Images as Data: How Computers See the World",
    "section": "0.4 Loading Your First Real Image",
    "text": "0.4 Loading Your First Real Image\nEnough with toy examples! Let‚Äôs load a real photograph and explore it:\n\n\nCode\nimport cv2\n\n# Load an image (you can upload your own to Colab!)\nimg = cv2.imread('images/image.jpg')\n\n# OpenCV loads images in BGR format, let's convert to RGB\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nprint(f\"Image shape: {img_rgb.shape}\")\nprint(f\"Image size: {img_rgb.size} pixels\")\nprint(f\"Data type: {img_rgb.dtype}\")\n\n# Display the image\nplt.figure(figsize=(10, 6))\nplt.imshow(img_rgb)\nplt.title(\"Your First Real Image!\")\nplt.axis('off')\nplt.show()\n\n# Let's examine a small patch\npatch = img_rgb[100:110, 100:110]  # 10x10 pixel patch\nprint(f\"\\nA small 10x10 patch from the image:\")\nprint(f\"Shape: {patch.shape}\")\nprint(f\"First pixel RGB values: {patch[0, 0]}\")\n\n\nImage shape: (627, 1200, 3)\nImage size: 2257200 pixels\nData type: uint8\n\nA small 10x10 patch from the image:\nShape: (10, 10, 3)\nFirst pixel RGB values: [ 2 31 37]"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#the-magic-of-image-operations",
    "href": "posts/series/cv-foundations/02-images-as-data.html#the-magic-of-image-operations",
    "title": "Images as Data: How Computers See the World",
    "section": "0.5 The Magic of Image Operations",
    "text": "0.5 The Magic of Image Operations\nNow that we understand images as numbers, we can do mathematical operations on them! This is where the fun begins.\n\n0.5.1 Making Images Brighter or Darker\n\n\nCode\n# Load and prepare image\nimg = cv2.imread('images/image.jpg')\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Make it brighter (add to all pixels)\nbrighter = np.clip(img_rgb + 50, 0, 255).astype(np.uint8)\n\n# Make it darker (subtract from all pixels)  \ndarker = np.clip(img_rgb - 50, 0, 255).astype(np.uint8)\n\n# Show the results\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(darker)\nplt.title(\"Darker (-50)\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(img_rgb)\nplt.title(\"Original\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(brighter)\nplt.title(\"Brighter (+50)\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n0.5.2 Playing with Individual Color Channels\n\n\nCode\n# Create different color effects\nred_only = img_rgb.copy()\nred_only[:, :, 1] = 0  # Remove green\nred_only[:, :, 2] = 0  # Remove blue\n\ngreen_only = img_rgb.copy()\ngreen_only[:, :, 0] = 0  # Remove red\ngreen_only[:, :, 2] = 0  # Remove blue\n\nblue_only = img_rgb.copy()\nblue_only[:, :, 0] = 0  # Remove red\nblue_only[:, :, 1] = 0  # Remove green\n\n# Display the results\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(red_only)\nplt.title(\"Red Channel Only\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(green_only)\nplt.title(\"Green Channel Only\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(blue_only)\nplt.title(\"Blue Channel Only\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#the-grayscale-transformation",
    "href": "posts/series/cv-foundations/02-images-as-data.html#the-grayscale-transformation",
    "title": "Images as Data: How Computers See the World",
    "section": "0.6 The Grayscale Transformation",
    "text": "0.6 The Grayscale Transformation\nConverting to grayscale is one of the most common operations in computer vision. But it‚Äôs not just about averaging the RGB values‚Äîthere‚Äôs a secret formula!\n\n\nCode\n# Method 1: Simple average (not the best)\ngray_simple = np.mean(img_rgb, axis=2).astype(np.uint8)\n\n# Method 2: Weighted average (the right way!)\n# Human eyes are more sensitive to green, less to blue\ngray_weighted = (0.299 * img_rgb[:, :, 0] + \n                0.587 * img_rgb[:, :, 1] + \n                0.114 * img_rgb[:, :, 2]).astype(np.uint8)\n\n# Method 3: Using OpenCV (does the weighted average for us)\ngray_opencv = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(gray_simple, cmap='gray')\nplt.title(\"Simple Average\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(gray_weighted, cmap='gray')\nplt.title(\"Weighted Average\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(gray_opencv, cmap='gray')\nplt.title(\"OpenCV Method\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#fun-with-image-arithmetic",
    "href": "posts/series/cv-foundations/02-images-as-data.html#fun-with-image-arithmetic",
    "title": "Images as Data: How Computers See the World",
    "section": "0.7 Fun with Image Arithmetic",
    "text": "0.7 Fun with Image Arithmetic\nSince images are just arrays, we can do all sorts of mathematical operations:\n\n\nCode\n# Create a simple pattern\nrows, cols = img_rgb.shape[:2]\nx_gradient = np.linspace(0, 255, cols).astype(np.uint8)\ny_gradient = np.linspace(0, 255, rows).astype(np.uint8)\n\n# Create gradient patterns\nhorizontal_gradient = np.tile(x_gradient, (rows, 1))\nvertical_gradient = np.tile(y_gradient.reshape(-1, 1), (1, cols))\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(horizontal_gradient, cmap='gray')\nplt.title(\"Horizontal Gradient\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(vertical_gradient, cmap='gray')\nplt.title(\"Vertical Gradient\")\nplt.axis('off')\n\n# Combine gradients\ncombined = (horizontal_gradient + vertical_gradient) // 2\nplt.subplot(1, 3, 3)\nplt.imshow(combined, cmap='gray')\nplt.title(\"Combined Gradients\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#your-first-image-filter-the-blur-effect",
    "href": "posts/series/cv-foundations/02-images-as-data.html#your-first-image-filter-the-blur-effect",
    "title": "Images as Data: How Computers See the World",
    "section": "0.8 Your First Image Filter: The Blur Effect",
    "text": "0.8 Your First Image Filter: The Blur Effect\nLet‚Äôs create a simple blur effect by averaging neighboring pixels:\n\n\nCode\ndef simple_blur(image, kernel_size=5):\n    \"\"\"Apply a simple blur by averaging neighboring pixels\"\"\"\n    blurred = np.zeros_like(image)\n    offset = kernel_size // 2\n    \n    for i in range(offset, image.shape[0] - offset):\n        for j in range(offset, image.shape[1] - offset):\n            # Average the pixels in the kernel area\n            neighborhood = image[i-offset:i+offset+1, j-offset:j+offset+1]\n            blurred[i, j] = np.mean(neighborhood)\n    \n    return blurred\n\n# Apply our blur to a grayscale image\ngray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nblurred_img = simple_blur(gray_img)\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.imshow(gray_img, cmap='gray')\nplt.title(\"Original\")\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(blurred_img, cmap='gray')\nplt.title(\"Our Custom Blur\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#the-big-picture-what-weve-learned",
    "href": "posts/series/cv-foundations/02-images-as-data.html#the-big-picture-what-weve-learned",
    "title": "Images as Data: How Computers See the World",
    "section": "0.9 The Big Picture: What We‚Äôve Learned",
    "text": "0.9 The Big Picture: What We‚Äôve Learned\nüéâ Congratulations! You‚Äôve just unlocked one of the most important concepts in computer vision. Here‚Äôs what you now know:\n\nImages are arrays of numbers (0-255 for each pixel)\nGrayscale images are 2D arrays (height √ó width)\nColor images are 3D arrays (height √ó width √ó 3 channels)\nImage processing is just mathematical operations on these arrays\nYou can create effects by manipulating pixel values"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#practical-exercises",
    "href": "posts/series/cv-foundations/02-images-as-data.html#practical-exercises",
    "title": "Images as Data: How Computers See the World",
    "section": "0.10 Practical Exercises",
    "text": "0.10 Practical Exercises\nBefore moving to the next post, try these fun experiments:\n\n0.10.1 üéØ Exercise 1: Create Your Own Pattern\n\n\nCode\n# Create a checkerboard pattern\nsize = 200\ncheckerboard = np.zeros((size, size))\nsquare_size = 25\n\nfor i in range(0, size, square_size):\n    for j in range(0, size, square_size):\n        if (i // square_size + j // square_size) % 2 == 0:\n            checkerboard[i:i+square_size, j:j+square_size] = 255\n\nplt.imshow(checkerboard, cmap='gray')\nplt.title(\"Checkerboard Pattern\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n0.10.2 üéØ Exercise 2: Image Negative\n\n\nCode\n# Create a negative effect (like old film negatives)\nnegative = 255 - img_rgb\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original\")\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(negative)\nplt.title(\"Negative\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#whats-coming-next",
    "href": "posts/series/cv-foundations/02-images-as-data.html#whats-coming-next",
    "title": "Images as Data: How Computers See the World",
    "section": "0.11 What‚Äôs Coming Next?",
    "text": "0.11 What‚Äôs Coming Next?\nIn our next adventure, ‚ÄúOpenCV Essentials: Your First Computer Vision Toolkit‚Äù, we‚Äôll:\n\nMaster the essential OpenCV functions\nLearn to resize, crop, and rotate images like a pro\nBuild our first interactive image viewer\nCreate a simple photo editor\n\nThe best part? Everything we‚Äôve learned about images as numbers will make OpenCV operations crystal clear!"
  },
  {
    "objectID": "posts/series/cv-foundations/02-images-as-data.html#key-takeaways",
    "href": "posts/series/cv-foundations/02-images-as-data.html#key-takeaways",
    "title": "Images as Data: How Computers See the World",
    "section": "0.12 Key Takeaways",
    "text": "0.12 Key Takeaways\n\nImages = Numbers: Every pixel is just a number (or three numbers for color)\nShape Matters: Always check image.shape to understand your data\nMath is Magic: Simple arithmetic creates powerful visual effects\nStart Simple: Complex algorithms are built from these basic operations\n\n\n\n\n\n\n\nHands-On Practice\n\n\n\nReady to experiment? Try the full interactive notebook: Images as Data - Interactive Lab\nUpload your own photos and see them transform into numbers!\n\n\n\n\n\n\n\n\nSeries Navigation\n\n\n\n\nPrevious: Why Computer Vision?\nNext: OpenCV Essentials: Your First Computer Vision Toolkit\n\nSeries Home: Computer Vision Foundations\n\n\n\n\nRemember: You‚Äôve just learned to see the world the way computers do. That‚Äôs no small feat! In the next post, we‚Äôll use this knowledge to build amazing things with OpenCV."
  },
  {
    "objectID": "posts/series/anomaly-detection/index.html",
    "href": "posts/series/anomaly-detection/index.html",
    "title": "Anomaly Detection Series",
    "section": "",
    "text": "This series will guide you through the wild and wonderful world of anomaly detection, with a focus on intuition, practical examples, and a healthy dose of humor. Whether you‚Äôre a data scientist, a machine learning enthusiast, or just someone who likes finding the oddballs in a crowd, you‚Äôre in the right place.\n\n\n\nFinding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)\n\nStay tuned for more posts, including hands-on tutorials and advanced techniques!"
  },
  {
    "objectID": "posts/series/anomaly-detection/index.html#posts-in-this-series",
    "href": "posts/series/anomaly-detection/index.html#posts-in-this-series",
    "title": "Anomaly Detection Series",
    "section": "",
    "text": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)\n\nStay tuned for more posts, including hands-on tutorials and advanced techniques!"
  },
  {
    "objectID": "posts/series/vlm.html",
    "href": "posts/series/vlm.html",
    "title": "VLM Series",
    "section": "",
    "text": "This series takes a fastai-inspired, top-down approach to Vision-Language Models (VLMs). We‚Äôll start with real-world applications and working code, then dive deeper into the concepts behind these powerful models. The focus is on open-source VLMs and practical workflows using PyTorch and HuggingFace.\n\n\n\nStart with working code: Each post begins with hands-on examples defining real tasks\nIterative deepening: Concepts are revisited with increasing depth\nLearn by building: Notebooks and exercises included for every topic\n\n\n\n\n\nHands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists\n\nWhat is Qwen3-14B?\nHow to use it for vision-language tasks\nHands-on: inference and fine-tuning\n\nFine-Tuning Qwen3-14B with Unsloth\n\nPractical guide to fine-tuning with Unsloth\nColab and local multi-GPU workflows\nTips for data scientists"
  },
  {
    "objectID": "posts/series/vlm.html#series-overview",
    "href": "posts/series/vlm.html#series-overview",
    "title": "VLM Series",
    "section": "",
    "text": "This series takes a fastai-inspired, top-down approach to Vision-Language Models (VLMs). We‚Äôll start with real-world applications and working code, then dive deeper into the concepts behind these powerful models. The focus is on open-source VLMs and practical workflows using PyTorch and HuggingFace.\n\n\n\nStart with working code: Each post begins with hands-on examples defining real tasks\nIterative deepening: Concepts are revisited with increasing depth\nLearn by building: Notebooks and exercises included for every topic\n\n\n\n\n\nHands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists\n\nWhat is Qwen3-14B?\nHow to use it for vision-language tasks\nHands-on: inference and fine-tuning\n\nFine-Tuning Qwen3-14B with Unsloth\n\nPractical guide to fine-tuning with Unsloth\nColab and local multi-GPU workflows\nTips for data scientists"
  },
  {
    "objectID": "posts/series/vlm.html#prerequisites",
    "href": "posts/series/vlm.html#prerequisites",
    "title": "VLM Series",
    "section": "0.2 Prerequisites",
    "text": "0.2 Prerequisites\n\nBasic Python and PyTorch\nFamiliarity with Jupyter notebooks\nSome experience with deep learning (helpful, not required)"
  },
  {
    "objectID": "posts/series/vlm.html#tools-well-use",
    "href": "posts/series/vlm.html#tools-well-use",
    "title": "VLM Series",
    "section": "0.3 Tools We‚Äôll Use",
    "text": "0.3 Tools We‚Äôll Use\n\nPyTorch for model development\nHuggingFace Transformers for VLMs\nFastAI for rapid prototyping (where applicable)\nOpen-source VLMs (Qwen, LLaVA, etc.)"
  },
  {
    "objectID": "posts/series/vlm.html#getting-help",
    "href": "posts/series/vlm.html#getting-help",
    "title": "VLM Series",
    "section": "0.4 Getting Help",
    "text": "0.4 Getting Help\n\nUse the comments section below each post\nCheck the GitHub repository for code\nJoin our discussion forum (coming soon)\n\n\n\n\n\n\n\nNote\n\n\n\nThis series is updated regularly based on reader feedback and new developments in VLM research."
  },
  {
    "objectID": "posts/series/tools/intro.html",
    "href": "posts/series/tools/intro.html",
    "title": "Tools Series",
    "section": "",
    "text": "This series highlights the best modern tools for Python developers, with a focus on speed, reproducibility, and developer happiness. Each post dives deep into a single tool, showing not just how to use it, but why it matters.\n\n\n\nUV: A Bolt of Lightning for Python Packaging"
  },
  {
    "objectID": "posts/series/tools/intro.html#posts-in-this-series",
    "href": "posts/series/tools/intro.html#posts-in-this-series",
    "title": "Tools Series",
    "section": "",
    "text": "UV: A Bolt of Lightning for Python Packaging"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html",
    "href": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html",
    "title": "Fine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists",
    "section": "",
    "text": "Tip\n\n\n\nThis post is part of the VLM Series. Feedback and questions are welcome!"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#introduction",
    "href": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#introduction",
    "title": "Fine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists",
    "section": "0.1 Introduction",
    "text": "0.1 Introduction\nLarge language models (LLMs) like Qwen3-14B are powerful, but off-the-shelf models can fall short in specialized domains. Fine-tuning lets us inject private domain knowledge, align behavior, and optimize for unique downstream tasks.\nIn this guide, we‚Äôll show how to fine-tune Qwen3-14B using Unsloth, a blazing-fast fine-tuning library that works on Colab or local GPUs. We‚Äôll cover both:\n\nColab-based fine-tuning (for light experiments and demos)\nLocal multi-GPU fine-tuning (for serious workloads)"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#why-use-unsloth",
    "href": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#why-use-unsloth",
    "title": "Fine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists",
    "section": "0.2 Why Use Unsloth?",
    "text": "0.2 Why Use Unsloth?\nUnsloth adds significant speed and memory optimizations for training HuggingFace models‚Äîup to 2x faster on consumer GPUs.\nFeatures: - Integrated QLoRA & LoRA - FlashAttention-2 support - HuggingFace-compatible models"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#fine-tuning-in-google-colab",
    "href": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#fine-tuning-in-google-colab",
    "title": "Fine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists",
    "section": "0.3 1. Fine-Tuning in Google Colab",
    "text": "0.3 1. Fine-Tuning in Google Colab\n\n0.3.1 Step 1: Setup\n!pip install --quiet unsloth datasets trl\n\n\n0.3.2 Step 2: Load the Model(QLoRA)\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/qwen2-14b-chat-gptq\",\n    max_seq_length = 2048,\n    dtype = None,\n    load_in_4bit = True,\n)\n\n\n0.3.3 Step 3: Prepare the Dataset\nUse a HuggingFace datasets object:\nfrom datasets import load_dataset\ndataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:500]\")\nOr bring your own in JSONL format:\n{\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}\nStep 4: Fine-Tune with LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,\n    lora_alpha = 16,\n    lora_dropout = 0.05,\n    bias = \"none\",\n    task_type = \"CAUSAL_LM\",\n)\nthen train with:\nfrom trl import SFTTrainer\ntrainer = SFTTrainer(model=model, tokenizer=tokenizer, train_dataset=dataset)\ntrainer.train()"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#fine-tuning-locally-multi-gpu-a100-rtx",
    "href": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#fine-tuning-locally-multi-gpu-a100-rtx",
    "title": "Fine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists",
    "section": "0.4 2. Fine-Tuning Locally (Multi-GPU / A100 / RTX)",
    "text": "0.4 2. Fine-Tuning Locally (Multi-GPU / A100 / RTX)\n\n0.4.1 Setup linux\npip install unsloth[all] accelerate deepspeed\naccelerate config\nEnable multi-GPU with DeepSpeed or accelerate.\n\n\n0.4.2 Launch Training\naccelerate launch train.py\nYour train.py should import and use FastLanguageModel just like in Colab. You‚Äôll get better memory handling and faster throughput with FP16 + QLoRA.\nFor training large corpora, you can stream JSONL from disk, or integrate with DVC or HuggingFace Datasets to handle TB-scale data."
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#practical-use-cases",
    "href": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#practical-use-cases",
    "title": "Fine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists",
    "section": "0.5 Practical Use Cases",
    "text": "0.5 Practical Use Cases\n\nCustom Assistants: Inject your product or company domain into the model.\nData QA Bots: Fine-tune on your own feature dictionaries, KPIs, and docs.\nMath Tutors: Reinforce multi-step reasoning with math datasets (e.g., GSM8K)."
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#tips-for-success",
    "href": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#tips-for-success",
    "title": "Fine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists",
    "section": "0.6 Tips for Success",
    "text": "0.6 Tips for Success\n\nUse max_seq_length=2048 for long-context reasoning tasks.\nRegularize using small LoRA dropout (0.05 or 0.1).\nEvaluate outputs on real tasks‚Äîdon‚Äôt just trust loss!"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#wrap-up",
    "href": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#wrap-up",
    "title": "Fine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists",
    "section": "0.7 Wrap Up",
    "text": "0.7 Wrap Up\nFine-tuning Qwen3-14B with Unsloth makes LLM customization accessible‚Äîwhether you‚Äôre in Colab or scaling up on A100s.\nLet me know if you want a follow-up post on:\n\nEvaluation methods\nQuantization after fine-tuning\nDeploying fine-tuned models"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#next-steps",
    "href": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#next-steps",
    "title": "Fine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists",
    "section": "0.8 Next Steps",
    "text": "0.8 Next Steps\n\nBack to VLM Series Overview\nRead the first post: Hands-On with Qwen3-14B"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#references",
    "href": "posts/series/vlm-qwen3-14b/finetune-qwen3-14b.html#references",
    "title": "Fine-Tuning Qwen3-14B with Unsloth: A Practical Guide for Data Scientists",
    "section": "0.9 References",
    "text": "0.9 References\n\nUnsloth Documentation\nQwen3-14B on HuggingFace\nVLM Series Overview\n\n\n\n\n\n\n\nNote\n\n\n\nThis post is part of the VLM Series. Feedback and questions are welcome!"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "",
    "text": "Related Content\n\n\n\nThis post is part of our deep learning foundations series. You might also be interested in: - Data Science Steps Series - Feature Preprocessing"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#why-matrix-multiplication-matters",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#why-matrix-multiplication-matters",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.1 Why Matrix Multiplication Matters",
    "text": "1.1 Why Matrix Multiplication Matters\nMatrix multiplication is fundamental to deep learning because:\n\nIt‚Äôs the core operation in neural network layers\nIt enables efficient parallel computation\nIt allows us to represent complex transformations compactly"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#implementation-from-scratch",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#implementation-from-scratch",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.2 Implementation from Scratch",
    "text": "1.2 Implementation from Scratch\nLet‚Äôs implement matrix multiplication using Python and NumPy:\n\nimport numpy as np\nimport torch\nfrom typing import List, Tuple\nimport matplotlib.pyplot as plt\n\ndef matmul(a: List[List[float]], b: List[List[float]]) -&gt; List[List[float]]:\n    \"\"\"Matrix multiplication from scratch\"\"\"\n    # Check dimensions\n    assert len(a[0]) == len(b), \"Incompatible dimensions\"\n    \n    # Initialize result matrix\n    result = [[0.0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    \n    # Perform multiplication\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example matrices\nA = [[1, 2], [3, 4]]\nB = [[5, 6], [7, 8]]\n\n# Calculate result\nresult = matmul(A, B)\nprint(\"Result of matrix multiplication:\")\nprint(np.array(result))\n\nResult of matrix multiplication:\n[[19. 22.]\n [43. 50.]]"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#visualizing-matrix-multiplication",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#visualizing-matrix-multiplication",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.3 Visualizing Matrix Multiplication",
    "text": "1.3 Visualizing Matrix Multiplication\nLet‚Äôs create a visual representation of how matrix multiplication works:\n\ndef plot_matrix_mult(A: np.ndarray, B: np.ndarray) -&gt; None:\n    \"\"\"Visualize matrix multiplication process\"\"\"\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Plot first matrix\n    ax1.imshow(A, cmap='viridis')\n    ax1.set_title('Matrix A')\n    \n    # Plot second matrix\n    ax2.imshow(B, cmap='viridis')\n    ax2.set_title('Matrix B')\n    \n    # Plot result\n    result = np.dot(A, B)\n    ax3.imshow(result, cmap='viridis')\n    ax3.set_title('A √ó B')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Create example matrices\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nplot_matrix_mult(A, B)"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#pytorch-implementation",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#pytorch-implementation",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.4 PyTorch Implementation",
    "text": "1.4 PyTorch Implementation\nIn practice, we use optimized libraries like PyTorch:\n\n# Convert to PyTorch tensors\nA_torch = torch.tensor(A, dtype=torch.float32)\nB_torch = torch.tensor(B, dtype=torch.float32)\n\n# PyTorch matrix multiplication\nresult_torch = torch.matmul(A_torch, B_torch)\nprint(\"PyTorch result:\")\nprint(result_torch)\n\nPyTorch result:\ntensor([[19., 22.],\n        [43., 50.]])"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#performance-comparison",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#performance-comparison",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.5 Performance Comparison",
    "text": "1.5 Performance Comparison\nLet‚Äôs compare our implementation with NumPy and PyTorch:\n\nimport time\n\ndef benchmark_matmul(size: int = 100) -&gt; None:\n    \"\"\"Compare performance of different implementations\"\"\"\n    # Generate random matrices\n    A = np.random.randn(size, size)\n    B = np.random.randn(size, size)\n    \n    # Custom implementation\n    start = time.time()\n    _ = matmul(A.tolist(), B.tolist())\n    custom_time = time.time() - start\n    \n    # NumPy\n    start = time.time()\n    _ = np.dot(A, B)\n    numpy_time = time.time() - start\n    \n    # PyTorch\n    A_torch = torch.tensor(A)\n    B_torch = torch.tensor(B)\n    start = time.time()\n    _ = torch.matmul(A_torch, B_torch)\n    torch_time = time.time() - start\n    \n    print(f\"Custom implementation: {custom_time:.4f}s\")\n    print(f\"NumPy: {numpy_time:.4f}s\")\n    print(f\"PyTorch: {torch_time:.4f}s\")\n\nbenchmark_matmul()\n\nCustom implementation: 0.0618s\nNumPy: 0.0094s\nPyTorch: 0.0001s"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#key-takeaways",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#key-takeaways",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.6 Key Takeaways",
    "text": "1.6 Key Takeaways\n\nMatrix multiplication is a fundamental operation in deep learning\nUnderstanding it from first principles helps debug neural networks\nLibraries like PyTorch provide highly optimized implementations\nThe operation is inherently parallelizable\n\n\n\n\n\n\n\nFastAI Insight\n\n\n\nJeremy Howard emphasizes understanding matrix multiplication from scratch because it‚Äôs the foundation of neural network operations. This understanding helps in debugging and optimizing deep learning models."
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#next-steps",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#next-steps",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.7 Next Steps",
    "text": "1.7 Next Steps\nIn future posts, we‚Äôll explore: - How matrix multiplication enables neural network layers - Efficient implementations using CUDA - Common optimization techniques"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#related-posts",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#related-posts",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.8 Related Posts",
    "text": "1.8 Related Posts\n\nData Science Steps Series\nFeature Preprocessing\nUsing Nougat for Research Papers"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/index.html#references",
    "href": "posts/matrix_multiplication_from_fastai_course/index.html#references",
    "title": "Understanding Matrix Multiplication from FastAI",
    "section": "1.9 References",
    "text": "1.9 References\n\nFastAI Course\nDeep Learning Book - Linear Algebra Chapter\nPyTorch Documentation"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html",
    "href": "posts/data-science-steps-to-follow-part03/index.html",
    "title": "Data Science Steps to Follow - 03",
    "section": "",
    "text": "Data Science Steps Series\n\n\n\nThis is Part 3 of a 6-part series on data science fundamentals:\n\nPart 1: EDA Fundamentals\nPart 2: Feature Preprocessing and Generation\nPart 3: Handling Anonymized Data (You are here)\nPart 4: Text Feature Extraction\nPart 5: Image Feature Extraction\nPart 6: Advanced Techniques"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html#explore-individual-feature",
    "href": "posts/data-science-steps-to-follow-part03/index.html#explore-individual-feature",
    "title": "Data Science Steps to Follow - 03",
    "section": "4.1 1. Explore individual feature",
    "text": "4.1 1. Explore individual feature\n\nGuess the meaning of columns.\nGuess the types of columns. Separate them numerical, categorical, ordinal, date, text, etc."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations",
    "href": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations",
    "title": "Data Science Steps to Follow - 03",
    "section": "4.2 2. Explore feature relations",
    "text": "4.2 2. Explore feature relations\n\nFind relation between pairs.\nFind feature groups.\n\n\n4.2.1 1. Individual feature\n\nThere was a competition, where like there are different features, which were anonymized means the features names are x1, x2, etc. There were some numerical and some hash value, which could be categorical feature.\nWhat the lecturer did, first create a baseline with random forest. fillna with -999, categorical featueres factorize.\nThen plot feature importance from this baseline model.\nHe found that, one feature named as x8 has highest influence on the target variable.\nSo he starts to invesitage a little bit deeper.\nThen tried to find mean and std values. It seems close to 0 and 1. It seems normalized but it is not exactly 0 and 1 but extra decimal places. May be because of train and test.\nThen search for other reapeatd values by value counts. It seems there are lots of repeted values.\nAs we understood them, they are normalized, we tried to find the normalize parameter, means scaling and shift parameter. Lets‚Äôs try to find it, or is it actaully possible.\n\nSearch for unique values and sort them.\nThen use np.diff to find the difference between two consecutive values. It seems the values are same all the time.\nThen devide this values with to our sorted array. It is almost 1. May be not 1 because of some numerical error.\nSo if we devide this vlaue to our feature, we will get the original values. It is also visible each positive number decimal places are same and also each negative number decimal places are same. This could be part of shifting parameter.\nSo we devide to our previous value and substract to decimal place, we get almost integer values.\nAfter that it seems that we are right direction, because we are getting integer values. However we got shifing value, a fractioanal part, but how to get the full part of shifting value.\nSo the lecturer had a hanch. He just tried value counts of integer values. Then he found an extremely different number from other and the value was - 1968 . So he assumed may be it is some kind of year and one person put 0 or forgot to enter, then system converted to 0. So may be the shifting value is 1968. So he tried to substract 1968 from the feature and then he got the original values.\nBut how it helps in the competition. One can use different things from it. But at that competition he could not use this feature. But it was very interseting to see how he found the original values.\n\nif there are small features we can see manually. If there are many features.\ndf.dtypes\ndf.info()\nx.value_counts()\nx.isnull()"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations-1",
    "href": "posts/data-science-steps-to-follow-part03/index.html#explore-feature-relations-1",
    "title": "Data Science Steps to Follow - 03",
    "section": "4.3 2. Explore feature relations",
    "text": "4.3 2. Explore feature relations\n\nTo explore different features, important things can be done is visualization\n\nExplore individual features\n\n\nHistograms\nplots\nstatistics\n\n\nExplore feature relations\n\n\nScatter plots\nCorrelation plots\nPlot(index vs feature statistics)\nAnd more.\n\n\nEDA is art and visualization is the tool.\n\n\n\n4.3.1 1. Individual features\nplt.hist(x)\n\nSometimes it is misleading. So change number of bins.\nNever make a hypothesis from a single plot. Try to plot different things and then make a decision.\nSometimes in histogram you will see some spikes. It could be anything. Actually in particular case the organizer put the missing vlaue with its mean. We can change this value with other than mean.\nwe can also plot x is index and y is feature value. Not conenct with line but with circles only. python plt.plot(x,'.' )\n\n\n\nindex_image\n\n\nIf we see horizontal line in such plots, it means there are repeated values and if there is no vertical lines, it means the data is shuffled nicely.\nWe can also color code based on labels.\nplt.scater(x,y,c=y)\nPandas describe function also helps a lot\npd.describe()\nx.mean()\nx.var()\nAlso value counts and isnull is very helpfull\nx.value_counts()\nx.isnull()\n\n\n\n4.3.2 2. Feature relation\n\nScatter plot\n\nplt.scatter(x,y)\n\nfor classificaiton we can color map the label\nfor regression heatmap can be used.\nAlso we can compare the scatter plot in trianing and test set is same.\n\n\n\ncolor_code_train_test\n\n\nThe following graph show the diagonal realtion. The equation of a diagonal, x1 -&gt; xaxis and x2-&gt; y axis\n\n\\[x2&lt;=1 -x1\\]\n\nThe equation of diagonal line is $ x1 + x2 = 1 $\n\n\n\n\ndiagonal_equation\n\n\n\nSuppose we found this relaiton but how to use them. There are differet ways but for tree based model we can create the difference or ratio of these two features.\nIf we see the following scatter plot, we can see that there are some outliers. So we can remove them.\n\n\n\nscatter plot\n\n\nSo how this is helpful, our goal is to generate features. How to generate feature from this plot. As we see two traingles, we can create a feature where each triangle will get a set of points and hope this feature will help.\nIf we have smaller number of features, we can use pandas for all features together.\n\npd.scatter_matrix(df)\n\nIt is always good to use scatter plot and histogram in same plot. Scatter plot -&gt; week information about densities, while histogram -&gt; don‚Äôt show feature interaction.\nWe can also create a correlation plot. It is a heatmap. It is very helpfull to find the correlation between features. It is also good to see the correlation between features and labels.\n\ndf.corr(), plt.matshow(..)\n\nWe can also create other matrix other than correlation matrix, How many times, one feature is greater than another feature.\nif the matrix is a total mess like following\n\n\n\n\nmessy_matrix\n\n\nwe can create some kind of clsutering and then plot them, like k means clustering or rows and columnd and reorder those features. The following plot is the result of k means clustering.\n\n\n\nordered\n\n\n\n4.3.2.1 Feature groups\n\nNew features based on groups\nOne important feature plot could\n\ndf.mean().plot(style='.')\nx -&gt; feature y -&gt; feature mean\n\nIf this is random, then may be we will see random. But if we sort them.\n\ndf.mean().sort_values().plot(style='.')\n\n\n\nordered_feature_mean\n\n\n\nNow we can have close look to each group and use imagination to create new features.\n\nNext post can be found here"
  },
  {
    "objectID": "posts/nougat-to-read-scientific-pdf-files/index.html",
    "href": "posts/nougat-to-read-scientific-pdf-files/index.html",
    "title": "How to Use Nougat to Read Scientific Paper",
    "section": "",
    "text": "Copied from here"
  },
  {
    "objectID": "posts/nougat-to-read-scientific-pdf-files/index.html#jupter-notebook-is-the-following",
    "href": "posts/nougat-to-read-scientific-pdf-files/index.html#jupter-notebook-is-the-following",
    "title": "How to Use Nougat to Read Scientific Paper",
    "section": "2.1 Jupter notebook is the following",
    "text": "2.1 Jupter notebook is the following\n#pip install -q pymupdf python-Levenshtein nltk\nfrom transformers import AutoProcessor, VisionEncoderDecoderModel\nimport torch\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Tuple \nimport io\nimport fitz\nfrom huggingface_hub import hf_hub_download\nfrom PIL import Image\nfrom collections import defaultdict\nfrom transformers import StoppingCriteria, StoppingCriteriaList\nprocessor = AutoProcessor.from_pretrained(\"facebook/nougat-small\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-small\")\nDownloading (‚Ä¶)rocessor_config.json:   0%|          | 0.00/479 [00:00&lt;?, ?B/s]\n\n\n\nDownloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/4.49k [00:00&lt;?, ?B/s]\n\n\n\nDownloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/2.14M [00:00&lt;?, ?B/s]\n\n\n\nDownloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/96.0 [00:00&lt;?, ?B/s]\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\nDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/4.77k [00:00&lt;?, ?B/s]\n\n\n\nDownloading pytorch_model.bin:   0%|          | 0.00/990M [00:00&lt;?, ?B/s]\n\n\n\nDownloading (‚Ä¶)neration_config.json:   0%|          | 0.00/165 [00:00&lt;?, ?B/s]\n%%capture\n#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = \"cpu\"\nmodel.to(device)\n     \nfilepath = hf_hub_download(repo_id=\"ysharma/nougat\", filename=\"input/nougat.pdf\", repo_type=\"space\")\nDownloading nougat.pdf:   0%|          | 0.00/4.13M [00:00&lt;?, ?B/s]\ndef rasterize_paper(\n    pdf: Path,\n    outpath: Optional[Path] = None,\n    dpi: int = 96,\n    return_pil=False,\n    pages=None,\n) -&gt; Optional[List[io.BytesIO]]:\n    \"\"\"\n    Rasterize a PDF file to PNG images.\n\n    Args:\n        pdf (Path): The path to the PDF file.\n        outpath (Optional[Path], optional): The output directory. If None, the PIL images will be returned instead. Defaults to None.\n        dpi (int, optional): The output DPI. Defaults to 96.\n        return_pil (bool, optional): Whether to return the PIL images instead of writing them to disk. Defaults to False.\n        pages (Optional[List[int]], optional): The pages to rasterize. If None, all pages will be rasterized. Defaults to None.\n\n    Returns:\n        Optional[List[io.BytesIO]]: The PIL images if `return_pil` is True, otherwise None.\n    \"\"\"\n\n    pillow_images = []\n    if outpath is None:\n        return_pil = True\n    try:\n        if isinstance(pdf, (str, Path)):\n            pdf = fitz.open(pdf)\n        if pages is None:\n            pages = range(len(pdf))\n        for i in pages:\n            page_bytes: bytes = pdf[i].get_pixmap(dpi=dpi).pil_tobytes(format=\"PNG\")\n            if return_pil:\n                pillow_images.append(io.BytesIO(page_bytes))\n            else:\n                with (outpath / (\"%02d.png\" % (i + 1))).open(\"wb\") as f:\n                    f.write(page_bytes)\n    except Exception:\n        pass\n    if return_pil:\n        return pillow_images\nimages = rasterize_paper(pdf=filepath, return_pil=True)\nlen(images)\n17\nimage = Image.open(images[0])\nimage\n\n\n\npng\n\n\n# prepare image for the model\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\nprint(pixel_values.shape)\n     \ntorch.Size([1, 3, 896, 672])\nclass RunningVarTorch:\n    def __init__(self, L=15, norm=False):\n        self.values = None\n        self.L = L\n        self.norm = norm\n\n    def push(self, x: torch.Tensor):\n        assert x.dim() == 1\n        if self.values is None:\n            self.values = x[:, None]\n        elif self.values.shape[1] &lt; self.L:\n            self.values = torch.cat((self.values, x[:, None]), 1)\n        else:\n            self.values = torch.cat((self.values[:, 1:], x[:, None]), 1)\n\n    def variance(self):\n        if self.values is None:\n            return\n        if self.norm:\n            return torch.var(self.values, 1) / self.values.shape[1]\n        else:\n            return torch.var(self.values, 1)\nclass StoppingCriteriaScores(StoppingCriteria):\n    def __init__(self, threshold: float = 0.015, window_size: int = 200):\n        super().__init__()\n        self.threshold = threshold\n        self.vars = RunningVarTorch(norm=True)\n        self.varvars = RunningVarTorch(L=window_size)\n        self.stop_inds = defaultdict(int)\n        self.stopped = defaultdict(bool)\n        self.size = 0\n        self.window_size = window_size\n\n    @torch.no_grad()\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n        last_scores = scores[-1]\n        self.vars.push(last_scores.max(1)[0].float().cpu())\n        self.varvars.push(self.vars.variance())\n        self.size += 1\n        if self.size &lt; self.window_size:\n            return False\n\n        varvar = self.varvars.variance()\n        for b in range(len(last_scores)):\n            if varvar[b] &lt; self.threshold:\n                if self.stop_inds[b] &gt; 0 and not self.stopped[b]:\n                    self.stopped[b] = self.stop_inds[b] &gt;= self.size\n                else:\n                    self.stop_inds[b] = int(\n                        min(max(self.size, 1) * 1.15 + 150 + self.window_size, 4095)\n                    )\n            else:\n                self.stop_inds[b] = 0\n                self.stopped[b] = False\n        return all(self.stopped.values()) and len(self.stopped) &gt; 0\n\n# autoregressively generate tokens, with custom stopping criteria (as defined by the Nougat authors)\noutputs = model.generate(pixel_values.to(device),\n                          min_length=1,\n                          max_length=3584,\n                          bad_words_ids=[[processor.tokenizer.unk_token_id]],\n                          return_dict_in_generate=True,\n                          output_scores=True,\n                          stopping_criteria=StoppingCriteriaList([StoppingCriteriaScores()]),\n)\ngenerated = processor.batch_decode(outputs[0], skip_special_tokens=True)[0]\ngenerated = processor.post_process_generation(generated, fix_markdown=False)\nprint(generated)\n# Nougat: Neural Optical Understanding for Academic Documents\n\n Lukas Blecher\n\nCorrespondence to: lblecher@meta.com\n\nGuillem Cucurull\n\nThomas Scialom\n\nRobert Stojnic\n\nMeta AI\n\nThe paper reports 8.1M papers but the authors recently updated the numbers on the GitHub page https://github.com/allenai/s2orc\n\n###### Abstract\n\nScientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (**N**eural **O**ptical **U**nderstanding for **A**cademic Documents), a Visual Transformer model that performs an _Optical Character Recognition_ (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.\n\n## 1 Introduction\n\nThe majority of scientific knowledge is stored in books or published in scientific journals, most commonly in the Portable Document Format (PDF). Next to HTML, PDFs are the second most prominent data format on the internet, making up 2.4% of common crawl [1]. However, the information stored in these files is very difficult to extract into any other formats. This is especially true for highly specialized documents, such as scientific research papers, where the semantic information of mathematical expressions is lost.\n\nExisting Optical Character Recognition (OCR) engines, such as Tesseract OCR [2], excel at detecting and classifying individual characters and words in an image, but fail to understand the relationship between them due to their line-by-line approach. This means that they treat superscripts and subscripts in the same way as the surrounding text, which is a significant drawback for mathematical expressions. In mathematical notations like fractions, exponents, and matrices, relative positions of characters are crucial.\n\nConverting academic research papers into machine-readable text also enables accessibility and searchability of science as a whole. The information of millions of academic papers can not be fully accessed because they are locked behind an unreadable format. Existing corpora, such as the S2ORC dataset [3], capture the text of 12M2 papers using GROBID [4], but are missing meaningful representations of the mathematical equations.\n\nFootnote 2: The paper reports 8.1M papers but the authors recently updated the numbers on the GitHub page https://github.com/allenai/s2orc\n\nTo this end, we introduce Nougat, a transformer based model that can convert images of document pages to formatted markup text.\n\nThe primary contributions in this paper are\n\n* Release of a pre-trained model capable of converting a PDF to a lightweight markup language. We release the code and the model on GitHub3 Footnote 3: https://github.com/facebookresearch/nougat\n* We introduce a pipeline to create dataset for pairing PDFs to source code\n* Our method is only dependent on the image of a page, allowing access to scanned papers and books"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html",
    "href": "posts/data-science-steps-to-follow-part04/index.html",
    "title": "Data Science Steps to Follow - 04",
    "section": "",
    "text": "Data Science Steps Series\n\n\n\nThis is Part 3 of a 6-part series on data science fundamentals:\n\nPart 1: EDA Fundamentals\nPart 2: Feature Preprocessing and Generation\nPart 3: Handling Anonymized Data\nPart 4: Text Feature Extraction (you are here)\nPart 5: Image Feature Extraction\nPart 6: Advanced Techniques"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html#bag-of-words",
    "href": "posts/data-science-steps-to-follow-part04/index.html#bag-of-words",
    "title": "Data Science Steps to Follow - 04",
    "section": "2.1 1. Bag of words",
    "text": "2.1 1. Bag of words\n1.1. CountVectorizer\n\neach word is separated and count number of occurences\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())\n\nWe may be need to do some post processing. As we know KNN, neural networks are sensitive to the scale of the features. So we need to scale the features. We can use TF-IDF to do this.\n\n1.2. TfidfVectorizer\n\nWhat actually is just not frequency but normalized frequency.\nTerm frequency:\n\ntf = 1/ x.sum[axis=1](:,None)\nx = x * tf\n\nInverse document frequency:\n\nidf = np.log(x.shape[0]/(x&gt;0).sum(axis=0)))\nx = x*idf\nsklearn.feature_extraction.text.TfidfVectorizer\n1.3 N-grams\n\nNot only words but n-consequent words\n\nsklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2)) \n# may be parameter analyzer\n\n2.1.1 Text Preprocessing\n\nActually before applying any Bag of words we need to preprocess the text. We need to remove the stop words, stemming, lemmatization, etc.* Conventionally preprocessing are\n\nTokenization -&gt; Very very sunny day -&gt; [Very, very, sunny, day]\nLowercasing -&gt; [very, very, sunny, day] -&gt; [very, very, sunny, day] -&gt;CountVectorizer from sklearn will automatically do this\nRemoving punctuation\nRemoving stopwords -&gt; [The cow jumped over the moon] -&gt; [cow, jumped, moon]\n\nAriticles or preprositon words\nVery common words\nCan be used NLTK library\nsklearn.feature_extraction.text.CountVectorizer(max_df)\nmax_df is the frequency threshold, after which the word is removed\n\nStemming/Lemmatization\nStemming\n\n[democracy, democratic, democratization] -&gt; [democr]\n[Saw] -&gt; [s]\n\nLemitization\n\n[democracy, democratic, democratization] -&gt; [democracy]\n[Saw, sawing, sawed] -&gt; [see or saw] depending on text"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html#summray-of-bag-of-words-pipeline",
    "href": "posts/data-science-steps-to-follow-part04/index.html#summray-of-bag-of-words-pipeline",
    "title": "Data Science Steps to Follow - 04",
    "section": "2.2 Summray of Bag of words Pipeline",
    "text": "2.2 Summray of Bag of words Pipeline\n\nPreprocessing Lowercasing, removing punctuation, removing stopwords, stemming/lemmatization\nN-grams helps to get local context\nPost processing TF-IDF"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part04/index.html#embeddings",
    "href": "posts/data-science-steps-to-follow-part04/index.html#embeddings",
    "title": "Data Science Steps to Follow - 04",
    "section": "2.3 2. Embeddings",
    "text": "2.3 2. Embeddings\n\n2.3.1 Word2vec\n\nVector representation of words and text\nEach word is represented as a vector, in some sophisticated way, which could have 100 dimensions or more.\nSame words will have similar vectors. king-&gt;queen\nAlso addition and subtraction of vectors will have some meaning. -&gt; king + woman - man = queen\nSeveral implementaton of word2vec\n\nWord2vec\nGlove\nFastText\n\nSentences\n\nDoc2vec\n\nBased on situation we can use word or sentence embeddings. Actually try both and take the best one.\nAll the preprocessing steps can be applied to the text before applying word2vec.\n\n\n\n2.3.2 Comparion Bag of words and Word2vec\n\nBag of words\n\nVery large vector\nmeaning is easy value in vector is known\n\nWord2vec\n\nRelative Small vector\nValues of vector can be interpreted only some cases\nThe words with simlar meaning will have similar embeddings\n\n\nNext post can be found here"
  },
  {
    "objectID": "tags.html",
    "href": "tags.html",
    "title": "Tags",
    "section": "",
    "text": "Welcome to the tag index! Here you can find all posts organized by their tags. Use the search and filter options to find specific content.\n\n\n\n\n\n\n\n\nBelow is a searchable table of all posts with their associated tags:"
  },
  {
    "objectID": "tags.html#all-posts",
    "href": "tags.html#all-posts",
    "title": "Tags",
    "section": "",
    "text": "Below is a searchable table of all posts with their associated tags:"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html",
    "href": "posts/data-science-steps-to-follow-part01/index.html",
    "title": "Data Science Steps to Follow - 01",
    "section": "",
    "text": "Data Science Steps Series\n\n\n\nThis is Part 1 of a 6-part series on data science fundamentals:\n\nPart 1: EDA Fundamentals (You are here)\nPart 2: Feature Preprocessing and Generation\nPart 3: Handling Anonymized Data\nPart 4: Text Feature Extraction\nPart 5: Image Feature Extraction\nPart 6: Advanced Techniques"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#what-is-eda",
    "href": "posts/data-science-steps-to-follow-part01/index.html#what-is-eda",
    "title": "Data Science Steps to Follow - 01",
    "section": "2.1 What is EDA?",
    "text": "2.1 What is EDA?\nEDA is the critical first step in any data science project. It helps us:\n\nUnderstand the data deeply\nBuild intuition about patterns and relationships\nGenerate hypotheses for feature engineering\nFind insights that inform modeling decisions"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#the-power-of-visualization",
    "href": "posts/data-science-steps-to-follow-part01/index.html#the-power-of-visualization",
    "title": "Data Science Steps to Follow - 01",
    "section": "2.2 The Power of Visualization",
    "text": "2.2 The Power of Visualization\nOne of the most powerful EDA tools is visualization. Let‚Äôs look at a fascinating example from a Kaggle competition:\n\n\n\n\n\n\nReal-World Example\n\n\n\nIn a promotion prediction competition, simple visualization revealed: - Two key features: promos sent and promos used - A direct relationship between their difference and the target - This insight led to 81% accuracy without complex modeling!\n\n\n# Example visualization code\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_promo_relationship(df):\n    plt.figure(figsize=(10, 6))\n    plt.scatter('promos_sent', 'promos_used', data=df)\n    plt.xlabel('Number of Promos Sent')\n    plt.ylabel('Number of Promos Used')\n    plt.title('Promo Usage Pattern')\n    plt.show()"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#building-intuition",
    "href": "posts/data-science-steps-to-follow-part01/index.html#building-intuition",
    "title": "Data Science Steps to Follow - 01",
    "section": "2.3 Building Intuition",
    "text": "2.3 Building Intuition\n\n2.3.1 1. Domain Knowledge Acquisition\nBefore diving into analysis, focus on:\n\nUnderstanding the business goal\nResearching similar problems\nLearning industry-specific metrics\nReading relevant documentation\n\n\n\n2.3.2 2. Data Validation\n\n\n\n\n\n\nKey Validation Checks\n\n\n\n\nValue ranges (e.g., age between 0-120)\nLogical relationships (clicks ‚â§ impressions)\nMissing value patterns\nOutliers and anomalies\n\n\n\ndef validate_data(df):\n    \"\"\"Basic data validation checks\"\"\"\n    issues = []\n    \n    # Age check\n    if df['age'].max() &gt; 120:\n        issues.append(\"Found age &gt; 120\")\n        \n    # Click validation\n    if any(df['clicks'] &gt; df['impressions']):\n        issues.append(\"Found clicks &gt; impressions\")\n    \n    return issues\n\n\n2.3.3 3. Understanding Data Generation\nKey considerations:\n\nSampling methodology\nTrain/test split rationale\nTime-based patterns\nData collection process\n\n\n\n\n\n\n\nJeremy Howard‚Äôs Insight\n\n\n\n‚ÄúIn data science, there are no outliers - only opportunities to understand your data better.‚Äù"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#related-resources",
    "href": "posts/data-science-steps-to-follow-part01/index.html#related-resources",
    "title": "Data Science Steps to Follow - 01",
    "section": "2.4 Related Resources",
    "text": "2.4 Related Resources\n\nUnderstanding Matrix Multiplication - Essential math for data science\nUsing Nougat for Research Papers - Tool for data science research"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#next-steps",
    "href": "posts/data-science-steps-to-follow-part01/index.html#next-steps",
    "title": "Data Science Steps to Follow - 01",
    "section": "2.5 Next Steps",
    "text": "2.5 Next Steps\nContinue to Part 2: Feature Preprocessing and Generation, where we‚Äôll explore how to transform raw data into meaningful features."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part01/index.html#references",
    "href": "posts/data-science-steps-to-follow-part01/index.html#references",
    "title": "Data Science Steps to Follow - 01",
    "section": "2.6 References",
    "text": "2.6 References\n\nCoursera: How to Win a Data Science Competition\nFast.ai: Practical Deep Learning\nPython Data Science Handbook"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part05/index.html",
    "href": "posts/data-science-steps-to-follow-part05/index.html",
    "title": "Data Science Steps to Follow - 05",
    "section": "",
    "text": "Data Science Steps Series\n\n\n\nThis is Part 5 of a 6-part series on data science fundamentals:\n\nPart 1: EDA Fundamentals\nPart 2: Feature Preprocessing and Generation\nPart 3: Handling Anonymized Data\nPart 4: Text Feature Extraction\nPart 5: Image Feature Extraction ( You are here)\nPart 6: Advanced Techniques"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part05/index.html#next-steps",
    "href": "posts/data-science-steps-to-follow-part05/index.html#next-steps",
    "title": "Data Science Steps to Follow - 05",
    "section": "2.1 Next Steps",
    "text": "2.1 Next Steps\nContinue to Part 6: Advanced techniques."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html",
    "href": "posts/data-science-steps-to-follow-part02/index.html",
    "title": "Data Science Steps to Follow - 02",
    "section": "",
    "text": "Data Science Steps Series\n\n\n\nThis is Part 2 of a 6-part series on data science fundamentals:\n\nPart 1: EDA Fundamentals\nPart 2: Feature Preprocessing and Generation (You are here)\nPart 3: Handling Anonymized Data\nPart 4: Text Feature Extraction\nPart 5: Image Feature Extraction\nPart 6: Advanced Techniques"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#interactive-data-explorer",
    "href": "posts/data-science-steps-to-follow-part02/index.html#interactive-data-explorer",
    "title": "Data Science Steps to Follow - 02",
    "section": "2.1 Interactive Data Explorer",
    "text": "2.1 Interactive Data Explorer\n\n\nShow/hide code for interactive data explorer\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Generate sample data\nnp.random.seed(42)\nn_samples = 1000\n\ndata = {\n    'age': np.random.normal(35, 10, n_samples),\n    'income': np.random.lognormal(10, 1, n_samples),\n    'education_years': np.random.randint(8, 22, n_samples),\n    'satisfaction': np.random.randint(1, 6, n_samples)\n}\n\ndf = pd.DataFrame(data)\n\n# Create interactive scatter plot\nfig = px.scatter(df, x='age', y='income', \n                 color='satisfaction',\n                 size='education_years',\n                 hover_data=['education_years'],\n                 title='Interactive Feature Relationships')\nfig.show()"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#feature-preprocessing-steps",
    "href": "posts/data-science-steps-to-follow-part02/index.html#feature-preprocessing-steps",
    "title": "Data Science Steps to Follow - 02",
    "section": "2.2 Feature Preprocessing Steps",
    "text": "2.2 Feature Preprocessing Steps\n\n2.2.1 1. Handling Missing Values\n\nCodeExample\n\n\ndef handle_missing_values(df):\n    # Numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n    \n    # Categorical columns\n    cat_cols = df.select_dtypes(exclude=[np.number]).columns\n    df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n    \n    return df\n\n\n# Example usage\ndf_clean = handle_missing_values(df.copy())\nprint(\"Missing values after cleaning:\")\nprint(df_clean.isnull().sum())\n\n\n\n\n\n2.2.2 2. Scaling Features\nLet‚Äôs compare different scaling methods:\n\n\nShow/hide scaling comparison code\ndef compare_scaling_methods(data):\n    # Original data\n    original = data['age'].copy()\n    \n    # Standard scaling\n    scaler = StandardScaler()\n    standard_scaled = scaler.fit_transform(original.values.reshape(-1, 1))\n    \n    # Min-max scaling\n    min_max_scaler = MinMaxScaler()\n    minmax_scaled = min_max_scaler.fit_transform(original.values.reshape(-1, 1))\n    \n    # Plotting\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Original distribution\n    sns.histplot(original, ax=axes[0])\n    axes[0].set_title('Original Data')\n    \n    # Standard scaled\n    sns.histplot(standard_scaled, ax=axes[1])\n    axes[1].set_title('Standard Scaled')\n    \n    # Min-max scaled\n    sns.histplot(minmax_scaled, ax=axes[2])\n    axes[2].set_title('Min-Max Scaled')\n    \n    plt.tight_layout()\n    plt.show()\n\ncompare_scaling_methods(df)\n\n\n\n\n\n\n\n2.2.3 3. Feature Generation\n\n\n\n\n\n\nInteractive Feature Generator\n\n\n\nUse the code below to experiment with different feature combinations:\n\n\n\n\nShow/hide feature generation code\ndef generate_features(df):\n    \"\"\"Generate new features from existing ones\"\"\"\n    # Polynomial features\n    df['income_squared'] = df['income'] ** 2\n    \n    # Interaction features\n    df['income_per_education'] = df['income'] / df['education_years']\n    \n    # Binning\n    df['age_group'] = pd.qcut(df['age'], q=5, labels=['Very Young', 'Young', 'Middle', 'Senior', 'Elder'])\n    \n    return df\n\n# Generate new features\ndf_featured = generate_features(df.copy())\n\n# Show correlations\nplt.figure(figsize=(10, 8))\nsns.heatmap(df_featured.select_dtypes(include=[np.number]).corr(), \n            annot=True, cmap='coolwarm', center=0)\nplt.title('Feature Correlations')\nplt.show()"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#best-practices",
    "href": "posts/data-science-steps-to-follow-part02/index.html#best-practices",
    "title": "Data Science Steps to Follow - 02",
    "section": "2.3 Best Practices",
    "text": "2.3 Best Practices\n\n\n\n\n\n\nKey Points to Remember\n\n\n\n\nAlways scale features after splitting into train/test sets\nHandle missing values before feature generation\nDocument all preprocessing steps for reproducibility\nValidate generated features with domain experts"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#interactive-feature-selection-tool",
    "href": "posts/data-science-steps-to-follow-part02/index.html#interactive-feature-selection-tool",
    "title": "Data Science Steps to Follow - 02",
    "section": "2.4 Interactive Feature Selection Tool",
    "text": "2.4 Interactive Feature Selection Tool\n\n\nShow/hide feature selection tool\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\ndef plot_feature_importance(X, y, k=5):\n    \"\"\"Plot top k most important features\"\"\"\n    selector = SelectKBest(score_func=f_regression, k=k)\n    selector.fit(X, y)\n    \n    # Get feature scores\n    scores = pd.DataFrame({\n        'Feature': X.columns,\n        'Score': selector.scores_\n    }).sort_values('Score', ascending=False)\n    \n    # Create interactive bar plot\n    fig = px.bar(scores, x='Feature', y='Score',\n                 title=f'Top {k} Most Important Features',\n                 labels={'Score': 'Importance Score'})\n    fig.show()\n\n# Example usage\nX = df_featured.select_dtypes(include=[np.number]).drop('satisfaction', axis=1)\ny = df_featured['satisfaction']\nplot_feature_importance(X, y)"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#next-steps",
    "href": "posts/data-science-steps-to-follow-part02/index.html#next-steps",
    "title": "Data Science Steps to Follow - 02",
    "section": "2.5 Next Steps",
    "text": "2.5 Next Steps\nContinue to Part 3: Handling Anonymized Data to learn about working with masked and anonymized datasets."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#related-resources",
    "href": "posts/data-science-steps-to-follow-part02/index.html#related-resources",
    "title": "Data Science Steps to Follow - 02",
    "section": "2.6 Related Resources",
    "text": "2.6 Related Resources\n\nUnderstanding Matrix Multiplication - Important for feature transformations\nData Science Series Overview"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part02/index.html#references",
    "href": "posts/data-science-steps-to-follow-part02/index.html#references",
    "title": "Data Science Steps to Follow - 02",
    "section": "2.7 References",
    "text": "2.7 References\n\nScikit-learn Preprocessing Guide\nFeature Engineering for Machine Learning\nPython Data Science Handbook"
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/00_matmul.html",
    "href": "posts/matrix_multiplication_from_fastai_course/00_matmul.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "This notebook is actually reprodcution fo course practical deep learning for coders part2. Matrix Multiplication starts with Lession 11 (1:08:47).\nTo get the noebook shown in the video, one should clone the course repository and run the notebook 01_matmul.ipynb."
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#first-description",
    "href": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#first-description",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "This notebook is actually reprodcution fo course practical deep learning for coders part2. Matrix Multiplication starts with Lession 11 (1:08:47).\nTo get the noebook shown in the video, one should clone the course repository and run the notebook 01_matmul.ipynb."
  },
  {
    "objectID": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#get-data",
    "href": "posts/matrix_multiplication_from_fastai_course/00_matmul.html#get-data",
    "title": "Matrix Multiplication",
    "section": "0.2 Get data",
    "text": "0.2 Get data\nWe first download the mnist data to work with it\n\n\nCode\nfrom pathlib import Path\nimport gzip, time, os, pickle, math\nfrom urllib.request import urlretrieve\n\n\n\n\nCode\nHOME = os.getenv('HOME')\ndata_path = Path(fr'{HOME}/Schreibtisch/projects/git_data/course22p2/nbs/data')\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = data_path\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\n\n\n\nCode\nwith gzip.open(path_gz, 'rb') as f_in:\n    ((x_train, y_train), (x_valid, y_valid),_ ) =pickle.load(f_in, encoding='latin1')\n\n\n\n\nCode\nx_train.shape\n\n\n(50000, 784)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Data Science & Machine Learning Blog",
    "section": "",
    "text": "Welcome to my technical blog where I share insights about Data Science, Machine Learning, and Deep Learning!"
  },
  {
    "objectID": "posts/welcome/index.html#what-to-expect",
    "href": "posts/welcome/index.html#what-to-expect",
    "title": "Welcome To My Data Science & Machine Learning Blog",
    "section": "0.1 What to Expect",
    "text": "0.1 What to Expect\nIn this blog, you‚Äôll find:\n\n0.1.1 1. Data Science Series\nA comprehensive guide covering everything from exploratory data analysis to advanced modeling:\n\nPart 1: Exploratory Data Analysis Fundamentals\nPart 2: Feature Preprocessing and Generation\nPart 3: Handling Anonymized Data\nPart 4: Text Feature Extraction\nPart 5: Image Feature Extraction\nPart 6: Advanced Techniques\n\n\n\n0.1.2 2. Deep Learning Concepts\nExplanations inspired by Jeremy Howard‚Äôs teaching style:\n\nUnderstanding Matrix Multiplication\nMore coming soon!\n\n\n\n0.1.3 3. Technical Tools\nExploring modern data science tools:\n\nUsing Nougat to Read Scientific Papers\n\n\n\n0.1.4 4. Practical Tutorials\nHands-on guides with real-world applications (Coming Soon)"
  },
  {
    "objectID": "posts/welcome/index.html#getting-started",
    "href": "posts/welcome/index.html#getting-started",
    "title": "Welcome To My Data Science & Machine Learning Blog",
    "section": "0.2 Getting Started",
    "text": "0.2 Getting Started\nIf you‚Äôre new to data science, I recommend starting with the Data Science Series Part 1, where we explore the fundamentals of data analysis.\nFor those interested in deep learning, check out the matrix multiplication tutorial to understand the building blocks of neural networks.\nStay tuned for regular updates and deep technical content! Feel free to connect with me on GitHub or Twitter."
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html",
    "href": "posts/series/vlm-qwen3-14b/index.html",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "",
    "text": "As data scientists, we constantly move between raw data and real-world decisions. Whether you‚Äôre explaining anomalies, generating insights, or deploying models, reasoning is core to our work. But most large language models (LLMs) are still parrots‚Äîpattern matchers without deeper understanding.\nEnter Qwen3-14B, Alibaba‚Äôs newest open-source model. It‚Äôs not just another massive transformer‚Äîit‚Äôs been designed and instruction-tuned with reasoning and conversation in mind. And thanks to the amazing open-source work by Unsloth, we get a full-featured Colab notebook that lets us try it right now, without needing a GPU cluster.\nThis post is a walkthrough of that notebook:\nColab Notebook: Qwen3 (14B) - Reasoning & Conversational\nWe‚Äôll unpack each section, explain how things work, and give you hands-on examples to help you integrate Qwen3 into your own workflow.\n\n\n\n\n\n\nTip\n\n\n\nThis post is part of the VLM Series. Feedback and questions are welcome!"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#introduction-what-if-llms-could-actually-reason",
    "href": "posts/series/vlm-qwen3-14b/index.html#introduction-what-if-llms-could-actually-reason",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "",
    "text": "As data scientists, we constantly move between raw data and real-world decisions. Whether you‚Äôre explaining anomalies, generating insights, or deploying models, reasoning is core to our work. But most large language models (LLMs) are still parrots‚Äîpattern matchers without deeper understanding.\nEnter Qwen3-14B, Alibaba‚Äôs newest open-source model. It‚Äôs not just another massive transformer‚Äîit‚Äôs been designed and instruction-tuned with reasoning and conversation in mind. And thanks to the amazing open-source work by Unsloth, we get a full-featured Colab notebook that lets us try it right now, without needing a GPU cluster.\nThis post is a walkthrough of that notebook:\nColab Notebook: Qwen3 (14B) - Reasoning & Conversational\nWe‚Äôll unpack each section, explain how things work, and give you hands-on examples to help you integrate Qwen3 into your own workflow.\n\n\n\n\n\n\nTip\n\n\n\nThis post is part of the VLM Series. Feedback and questions are welcome!"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#model-and-tokenizer-loading",
    "href": "posts/series/vlm-qwen3-14b/index.html#model-and-tokenizer-loading",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.2 Model and Tokenizer Loading",
    "text": "0.2 Model and Tokenizer Loading\nLet‚Äôs get started by loading the model and tokenizer using HuggingFace Transformers:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"unsloth/qwen2-14b-chat-gptq\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True)"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#example-the-reasoning-difference",
    "href": "posts/series/vlm-qwen3-14b/index.html#example-the-reasoning-difference",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.3 Example: The Reasoning Difference",
    "text": "0.3 Example: The Reasoning Difference\nTo see the power of Qwen3, consider this classic chain-of-thought task:\nPrompt:\nAlice has 3 apples. She gives 1 to Bob, then buys 5 more. How many apples does she have?\nprompt = \"Alice has 3 apples. She gives 1 to Bob, then buys 5 more. How many apples does she have?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nExpected Output:\nLet‚Äôs break it down. Alice starts with 3 apples. She gives 1 to Bob ‚Üí 3 - 1 = 2. She buys 5 more ‚Üí 2 + 5 = 7. Answer: 7 apples."
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#system-prompts-and-personality",
    "href": "posts/series/vlm-qwen3-14b/index.html#system-prompts-and-personality",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.4 System Prompts and Personality",
    "text": "0.4 System Prompts and Personality\nQwen3 supports ‚Äúsystem prompts‚Äù that define tone and behavior:\nprompt = \"&lt;|im_start|&gt;system\\nYou are a sarcastic data science tutor.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nWhy is my model overfitting?&lt;|im_end|&gt;\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nOutput:\nOh, I don‚Äôt know‚Ä¶ maybe because you fed it every single variable and forgot cross-validation? Classic!"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#fine-tuning-and-customization-optional",
    "href": "posts/series/vlm-qwen3-14b/index.html#fine-tuning-and-customization-optional",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.5 Fine-Tuning and Customization (Optional)",
    "text": "0.5 Fine-Tuning and Customization (Optional)\nUnsloth supports fine-tuning with LoRA or QLoRA. You could:\n\nFeed your company‚Äôs docs and fine-tune a chatbot\nInject private datasets and business-specific reasoning\nModify for multi-modal pipelines\n\nNot covered directly in the notebook‚Äîsee the next blog post for a tutorial!"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#performance-on-data-science-tasks",
    "href": "posts/series/vlm-qwen3-14b/index.html#performance-on-data-science-tasks",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.6 Performance on Data Science Tasks",
    "text": "0.6 Performance on Data Science Tasks\nQwen3-14B shines at:\n\nEDA Explanations\nMath QA\nPrompt Chaining\nCode Review\n\nprompt = \"Explain what this code does:\\ndf.groupby('region')['sales'].mean().sort_values(ascending=False)\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nAnswer:\nThis groups the dataframe df by the ‚Äòregion‚Äô column, computes the mean sales for each group, and sorts the results in descending order."
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#tldr",
    "href": "posts/series/vlm-qwen3-14b/index.html#tldr",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.7 TL;DR",
    "text": "0.7 TL;DR\nIf you‚Äôre a data scientist looking for:\n\nOpen, commercial-friendly LLMs\nStrong reasoning abilities\nEasy deployment via Colab\n\nQwen3-14B is worth your time."
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#bonus-prompt-engineering-tips",
    "href": "posts/series/vlm-qwen3-14b/index.html#bonus-prompt-engineering-tips",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.8 Bonus: Prompt Engineering Tips",
    "text": "0.8 Bonus: Prompt Engineering Tips\n\nBe Explicit: Add ‚ÄúStep-by-step reasoning‚Äù to prompts.\nUse System Prompts: Tailor tone and format.\nLimit Token Budget: Keep max tokens reasonable for speed + clarity."
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#why-qwen3-14b",
    "href": "posts/series/vlm-qwen3-14b/index.html#why-qwen3-14b",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.9 Why Qwen3-14B?",
    "text": "0.9 Why Qwen3-14B?\nBefore diving into code, here‚Äôs what makes Qwen3 interesting:\n\nSize and Performance: 14B parameters‚Äîbig enough to be powerful, small enough to run locally with quantization.\nOpen Weight License: Truly open, including for commercial use.\nReasoning Optimized: Trained with a focus on multi-step logical tasks, coding, math, and chain-of-thought."
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#setup-running-a-14b-model-in-google-colab",
    "href": "posts/series/vlm-qwen3-14b/index.html#setup-running-a-14b-model-in-google-colab",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.10 Setup: Running a 14B Model in Google Colab?",
    "text": "0.10 Setup: Running a 14B Model in Google Colab?\nThe notebook from Unsloth uses the HuggingFace transformers library, AutoGPTQ, and 4-bit quantized weights. That means we can run Qwen3-14B on a free Colab GPU (ideally a T4 or A100) without melting our RAM.\n!pip install --upgrade --quiet transformers accelerate auto-gptq\n\n\n\n\n\n\nImportant\n\n\n\n\nRun the cell above in your Colab notebook before anything else.\nFor best results, use a GPU runtime (T4 or A100 preferred)."
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#next-steps",
    "href": "posts/series/vlm-qwen3-14b/index.html#next-steps",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.11 Next Steps",
    "text": "0.11 Next Steps\n\nBack to VLM Series Overview\nFine-Tuning Qwen3-14B with Unsloth (next post)\nTry the Colab Notebook"
  },
  {
    "objectID": "posts/series/vlm-qwen3-14b/index.html#references",
    "href": "posts/series/vlm-qwen3-14b/index.html#references",
    "title": "Hands-On with Qwen3-14B: A Reasoning-Centric LLM for Data Scientists",
    "section": "0.12 References",
    "text": "0.12 References\n\nQwen3-14B on HuggingFace\nQwen3-14B Paper\nUnsloth Qwen3-14B Colab Notebook\nVLM Series Overview\n\n\n\n\n\n\n\nNote\n\n\n\nThis post is part of the VLM Series. Feedback and questions are welcome!"
  },
  {
    "objectID": "posts/series/tools/uv2.html",
    "href": "posts/series/tools/uv2.html",
    "title": "UV: Python Packaging, Reimagined (and Lightning Fast)",
    "section": "",
    "text": "If you‚Äôve spent any time wrangling Python environments, you know the drill: pip for installs, venv for isolation, pip-tools for reproducibility, pipx for CLI tools, and maybe poetry or pdm for project management. Each tool solves a piece of the puzzle, but the overall experience? Sometimes it feels like assembling IKEA furniture with parts from five different sets.\nBut what if there was a tool that unified all of this‚Äîfaster, simpler, and more powerful than anything before? Enter UV, a Rust-powered, developer-obsessed packaging tool from the folks at Astral (the same team behind the blazing-fast Ruff linter). UV isn‚Äôt just another tool‚Äîit‚Äôs a rethink of what Python packaging can be, and it‚Äôs so fast it changes what‚Äôs possible.\nLet‚Äôs take a journey through UV‚Äôs world, from the old way to the new, and see how it unlocks workflows you might never have considered.\n\n\n\nLet‚Äôs say you want to start a new Flask app that needs requests. Here‚Äôs the classic routine:\nmkdir old_way\ncd old_way\npython3 -m venv .venv\nsource .venv/bin/activate\npip install flask requests\npip freeze &gt; requirements.txt\ntouch main.py\nYou create a directory, set up a virtual environment, activate it, install dependencies, freeze them, and finally start coding. It works, but it‚Äôs a lot of steps‚Äîand a lot of mental context switching.\n\n\n\n\nWith UV, the workflow is radically streamlined:\nuv init new_app\ncd new_app\nuv add flask requests\nuv run main.py\n\nuv init scaffolds your project: .venv, .git, pyproject.toml, uv.lock, and more.\nuv add installs dependencies, updates your manifest, and pins everything in a lockfile‚Äîblazingly fast.\nuv run executes your code in the right environment, no manual activation needed. Delete .venv and run again? UV will recreate it, perfectly reproducible.\n\nWhy is this so powerful? - Unified interface: No more juggling five tools. - Rust-powered speed: Dependency resolution and installs happen in milliseconds, not seconds. - Built-in reproducibility: The uv.lock file guarantees identical environments everywhere. - Smart caching: Shared packages across projects save disk space and time.\n\n\n\n\nHere‚Äôs where UV‚Äôs speed unlocks new workflows. Imagine you have a script that needs a couple of packages, but you don‚Äôt want to pollute your main environment:\n# demo.py\nimport requests\nfrom rich.pretty import pprint\n\nresp = requests.get(\"https://peps.python.org/api/peps.json\")\ndata = resp.json()\npprint([(k, v[\"title\"]) for k, v in data.items()][:10])\nWith UV, you can run this script with its dependencies, instantly:\nuv run --with requests --with rich demo.py\nUV spins up a temporary, cached environment, installs the packages (or pulls from cache), runs your script, and cleans up. No manual setup, no leftover environments.\n\n\n\n\nThanks to PEP 723, you can now embed dependency metadata directly in your script:\n# richdemo.py\n# /// script\n# requires-python = \"&gt;=3.11\"\n# dependencies = [\n#   \"requests&lt;3\",\n#   \"rich\",\n# ]\n# ///\n\nimport requests\nfrom rich.pretty import pprint\n\nresp = requests.get(\"https://peps.python.org/api/peps.json\")\ndata = resp.json()\npprint([(k, v[\"title\"]) for k, v in data.items()][:10])\nNow, just run:\nuv run richdemo.py\nUV reads the inline metadata, sets up the right environment, and runs the script. This is a game-changer for sharing scripts‚Äîno more ‚Äúinstall these packages first‚Äù disclaimers. Your code is instantly reproducible and runnable.\n\n\n\n\nUV‚Äôs speed means you can do things that were previously too slow or clunky to consider. For example, you can dynamically run functions in isolated environments‚Äîgreat for benchmarking, testing across dependency versions, or isolating tricky code.\nHere‚Äôs a ‚Äúparty trick‚Äù using the uvtrick package to benchmark different versions of scikit-learn‚Äôs PCA:\n# sk.py\nfrom sklearn.datasets import make_regression\nX, y = make_regression(n_samples=10_000, n_features=10, random_state=42)\n\ndef bench(X, y):\n    from time import time\n    from sklearn.decomposition import PCA\n    tic = time()\n    pca = PCA(n_components=2).fit(X, y)\n    toc = time()\n    return toc - tic\n\nprint(\"Running scikit-learn benchmarks\")\nfor version in [\"1.4\", \"1.5\"]:\n    for i in range(4):\n        from uvtrick import Env\n        timed = Env(f\"scikit-learn={version}\").run(bench, X, y)\n        print(version, timed)\nRun it with:\nuv run --with uvtrick --with scikit-learn sk.py\nYou can compare performance across versions, all without polluting your main environment. This is just the beginning‚Äîthink about running quick experiments, cron jobs, or utility scripts, all with perfectly isolated, reproducible dependencies.\n\n\n\n\nUV isn‚Äôt just faster. It‚Äôs a catalyst for new ways of working: - Shareable, self-contained scripts: Perfect for blogs, tutorials, and ‚ÄúToday I Scripted‚Äù moments. - Instant, throwaway environments: Run experiments, benchmarks, or one-off tasks without setup overhead. - Reproducibility by default: No more ‚Äúworks on my machine‚Äù headaches. - Integrated tool management: Install, run, or even temporarily use CLI tools (uv tool install ruff, uvx ruff check)‚Äîall with the same speed and simplicity.\nAstral‚Äôs work on UV is a gift to the Python community. It‚Äôs not just about speed (though you‚Äôll love that). It‚Äôs about making Python development more joyful, more reliable, and more creative.\nSo next time you reach for pip, venv, or pipx, ask yourself: what could you do if your tools were 10x faster and 10x simpler? With UV, you don‚Äôt have to imagine‚Äîit‚Äôs already here.\nHappy coding!"
  },
  {
    "objectID": "posts/series/tools/uv2.html#the-old-way-a-familiar-and-tedious-dance",
    "href": "posts/series/tools/uv2.html#the-old-way-a-familiar-and-tedious-dance",
    "title": "UV: Python Packaging, Reimagined (and Lightning Fast)",
    "section": "",
    "text": "Let‚Äôs say you want to start a new Flask app that needs requests. Here‚Äôs the classic routine:\nmkdir old_way\ncd old_way\npython3 -m venv .venv\nsource .venv/bin/activate\npip install flask requests\npip freeze &gt; requirements.txt\ntouch main.py\nYou create a directory, set up a virtual environment, activate it, install dependencies, freeze them, and finally start coding. It works, but it‚Äôs a lot of steps‚Äîand a lot of mental context switching."
  },
  {
    "objectID": "posts/series/tools/uv2.html#enter-uv-the-lightning-path",
    "href": "posts/series/tools/uv2.html#enter-uv-the-lightning-path",
    "title": "UV: Python Packaging, Reimagined (and Lightning Fast)",
    "section": "",
    "text": "With UV, the workflow is radically streamlined:\nuv init new_app\ncd new_app\nuv add flask requests\nuv run main.py\n\nuv init scaffolds your project: .venv, .git, pyproject.toml, uv.lock, and more.\nuv add installs dependencies, updates your manifest, and pins everything in a lockfile‚Äîblazingly fast.\nuv run executes your code in the right environment, no manual activation needed. Delete .venv and run again? UV will recreate it, perfectly reproducible.\n\nWhy is this so powerful? - Unified interface: No more juggling five tools. - Rust-powered speed: Dependency resolution and installs happen in milliseconds, not seconds. - Built-in reproducibility: The uv.lock file guarantees identical environments everywhere. - Smart caching: Shared packages across projects save disk space and time."
  },
  {
    "objectID": "posts/series/tools/uv2.html#the-speed-demon-scripting-without-friction",
    "href": "posts/series/tools/uv2.html#the-speed-demon-scripting-without-friction",
    "title": "UV: Python Packaging, Reimagined (and Lightning Fast)",
    "section": "",
    "text": "Here‚Äôs where UV‚Äôs speed unlocks new workflows. Imagine you have a script that needs a couple of packages, but you don‚Äôt want to pollute your main environment:\n# demo.py\nimport requests\nfrom rich.pretty import pprint\n\nresp = requests.get(\"https://peps.python.org/api/peps.json\")\ndata = resp.json()\npprint([(k, v[\"title\"]) for k, v in data.items()][:10])\nWith UV, you can run this script with its dependencies, instantly:\nuv run --with requests --with rich demo.py\nUV spins up a temporary, cached environment, installs the packages (or pulls from cache), runs your script, and cleans up. No manual setup, no leftover environments."
  },
  {
    "objectID": "posts/series/tools/uv2.html#self-contained-scripts-pep-723-magic",
    "href": "posts/series/tools/uv2.html#self-contained-scripts-pep-723-magic",
    "title": "UV: Python Packaging, Reimagined (and Lightning Fast)",
    "section": "",
    "text": "Thanks to PEP 723, you can now embed dependency metadata directly in your script:\n# richdemo.py\n# /// script\n# requires-python = \"&gt;=3.11\"\n# dependencies = [\n#   \"requests&lt;3\",\n#   \"rich\",\n# ]\n# ///\n\nimport requests\nfrom rich.pretty import pprint\n\nresp = requests.get(\"https://peps.python.org/api/peps.json\")\ndata = resp.json()\npprint([(k, v[\"title\"]) for k, v in data.items()][:10])\nNow, just run:\nuv run richdemo.py\nUV reads the inline metadata, sets up the right environment, and runs the script. This is a game-changer for sharing scripts‚Äîno more ‚Äúinstall these packages first‚Äù disclaimers. Your code is instantly reproducible and runnable."
  },
  {
    "objectID": "posts/series/tools/uv2.html#beyond-the-basics-dynamic-environments-and-advanced-tricks",
    "href": "posts/series/tools/uv2.html#beyond-the-basics-dynamic-environments-and-advanced-tricks",
    "title": "UV: Python Packaging, Reimagined (and Lightning Fast)",
    "section": "",
    "text": "UV‚Äôs speed means you can do things that were previously too slow or clunky to consider. For example, you can dynamically run functions in isolated environments‚Äîgreat for benchmarking, testing across dependency versions, or isolating tricky code.\nHere‚Äôs a ‚Äúparty trick‚Äù using the uvtrick package to benchmark different versions of scikit-learn‚Äôs PCA:\n# sk.py\nfrom sklearn.datasets import make_regression\nX, y = make_regression(n_samples=10_000, n_features=10, random_state=42)\n\ndef bench(X, y):\n    from time import time\n    from sklearn.decomposition import PCA\n    tic = time()\n    pca = PCA(n_components=2).fit(X, y)\n    toc = time()\n    return toc - tic\n\nprint(\"Running scikit-learn benchmarks\")\nfor version in [\"1.4\", \"1.5\"]:\n    for i in range(4):\n        from uvtrick import Env\n        timed = Env(f\"scikit-learn={version}\").run(bench, X, y)\n        print(version, timed)\nRun it with:\nuv run --with uvtrick --with scikit-learn sk.py\nYou can compare performance across versions, all without polluting your main environment. This is just the beginning‚Äîthink about running quick experiments, cron jobs, or utility scripts, all with perfectly isolated, reproducible dependencies."
  },
  {
    "objectID": "posts/series/tools/uv2.html#the-broader-implications-rethinking-python-workflows",
    "href": "posts/series/tools/uv2.html#the-broader-implications-rethinking-python-workflows",
    "title": "UV: Python Packaging, Reimagined (and Lightning Fast)",
    "section": "",
    "text": "UV isn‚Äôt just faster. It‚Äôs a catalyst for new ways of working: - Shareable, self-contained scripts: Perfect for blogs, tutorials, and ‚ÄúToday I Scripted‚Äù moments. - Instant, throwaway environments: Run experiments, benchmarks, or one-off tasks without setup overhead. - Reproducibility by default: No more ‚Äúworks on my machine‚Äù headaches. - Integrated tool management: Install, run, or even temporarily use CLI tools (uv tool install ruff, uvx ruff check)‚Äîall with the same speed and simplicity.\nAstral‚Äôs work on UV is a gift to the Python community. It‚Äôs not just about speed (though you‚Äôll love that). It‚Äôs about making Python development more joyful, more reliable, and more creative.\nSo next time you reach for pip, venv, or pipx, ask yourself: what could you do if your tools were 10x faster and 10x simpler? With UV, you don‚Äôt have to imagine‚Äîit‚Äôs already here.\nHappy coding!"
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "",
    "text": "Imagine you‚Äôre at a family reunion. Aunts, uncles, cousins‚Äîeveryone‚Äôs mingling around the potato salad, sharing stories about their perfectly average lives. But then, there‚Äôs cousin Eddie. While everyone else talks about their 9-to-5 jobs, Eddie casually mentions he just returned from six months living in an underwater cave ‚Äúresearching mermaid sociology.‚Äù\nThat, my friends, is an anomaly.\nAnd just as you can spot Eddie from across the room (probably wearing socks with sandals), computers can be trained to spot anomalies in data. Let‚Äôs dive into the fascinating world of Anomaly Detection‚Äîwith absolutely minimal math and maximum fun.\n\n\n\n\n\n\nNote\n\n\n\nAn anomaly is simply a data point that significantly deviates from the expected pattern or behavior of the majority of data.\n\n\nAn anomaly is basically the weirdo in your dataset‚Äîthe point that doesn‚Äôt follow the rules everyone else seems to be playing by. In the world of data science, identifying these oddballs can be incredibly valuable:\n\nIt could be fraudulent credit card activity (‚ÄúHmm, you‚Äôve never bought anything in Kazakhstan before, and now there‚Äôs a $5,000 purchase at 3 AM?‚Äù)\nA manufacturing defect (‚ÄúThis widget is supposed to be 2 inches, not 7 feet tall‚Äù)\nA potential new scientific discovery (‚ÄúWait, this star isn‚Äôt behaving like any other star we‚Äôve seen‚Äù)\n\nBut how do we teach computers to find these needles in our digital haystacks? Enter: Density Estimation."
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html#whats-an-anomaly-anyway",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html#whats-an-anomaly-anyway",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "",
    "text": "Imagine you‚Äôre at a family reunion. Aunts, uncles, cousins‚Äîeveryone‚Äôs mingling around the potato salad, sharing stories about their perfectly average lives. But then, there‚Äôs cousin Eddie. While everyone else talks about their 9-to-5 jobs, Eddie casually mentions he just returned from six months living in an underwater cave ‚Äúresearching mermaid sociology.‚Äù\nThat, my friends, is an anomaly.\nAnd just as you can spot Eddie from across the room (probably wearing socks with sandals), computers can be trained to spot anomalies in data. Let‚Äôs dive into the fascinating world of Anomaly Detection‚Äîwith absolutely minimal math and maximum fun.\n\n\n\n\n\n\nNote\n\n\n\nAn anomaly is simply a data point that significantly deviates from the expected pattern or behavior of the majority of data.\n\n\nAn anomaly is basically the weirdo in your dataset‚Äîthe point that doesn‚Äôt follow the rules everyone else seems to be playing by. In the world of data science, identifying these oddballs can be incredibly valuable:\n\nIt could be fraudulent credit card activity (‚ÄúHmm, you‚Äôve never bought anything in Kazakhstan before, and now there‚Äôs a $5,000 purchase at 3 AM?‚Äù)\nA manufacturing defect (‚ÄúThis widget is supposed to be 2 inches, not 7 feet tall‚Äù)\nA potential new scientific discovery (‚ÄúWait, this star isn‚Äôt behaving like any other star we‚Äôve seen‚Äù)\n\nBut how do we teach computers to find these needles in our digital haystacks? Enter: Density Estimation."
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html#density-estimation-the-wheres-everyone-hanging-out-approach",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html#density-estimation-the-wheres-everyone-hanging-out-approach",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "0.2 Density Estimation: The ‚ÄúWhere‚Äôs Everyone Hanging Out?‚Äù Approach",
    "text": "0.2 Density Estimation: The ‚ÄúWhere‚Äôs Everyone Hanging Out?‚Äù Approach\nImagine a crowded beach on a hot summer day. People naturally cluster in certain areas‚Äînear the ice cream stand, in the shade of palm trees, or in the water. If you spotted someone standing alone in the blazing sun far from everyone else, you‚Äôd think, ‚ÄúWhat‚Äôs that person doing all the way over there?‚Äù\nThis is essentially what density estimation does. It figures out where most of your data ‚Äúhangs out,‚Äù and then can identify points that are chilling in low-density neighborhoods.\n\n\n\n\n\nFigure¬†1: Visualizing data density - notice the outlier in the lower left"
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html#the-kernel-density-estimator-kde-spreading-good-vibes",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html#the-kernel-density-estimator-kde-spreading-good-vibes",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "0.3 The Kernel Density Estimator (KDE): Spreading Good Vibes",
    "text": "0.3 The Kernel Density Estimator (KDE): Spreading Good Vibes\nLet‚Äôs break down Kernel Density Estimation using another analogy:\nImagine each data point is a streetlight on a dark road. Each light casts a circular glow around it. Where many lights are close together, their glows overlap, creating brightly lit areas. Where lights are sparse, you get dimmer areas.\nIn KDE: - Each data point spreads a little ‚Äúprobability mass‚Äù around itself (the streetlight‚Äôs glow) - The shape of this spread is determined by something called a kernel function (the shape of the light‚Äôs glow) - Areas where many points overlap have high density (brightly lit areas) - Areas with few or no points have low density (dark areas)\nAnd anomalies? They‚Äôre hanging out in the dark, of course.\n\n\nCommon kernel functions include Gaussian (bell-shaped), Top Hat (flat circle), and Cosine (smooth hill), but the choice of kernel is less important than the bandwidth parameter."
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html#sec-bandwidth",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html#sec-bandwidth",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "0.4 The All-Important Bandwidth: Finding the Sweet Spot",
    "text": "0.4 The All-Important Bandwidth: Finding the Sweet Spot\nHere‚Äôs where things get interesting. The most crucial parameter in KDE is something called ‚Äúbandwidth.‚Äù Think of it as determining how far each data point‚Äôs influence reaches.\nToo small a bandwidth? Each point barely influences its surroundings, like tiny flashlights that only illuminate a foot around them. This creates a spiky, disconnected map that‚Äôs too sensitive to individual points.\nToo large a bandwidth? Each point‚Äôs influence spreads far and wide, like massive floodlights. Everything gets washed out, and you lose the ability to see interesting patterns.\nIt‚Äôs like making mashed potatoes: - Too little mashing (small bandwidth): You‚Äôve got chunky potatoes with distinct pieces - Too much mashing (large bandwidth): You‚Äôve made potato soup\nThe perfect bandwidth gives you that smooth, creamy consistency where everything comes together just right.\n\nUnder-smoothedJust RightOver-smoothed\n\n\n\n\n\nToo small bandwidth creates spiky estimates\n\n\n\n\n\n\n\nOptimal bandwidth balances detail and smoothness\n\n\n\n\n\n\n\nToo large bandwidth washes out important features"
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html#the-curse-of-dimensionality-when-more-is-less",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html#the-curse-of-dimensionality-when-more-is-less",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "0.5 The Curse of Dimensionality: When More is Less",
    "text": "0.5 The Curse of Dimensionality: When More is Less\nHere‚Äôs where our anomaly detector starts sweating nervously. As we add more dimensions (variables) to our data, things get weird fast.\nImagine playing hide-and-seek in: 1. A hallway (1D): Pretty easy to find someone 2. A field (2D): Harder, but still manageable 3. A multi-story building (3D): Much more challenging 4. A 100-dimensional hypercube: screams internally\nThe ‚Äúcurse of dimensionality‚Äù means that as dimensions increase, data becomes increasingly sparse, making it harder to estimate densities accurately. It‚Äôs like trying to find a friend in a city where each person can hide not just on any street or in any building, but in any parallel universe.\nTo maintain the same quality of estimation, we need exponentially more data as dimensions increase. No wonder our poor algorithm is cursing!\n\n\n\n\n\nFigure¬†2: Sample complexity grows exponentially with dimensions\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAccording to Stone‚Äôs theorem (1982), the convergence rate of KDE is highly dependent on dimensionality. To achieve the same estimation quality in higher dimensions, you need exponentially more samples!"
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html#when-anomalies-crash-your-training-party",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html#when-anomalies-crash-your-training-party",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "0.6 When Anomalies Crash Your Training Party",
    "text": "0.6 When Anomalies Crash Your Training Party\nThere‚Äôs a delicious irony in anomaly detection: if anomalies sneak into your training data, they can mess up your detector‚Äôs ability to find other anomalies. It‚Äôs like hiring a security guard who can‚Äôt tell the difference between a bank robber and a bank teller.\nStandard KDE isn‚Äôt robust against these pesky infiltrators. Its ‚Äúbreakdown point‚Äù (the fraction of data that needs to change to completely throw off your estimate) is close to zero. That‚Äôs like having a security system that fails if even one person tampers with it!"
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html#coming-to-the-rescue-robust-estimation",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html#coming-to-the-rescue-robust-estimation",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "0.7 Coming to the Rescue: Robust Estimation",
    "text": "0.7 Coming to the Rescue: Robust Estimation\nFear not! Robust statistics comes to our rescue with some clever techniques:\n\n0.7.1 The Median of Means Approach\nImagine you‚Äôre calculating the average height of people in a room, but Shaquille O‚ÄôNeal walks in. Suddenly, your average is way off! Instead, you could: 1. Split people into groups 2. Calculate the average height of each group 3. Take the median (middle value) of those averages\nThis ‚Äúmedian of means‚Äù approach is less affected by extreme values. If Shaq is in just one group, the other groups‚Äô averages remain unaffected, and the median won‚Äôt change much.\n\n\n0.7.2 M-estimation: Changing the Rules of the Game\nStandard estimation methods give equal importance to all points, including potential anomalies. M-estimation changes this by using special loss functions:\n\n0.7.2.1 Huber Loss: The ‚ÄúI‚Äôll Only Tolerate So Much‚Äù Approach\nHuber loss is like a parent‚Äôs patience: - For small deviations: ‚ÄúThat‚Äôs fine, I‚Äôm cool with that‚Äù (quadratic behavior) - For large deviations: ‚ÄúNope, I‚Äôm not getting more upset than this‚Äù (linear behavior)\nThis limits the influence of outliers without ignoring them completely.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Example of how Huber loss works\ndef huber_loss(x, delta=1.0):\n    if abs(x) &lt;= delta:\n        return 0.5 * x**2\n    else:\n        return delta * (abs(x) - 0.5 * delta)\n\nx = np.linspace(-3, 3, 1000)\ny = [huber_loss(val) for val in x]\n\nplt.figure(figsize=(8, 4))\nplt.plot(x, y)\nplt.title(\"Huber Loss Function\")\nplt.xlabel(\"Error\")\nplt.ylabel(\"Loss\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\nFigure¬†3: Visualization of Huber Loss function\n\n\n\n\n\n\n0.7.2.2 Hampel Loss: The ‚ÄúThree Strikes‚Äù System\nHampel loss takes it further with two thresholds: - Close points: Full attention (quadratic) - Medium-distance points: Limited attention (linear) - Far away points: Fixed penalty (constant)\nIt‚Äôs like saying: ‚ÄúIf you‚Äôre way out there doing your own thing, I‚Äôll acknowledge you exist, but I won‚Äôt let you control the entire situation.‚Äù"
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html#the-real-world-test-house-prices",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html#the-real-world-test-house-prices",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "0.8 The Real-World Test: House Prices",
    "text": "0.8 The Real-World Test: House Prices\nWhen applied to real-world data like house prices, these methods show their worth. Imagine trying to determine if a $10 million listing in a neighborhood of $300,000 homes is an anomaly or if it‚Äôs legitimately worth that much.\nStandard KDE might get thrown off by a few unusual listings in the training data. But robust methods, especially using Hampel loss, consistently outperform when the data contains those sneaky anomalies‚Äîparticularly when they make up less than 10% of the data (which is usually the case).\n\n\nCode\n# Python placeholder for performance comparison table\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = {\n    'Method': ['Standard KDE', 'Median of Means', 'Huber Loss', 'Hampel Loss'],\n    'AUC': [0.82, 0.88, 0.90, 0.93],\n    'Robust to Outliers': ['No', 'Medium', 'Yes', 'Yes (Best)']\n}\ndf = pd.DataFrame(data)\n\nfig, ax = plt.subplots(figsize=(7, 2))\nax.axis('off')\ntable = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\ntable.auto_set_font_size(False)\ntable.set_fontsize(12)\ntable.scale(1.2, 1.2)\nplt.title('Performance of different methods on house price anomaly detection', pad=20)\nplt.show()\n\n\n\n?(caption)"
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html#the-bottom-line",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html#the-bottom-line",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "0.9 The Bottom Line",
    "text": "0.9 The Bottom Line\nFinding anomalies is both art and science. Density estimation gives us a powerful framework, but we need to: - Choose the right bandwidth (not too smooth, not too chunky) - Be wary of high-dimensional data (the curse is real!) - Use robust methods to handle contaminated training data (Hampel loss for the win!)\nSo next time you spot cousin Eddie at the family reunion, remember‚Äîyou‚Äôre performing your own personal anomaly detection. Just be glad you don‚Äôt have to calculate his probability density function to know something‚Äôs a bit off!\n\n\n\n\n\n\nWhat‚Äôs Next?\n\n\n\nStay tuned for our next post: Finding anomalies by isolation‚Äîor as I like to call it, ‚ÄúThe Social Distancing Approach to Anomaly Detection.‚Äù"
  },
  {
    "objectID": "posts/series/anomaly-detection/finding-the-oddballs.html#references",
    "href": "posts/series/anomaly-detection/finding-the-oddballs.html#references",
    "title": "Finding the Oddballs: A Not-So-Technical Guide to Anomaly Detection(Density Estimation and Robustness)",
    "section": "0.10 References",
    "text": "0.10 References"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "",
    "text": "Picture this: It‚Äôs 2010, and computer vision researchers are frustrated. They‚Äôve spent decades perfecting edge detection, feature matching, and object recognition. Their algorithms can find corners, match keypoints, and even stitch panoramas.\nBut there‚Äôs one problem they can‚Äôt solve: telling cats from dogs.\nSeriously! While a 3-year-old child could easily distinguish between cats and dogs, the best computer vision systems struggled with this ‚Äúsimple‚Äù task. The classical approach required hand-crafting features for every possible variation‚Äîdifferent breeds, lighting conditions, poses, backgrounds. It was like trying to write rules for every possible way a cat could look. Impossible!\nThen something revolutionary happened‚Ä¶"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#the-great-computer-vision-crisis-of-2010",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#the-great-computer-vision-crisis-of-2010",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "",
    "text": "Picture this: It‚Äôs 2010, and computer vision researchers are frustrated. They‚Äôve spent decades perfecting edge detection, feature matching, and object recognition. Their algorithms can find corners, match keypoints, and even stitch panoramas.\nBut there‚Äôs one problem they can‚Äôt solve: telling cats from dogs.\nSeriously! While a 3-year-old child could easily distinguish between cats and dogs, the best computer vision systems struggled with this ‚Äúsimple‚Äù task. The classical approach required hand-crafting features for every possible variation‚Äîdifferent breeds, lighting conditions, poses, backgrounds. It was like trying to write rules for every possible way a cat could look. Impossible!\nThen something revolutionary happened‚Ä¶"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#the-imagenet-moment-2012",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#the-imagenet-moment-2012",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.2 The ImageNet Moment: 2012",
    "text": "0.2 The ImageNet Moment: 2012\nIn 2012, a team led by Alex Krizhevsky entered the ImageNet competition with something called AlexNet‚Äîa deep neural network. The results were shocking:\n\nPrevious best accuracy: 74.3%\nAlexNet accuracy: 84.7%\nImprovement: A massive 10+ percentage point jump!\n\nThis wasn‚Äôt just an incremental improvement‚Äîit was a paradigm shift. Deep learning had arrived, and computer vision would never be the same."
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#what-makes-deep-learning-different",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#what-makes-deep-learning-different",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.3 What Makes Deep Learning Different?",
    "text": "0.3 What Makes Deep Learning Different?\nLet‚Äôs understand why neural networks succeeded where classical methods struggled:\n\n0.3.1 Classical Approach: Hand-Crafted Features\n# Classical computer vision pipeline\ndef classical_cat_detector(image):\n    # Step 1: Extract hand-crafted features\n    edges = detect_edges(image)\n    corners = detect_corners(image)\n    textures = analyze_textures(image)\n    \n    # Step 2: Combine features with rules\n    if (pointy_ears and whiskers and fur_texture):\n        return \"cat\"\n    else:\n        return \"not_cat\"\nProblems: - Features had to be manually designed - Rules were brittle and specific - Couldn‚Äôt adapt to new variations - Required domain expertise\n\n\n0.3.2 Deep Learning Approach: Learned Features\n# Deep learning pipeline\ndef deep_cat_detector(image):\n    # The network learns its own features!\n    features = neural_network.extract_features(image)\n    prediction = neural_network.classify(features)\n    return prediction\nAdvantages: - Features are learned automatically - Adapts to data variations - Improves with more data - Works across different domains"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#your-first-neural-network",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#your-first-neural-network",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.4 Your First Neural Network",
    "text": "0.4 Your First Neural Network\nLet‚Äôs build a simple neural network to understand the magic:\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Simple neural network for image classification\nclass SimpleNet(nn.Module):\n    def __init__(self, num_classes=2):  # 2 classes: cat vs dog\n        super(SimpleNet, self).__init__()\n        \n        # Flatten 224x224x3 image to 150,528 features\n        self.flatten = nn.Flatten()\n        \n        # Simple fully connected layers\n        self.fc1 = nn.Linear(224 * 224 * 3, 512)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        \n        self.fc2 = nn.Linear(512, 128)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.5)\n        \n        self.fc3 = nn.Linear(128, num_classes)\n    \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.dropout1(self.relu1(self.fc1(x)))\n        x = self.dropout2(self.relu2(self.fc2(x)))\n        x = self.fc3(x)\n        return x\n\n# Create the network\nsimple_net = SimpleNet(num_classes=2)\nprint(\"Simple Neural Network:\")\nprint(simple_net)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in simple_net.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n\nSimple Neural Network:\nSimpleNet(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (fc1): Linear(in_features=150528, out_features=512, bias=True)\n  (relu1): ReLU()\n  (dropout1): Dropout(p=0.5, inplace=False)\n  (fc2): Linear(in_features=512, out_features=128, bias=True)\n  (relu2): ReLU()\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (fc3): Linear(in_features=128, out_features=2, bias=True)\n)\n\nTotal parameters: 77,136,770\n\n\nüéØ Try it yourself! Open in Colab"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#the-convolutional-revolution",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#the-convolutional-revolution",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.5 The Convolutional Revolution",
    "text": "0.5 The Convolutional Revolution\nBut wait‚Äîthere‚Äôs a problem with our simple network. It treats each pixel independently, ignoring spatial relationships. That‚Äôs like reading a book by looking at each letter separately!\nEnter Convolutional Neural Networks (CNNs)‚Äînetworks designed specifically for images:\n\n\nCode\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ConvNet, self).__init__()\n        \n        # Convolutional layers (feature extractors)\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        \n        # Pooling layers (downsampling)\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # Fully connected layers (classifier)\n        self.fc1 = nn.Linear(128 * 28 * 28, 512)  # 224/8 = 28 after 3 pooling layers\n        self.fc2 = nn.Linear(512, num_classes)\n        \n        # Activation and regularization\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, x):\n        # Feature extraction\n        x = self.pool(self.relu(self.conv1(x)))  # 224x224 -&gt; 112x112\n        x = self.pool(self.relu(self.conv2(x)))  # 112x112 -&gt; 56x56\n        x = self.pool(self.relu(self.conv3(x)))  # 56x56 -&gt; 28x28\n        \n        # Flatten and classify\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.dropout(self.relu(self.fc1(x)))\n        x = self.fc2(x)\n        \n        return x\n\n# Create CNN\ncnn = ConvNet(num_classes=2)\nprint(\"Convolutional Neural Network:\")\nprint(cnn)\n\n# Count parameters\ncnn_params = sum(p.numel() for p in cnn.parameters())\nprint(f\"\\nCNN parameters: {cnn_params:,}\")\nprint(f\"Simple net parameters: {total_params:,}\")\nprint(f\"CNN has {(total_params - cnn_params) / total_params * 100:.1f}% fewer parameters!\")\n\n\nConvolutional Neural Network:\nConvNet(\n  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=2, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.5, inplace=False)\n)\n\nCNN parameters: 51,475,010\nSimple net parameters: 77,136,770\nCNN has 33.3% fewer parameters!"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#understanding-convolutions-the-sliding-window",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#understanding-convolutions-the-sliding-window",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.6 Understanding Convolutions: The Sliding Window",
    "text": "0.6 Understanding Convolutions: The Sliding Window\nLet‚Äôs visualize what convolutions actually do:\n\n\nCode\ndef visualize_convolution():\n    \"\"\"Show how convolution works\"\"\"\n    # Create a simple image\n    image = np.array([\n        [1, 1, 1, 0, 0],\n        [0, 1, 1, 1, 0],\n        [0, 0, 1, 1, 1],\n        [0, 0, 1, 1, 0],\n        [0, 1, 1, 0, 0]\n    ])\n    \n    # Edge detection kernel\n    kernel = np.array([\n        [-1, -1, -1],\n        [-1,  8, -1],\n        [-1, -1, -1]\n    ])\n    \n    # Apply convolution manually\n    result = np.zeros((3, 3))\n    \n    plt.figure(figsize=(15, 5))\n    \n    # Show original image\n    plt.subplot(1, 4, 1)\n    plt.imshow(image, cmap='gray')\n    plt.title(\"Original Image\")\n    plt.axis('off')\n    \n    # Show kernel\n    plt.subplot(1, 4, 2)\n    plt.imshow(kernel, cmap='RdBu')\n    plt.title(\"Edge Detection Kernel\")\n    plt.axis('off')\n    \n    # Show convolution process\n    for i in range(3):\n        for j in range(3):\n            patch = image[i:i+3, j:j+3]\n            result[i, j] = np.sum(patch * kernel)\n    \n    plt.subplot(1, 4, 3)\n    plt.imshow(result, cmap='gray')\n    plt.title(\"Convolution Result\")\n    plt.axis('off')\n    \n    # Show using PyTorch\n    image_tensor = torch.FloatTensor(image).unsqueeze(0).unsqueeze(0)\n    kernel_tensor = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n    \n    conv_layer = nn.Conv2d(1, 1, 3, bias=False)\n    conv_layer.weight.data = kernel_tensor\n    \n    pytorch_result = conv_layer(image_tensor).squeeze().detach().numpy()\n    \n    plt.subplot(1, 4, 4)\n    plt.imshow(pytorch_result, cmap='gray')\n    plt.title(\"PyTorch Result\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_convolution()"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#transfer-learning-standing-on-giants-shoulders",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#transfer-learning-standing-on-giants-shoulders",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.7 Transfer Learning: Standing on Giants‚Äô Shoulders",
    "text": "0.7 Transfer Learning: Standing on Giants‚Äô Shoulders\nHere‚Äôs the secret that makes deep learning practical: Transfer Learning. Instead of training from scratch, we use pre-trained models and adapt them to our needs:\n\n\nCode\nimport torchvision.models as models\n\n# Load a pre-trained ResNet model\npretrained_model = models.resnet18(pretrained=True)\nprint(\"Pre-trained ResNet-18:\")\nprint(pretrained_model)\n\n# Modify for our task (cat vs dog classification)\nnum_classes = 2\npretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, num_classes)\n\nprint(f\"\\nModified final layer for {num_classes} classes\")\nprint(f\"Final layer: {pretrained_model.fc}\")\n\n# Freeze early layers (optional)\nfor param in pretrained_model.parameters():\n    param.requires_grad = False\n\n# Only train the final layer\nfor param in pretrained_model.fc.parameters():\n    param.requires_grad = True\n\ntrainable_params = sum(p.numel() for p in pretrained_model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in pretrained_model.parameters())\n\nprint(f\"\\nTrainable parameters: {trainable_params:,}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Training only {trainable_params/total_params*100:.1f}% of parameters!\")\n\n\nPre-trained ResNet-18:\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\nModified final layer for 2 classes\nFinal layer: Linear(in_features=512, out_features=2, bias=True)\n\nTrainable parameters: 1,026\nTotal parameters: 11,177,538\nTraining only 0.0% of parameters!"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#your-first-image-classifier",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#your-first-image-classifier",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.8 Your First Image Classifier",
    "text": "0.8 Your First Image Classifier\nLet‚Äôs build a complete image classification system:\n\n\nCode\nclass ImageClassifier:\n    def __init__(self, model_name='resnet18', num_classes=2):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {self.device}\")\n        \n        # Load pre-trained model\n        if model_name == 'resnet18':\n            self.model = models.resnet18(pretrained=True)\n            self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n        \n        self.model = self.model.to(self.device)\n        \n        # Define transforms\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n        \n        self.classes = ['cat', 'dog']  # Update based on your classes\n    \n    def predict(self, image):\n        \"\"\"Predict class of a single image\"\"\"\n        self.model.eval()\n        \n        # Preprocess image\n        if isinstance(image, np.ndarray):\n            from PIL import Image\n            image = Image.fromarray(image)\n        \n        input_tensor = self.transform(image).unsqueeze(0).to(self.device)\n        \n        # Make prediction\n        with torch.no_grad():\n            outputs = self.model(input_tensor)\n            probabilities = torch.softmax(outputs, dim=1)\n            predicted_class = torch.argmax(probabilities, dim=1).item()\n            confidence = probabilities[0][predicted_class].item()\n        \n        return {\n            'class': self.classes[predicted_class],\n            'confidence': confidence,\n            'probabilities': probabilities.cpu().numpy()[0]\n        }\n    \n    def predict_batch(self, images):\n        \"\"\"Predict classes for multiple images\"\"\"\n        results = []\n        for image in images:\n            result = self.predict(image)\n            results.append(result)\n        return results\n\n# Create classifier\nclassifier = ImageClassifier()\n\n# Test with a sample image (you would load your own)\ndef test_classifier(image_path):\n    from PIL import Image\n    \n    # Load image\n    image = Image.open(image_path)\n    \n    # Make prediction\n    result = classifier.predict(image)\n    \n    # Display result\n    plt.figure(figsize=(10, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(image)\n    plt.title(f\"Input Image\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.bar(classifier.classes, result['probabilities'])\n    plt.title(f\"Prediction: {result['class']} ({result['confidence']:.2f})\")\n    plt.ylabel(\"Probability\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return result\n\n# Test the classifier\n# result = test_classifier('your_image.jpg')\n\n\nUsing device: cuda"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#visualizing-what-neural-networks-learn",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#visualizing-what-neural-networks-learn",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.9 Visualizing What Neural Networks Learn",
    "text": "0.9 Visualizing What Neural Networks Learn\nOne of the coolest things about deep learning is visualizing what the network actually learns:\n\n\nCode\ndef visualize_filters(model, layer_name='conv1'):\n    \"\"\"Visualize the filters learned by a convolutional layer\"\"\"\n    \n    # Get the layer\n    layer = dict(model.named_modules())[layer_name]\n    \n    # Get the weights\n    weights = layer.weight.data.cpu()\n    \n    # Normalize weights for visualization\n    weights = (weights - weights.min()) / (weights.max() - weights.min())\n    \n    # Plot filters\n    num_filters = min(16, weights.shape[0])  # Show first 16 filters\n    \n    plt.figure(figsize=(12, 8))\n    \n    for i in range(num_filters):\n        plt.subplot(4, 4, i + 1)\n        \n        if weights.shape[1] == 3:  # RGB filters\n            # Transpose from (C, H, W) to (H, W, C)\n            filter_img = weights[i].permute(1, 2, 0)\n            plt.imshow(filter_img)\n        else:  # Grayscale filters\n            plt.imshow(weights[i, 0], cmap='gray')\n        \n        plt.title(f\"Filter {i+1}\")\n        plt.axis('off')\n    \n    plt.suptitle(f\"Learned Filters in {layer_name}\")\n    plt.tight_layout()\n    plt.show()\n\n# Visualize filters from our pre-trained model\nvisualize_filters(pretrained_model, 'conv1')"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#feature-maps-seeing-through-the-networks-eyes",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#feature-maps-seeing-through-the-networks-eyes",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.10 Feature Maps: Seeing Through the Network‚Äôs Eyes",
    "text": "0.10 Feature Maps: Seeing Through the Network‚Äôs Eyes\n\n\nCode\ndef visualize_feature_maps(model, image, layer_name='layer1'):\n    \"\"\"Visualize feature maps from a specific layer\"\"\"\n    \n    # Hook to capture feature maps\n    feature_maps = []\n    \n    def hook_fn(module, input, output):\n        feature_maps.append(output.cpu())\n    \n    # Register hook\n    layer = dict(model.named_modules())[layer_name]\n    hook = layer.register_forward_hook(hook_fn)\n    \n    # Forward pass\n    model.eval()\n    with torch.no_grad():\n        _ = model(image.unsqueeze(0))\n    \n    # Remove hook\n    hook.remove()\n    \n    # Get feature maps\n    fmaps = feature_maps[0].squeeze(0)  # Remove batch dimension\n    \n    # Plot feature maps\n    num_maps = min(16, fmaps.shape[0])\n    \n    plt.figure(figsize=(12, 8))\n    \n    for i in range(num_maps):\n        plt.subplot(4, 4, i + 1)\n        plt.imshow(fmaps[i], cmap='viridis')\n        plt.title(f\"Feature Map {i+1}\")\n        plt.axis('off')\n    \n    plt.suptitle(f\"Feature Maps from {layer_name}\")\n    plt.tight_layout()\n    plt.show()\n\n# Create a sample input\nsample_input = torch.randn(3, 224, 224)\nvisualize_feature_maps(pretrained_model, sample_input)"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#the-deep-learning-advantage-why-it-works",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#the-deep-learning-advantage-why-it-works",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.11 The Deep Learning Advantage: Why It Works",
    "text": "0.11 The Deep Learning Advantage: Why It Works\nLet‚Äôs compare classical vs deep learning approaches on a real problem:\n\n\nCode\ndef compare_approaches(image):\n    \"\"\"Compare classical and deep learning approaches\"\"\"\n    \n    plt.figure(figsize=(15, 10))\n    \n    # Original image\n    plt.subplot(2, 3, 1)\n    plt.imshow(image)\n    plt.title(\"Original Image\")\n    plt.axis('off')\n    \n    # Classical approach: hand-crafted features\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    \n    # Edge features\n    edges = cv2.Canny(gray, 50, 150)\n    plt.subplot(2, 3, 2)\n    plt.imshow(edges, cmap='gray')\n    plt.title(\"Classical: Edge Features\")\n    plt.axis('off')\n    \n    # Texture features (using LBP-like approach)\n    from skimage.feature import local_binary_pattern\n    lbp = local_binary_pattern(gray, 24, 8, method='uniform')\n    plt.subplot(2, 3, 3)\n    plt.imshow(lbp, cmap='gray')\n    plt.title(\"Classical: Texture Features\")\n    plt.axis('off')\n    \n    # Deep learning approach: learned features\n    # Transform image for the model\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    input_tensor = transform(image).unsqueeze(0)\n    \n    # Get feature maps from different layers\n    feature_maps = []\n    \n    def get_features(name):\n        def hook(model, input, output):\n            feature_maps.append((name, output.cpu()))\n        return hook\n    \n    # Register hooks\n    pretrained_model.layer1.register_forward_hook(get_features('Low-level'))\n    pretrained_model.layer3.register_forward_hook(get_features('Mid-level'))\n    pretrained_model.layer4.register_forward_hook(get_features('High-level'))\n    \n    # Forward pass\n    pretrained_model.eval()\n    with torch.no_grad():\n        _ = pretrained_model(input_tensor)\n    \n    # Visualize learned features\n    for i, (name, fmaps) in enumerate(feature_maps):\n        plt.subplot(2, 3, 4 + i)\n        # Average across channels for visualization\n        avg_fmap = torch.mean(fmaps.squeeze(0), dim=0)\n        plt.imshow(avg_fmap, cmap='viridis')\n        plt.title(f\"Deep Learning: {name} Features\")\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Compare approaches (you would use your own image)\n# compare_approaches(your_image)"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#why-deep-learning-won",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#why-deep-learning-won",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.12 Why Deep Learning Won",
    "text": "0.12 Why Deep Learning Won\nHere‚Äôs why deep learning revolutionized computer vision:\n\n0.12.1 1. Automatic Feature Learning\n\nNo need to hand-craft features\nLearns optimal representations for the task\nAdapts to data variations\n\n\n\n0.12.2 2. Hierarchical Representations\n\nLow-level features (edges, textures)\nMid-level features (parts, patterns)\n\nHigh-level features (objects, concepts)\n\n\n\n0.12.3 3. End-to-End Learning\n\nOptimizes entire pipeline together\nFeatures and classifier learned jointly\nBetter overall performance\n\n\n\n0.12.4 4. Scalability\n\nPerformance improves with more data\nCan handle complex, real-world variations\nGeneralizes across domains"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#the-modern-deep-learning-pipeline",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#the-modern-deep-learning-pipeline",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.13 The Modern Deep Learning Pipeline",
    "text": "0.13 The Modern Deep Learning Pipeline\n\n\nCode\nclass ModernVisionPipeline:\n    def __init__(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # 1. Data preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(p=0.5),  # Data augmentation\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n        \n        # 2. Model architecture\n        self.model = models.resnet50(pretrained=True)\n        \n        # 3. Transfer learning setup\n        # Freeze early layers\n        for param in list(self.model.parameters())[:-10]:\n            param.requires_grad = False\n        \n        # 4. Optimizer and loss\n        self.optimizer = optim.Adam(\n            filter(lambda p: p.requires_grad, self.model.parameters()),\n            lr=0.001\n        )\n        self.criterion = nn.CrossEntropyLoss()\n    \n    def train_step(self, images, labels):\n        \"\"\"Single training step\"\"\"\n        self.model.train()\n        \n        # Forward pass\n        outputs = self.model(images)\n        loss = self.criterion(outputs, labels)\n        \n        # Backward pass\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        return loss.item()\n    \n    def evaluate(self, images, labels):\n        \"\"\"Evaluation step\"\"\"\n        self.model.eval()\n        \n        with torch.no_grad():\n            outputs = self.model(images)\n            _, predicted = torch.max(outputs, 1)\n            accuracy = (predicted == labels).float().mean()\n        \n        return accuracy.item()\n\n# Create modern pipeline\npipeline = ModernVisionPipeline()\nprint(\"Modern deep learning pipeline ready!\")\n\n\nModern deep learning pipeline ready!"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#whats-coming-next",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#whats-coming-next",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.14 What‚Äôs Coming Next?",
    "text": "0.14 What‚Äôs Coming Next?\nIn our next post, ‚ÄúModern Vision Models: CNNs, Vision Transformers, and DINOv2‚Äù, we‚Äôll explore:\n\nState-of-the-art architectures (ResNet, EfficientNet, Vision Transformers)\nFoundation models like DINOv2\nSelf-supervised learning (learning without labels)\nBuilding your own DINOv2 feature extractor\n\nYou‚Äôve just learned why deep learning revolutionized computer vision‚Äînext, we‚Äôll explore the cutting-edge models that are shaping the future!"
  },
  {
    "objectID": "posts/series/cv-foundations/07-why-deep-learning.html#key-takeaways",
    "href": "posts/series/cv-foundations/07-why-deep-learning.html#key-takeaways",
    "title": "Why Deep Learning? When Classical Methods Hit the Wall",
    "section": "0.15 Key Takeaways",
    "text": "0.15 Key Takeaways\n\nClassical methods hit a wall with complex real-world variations\nDeep learning learns features automatically instead of hand-crafting them\nCNNs are designed for images with spatial understanding\nTransfer learning makes deep learning practical for everyone\nHierarchical features enable understanding at multiple levels\nModern pipelines combine data augmentation, pre-training, and fine-tuning\n\n\n\n\n\n\n\nHands-On Lab\n\n\n\nReady to build your first deep learning classifier? Try the complete interactive notebook: Deep Learning Basics Lab\nTrain your own cat vs dog classifier and see the power of neural networks!\n\n\n\n\n\n\n\n\nSeries Navigation\n\n\n\n\nPrevious: Feature Magic: What Makes Images Unique\nNext: Modern Vision Models: CNNs, Vision Transformers, and DINOv2\nSeries Home: Computer Vision Foundations\n\n\n\n\nYou‚Äôve just witnessed the deep learning revolution! From struggling with cat vs dog classification to achieving superhuman performance on complex tasks‚Äîthis is why deep learning changed everything. Next, we‚Äôll explore the latest and greatest models that are pushing the boundaries even further."
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html",
    "href": "posts/series/cv-foundations/05-image-segmentation.html",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "",
    "text": "Image segmentation concept - dividing images into meaningful regions"
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#the-art-of-divide-and-conquer",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#the-art-of-divide-and-conquer",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.1 The Art of Divide and Conquer",
    "text": "0.1 The Art of Divide and Conquer\nImagine you‚Äôre looking at a crowded photo and trying to identify individual people. Your brain automatically separates the image into distinct regions‚Äîfaces, bodies, background objects. This is exactly what image segmentation does: it divides an image into meaningful parts that we can analyze separately.\nToday, we‚Äôll master the classical techniques that form the foundation of modern computer vision. From simple thresholding to advanced morphological operations, you‚Äôll learn to segment images like a pro!\n\n\n\n\n\n\nTip\n\n\n\nTry it yourself! Open this interactive Colab notebook to experiment with segmentation techniques as we build this tutorial."
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#what-is-image-segmentation",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#what-is-image-segmentation",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.2 What is Image Segmentation?",
    "text": "0.2 What is Image Segmentation?\nImage segmentation is the process of partitioning an image into multiple segments or regions. Each segment represents something meaningful‚Äîobjects, boundaries, or areas of interest.\nThink of it like: - Coloring book: Each region gets its own color - Jigsaw puzzle: Breaking the image into pieces - Map making: Dividing territory into districts\n\n\n\nSegmentation demonstration with multiple objects\n\n\nThis synthetic image shows perfect segmentation targets: - Bright objects on dark background - Different intensities for various techniques - Multiple shapes to test robustness - Noise elements to challenge algorithms"
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#technique-1-thresholding---the-foundation",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#technique-1-thresholding---the-foundation",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.3 Technique 1: Thresholding - The Foundation",
    "text": "0.3 Technique 1: Thresholding - The Foundation\nThresholding is the simplest segmentation technique. It converts grayscale images to binary (black and white) by setting a threshold value.\n\n\nCode\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load our synthetic segmentation demo image\nimg = cv2.imread('images/segmentation-demo.jpg')\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Simple thresholding\ndef apply_thresholding(image, threshold_value=127):\n    \"\"\"Apply different thresholding techniques\"\"\"\n    \n    # Simple binary thresholding\n    _, binary = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n    \n    # Adaptive thresholding (handles varying lighting)\n    adaptive_mean = cv2.adaptiveThreshold(\n        image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2\n    )\n    \n    adaptive_gaussian = cv2.adaptiveThreshold(\n        image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n    )\n    \n    # Otsu's thresholding (automatically finds best threshold)\n    _, otsu = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    return binary, adaptive_mean, adaptive_gaussian, otsu\n\n# Apply different thresholding techniques\nbinary, adaptive_mean, adaptive_gaussian, otsu = apply_thresholding(gray)\n\n# Display results\nplt.figure(figsize=(20, 12))\n\nplt.subplot(2, 3, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nplt.subplot(2, 3, 2)\nplt.imshow(gray, cmap='gray')\nplt.title(\"Grayscale\")\nplt.axis('off')\n\nplt.subplot(2, 3, 3)\nplt.imshow(binary, cmap='gray')\nplt.title(\"Binary Thresholding (T=127)\")\nplt.axis('off')\n\nplt.subplot(2, 3, 4)\nplt.imshow(adaptive_mean, cmap='gray')\nplt.title(\"Adaptive Mean Thresholding\")\nplt.axis('off')\n\nplt.subplot(2, 3, 5)\nplt.imshow(adaptive_gaussian, cmap='gray')\nplt.title(\"Adaptive Gaussian Thresholding\")\nplt.axis('off')\n\nplt.subplot(2, 3, 6)\nplt.imshow(otsu, cmap='gray')\nplt.title(\"Otsu's Automatic Thresholding\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üéØ Thresholding Results:\")\nprint(f\"- Binary: Simple threshold at 127\")\nprint(f\"- Adaptive Mean: Local neighborhood average\")\nprint(f\"- Adaptive Gaussian: Weighted neighborhood average\")\nprint(f\"- Otsu: Automatically found optimal threshold\")\n\n\n\n\n\nüéØ Thresholding Results:\n- Binary: Simple threshold at 127\n- Adaptive Mean: Local neighborhood average\n- Adaptive Gaussian: Weighted neighborhood average\n- Otsu: Automatically found optimal threshold\n\n\nüî• Amazing! Each technique handles different scenarios: - Binary: Works when lighting is uniform - Adaptive: Handles varying lighting conditions - Otsu: Automatically finds the best threshold"
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#technique-2-morphological-operations---shape-surgery",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#technique-2-morphological-operations---shape-surgery",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.4 Technique 2: Morphological Operations - Shape Surgery",
    "text": "0.4 Technique 2: Morphological Operations - Shape Surgery\nMorphological operations are like performing surgery on shapes. They can clean up noise, separate connected objects, or fill gaps.\n\n0.4.1 The Core Operations\n\n\nCode\n# Load our morphological demo image (designed for these operations)\nmorph_img = cv2.imread('images/morphological-demo.jpg', cv2.IMREAD_GRAYSCALE)\n\ndef demonstrate_morphological_operations(image):\n    \"\"\"Show the four basic morphological operations\"\"\"\n    \n    # Define structuring element (kernel)\n    kernel = np.ones((5, 5), np.uint8)\n    \n    # Basic operations\n    erosion = cv2.erode(image, kernel, iterations=1)\n    dilation = cv2.dilate(image, kernel, iterations=1)\n    opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n    closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n    \n    # Advanced operations\n    gradient = cv2.morphologyEx(image, cv2.MORPH_GRADIENT, kernel)\n    tophat = cv2.morphologyEx(image, cv2.MORPH_TOPHAT, kernel)\n    blackhat = cv2.morphologyEx(image, cv2.MORPH_BLACKHAT, kernel)\n    \n    return erosion, dilation, opening, closing, gradient, tophat, blackhat\n\n# Apply morphological operations\nerosion, dilation, opening, closing, gradient, tophat, blackhat = demonstrate_morphological_operations(morph_img)\n\n# Display results\nplt.figure(figsize=(20, 15))\n\nplt.subplot(3, 3, 1)\nplt.imshow(morph_img, cmap='gray')\nplt.title(\"Original (with noise and gaps)\")\nplt.axis('off')\n\nplt.subplot(3, 3, 2)\nplt.imshow(erosion, cmap='gray')\nplt.title(\"Erosion (shrinks objects)\")\nplt.axis('off')\n\nplt.subplot(3, 3, 3)\nplt.imshow(dilation, cmap='gray')\nplt.title(\"Dilation (expands objects)\")\nplt.axis('off')\n\nplt.subplot(3, 3, 4)\nplt.imshow(opening, cmap='gray')\nplt.title(\"Opening (removes noise)\")\nplt.axis('off')\n\nplt.subplot(3, 3, 5)\nplt.imshow(closing, cmap='gray')\nplt.title(\"Closing (fills gaps)\")\nplt.axis('off')\n\nplt.subplot(3, 3, 6)\nplt.imshow(gradient, cmap='gray')\nplt.title(\"Gradient (edge detection)\")\nplt.axis('off')\n\nplt.subplot(3, 3, 7)\nplt.imshow(tophat, cmap='gray')\nplt.title(\"Top Hat (small bright objects)\")\nplt.axis('off')\n\nplt.subplot(3, 3, 8)\nplt.imshow(blackhat, cmap='gray')\nplt.title(\"Black Hat (small dark objects)\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üîß Morphological Operations Explained:\")\nprint(\"- Erosion: Shrinks white regions (removes noise)\")\nprint(\"- Dilation: Expands white regions (fills small gaps)\")\nprint(\"- Opening: Erosion followed by dilation (removes small noise)\")\nprint(\"- Closing: Dilation followed by erosion (fills gaps in objects)\")\nprint(\"- Gradient: Difference between dilation and erosion (edges)\")\nprint(\"- Top Hat: Original minus opening (small bright features)\")\nprint(\"- Black Hat: Closing minus original (small dark features)\")\n\n\n\n\n\nüîß Morphological Operations Explained:\n- Erosion: Shrinks white regions (removes noise)\n- Dilation: Expands white regions (fills small gaps)\n- Opening: Erosion followed by dilation (removes small noise)\n- Closing: Dilation followed by erosion (fills gaps in objects)\n- Gradient: Difference between dilation and erosion (edges)\n- Top Hat: Original minus opening (small bright features)\n- Black Hat: Closing minus original (small dark features)\n\n\n\n\n0.4.2 Understanding Kernels\n\n\nCode\n# Different kernel shapes for different effects\ndef show_kernel_effects():\n    \"\"\"Demonstrate how different kernels affect morphological operations\"\"\"\n    \n    # Create different kernels\n    kernels = {\n        'Rectangle 3x3': np.ones((3, 3), np.uint8),\n        'Rectangle 7x7': np.ones((7, 7), np.uint8),\n        'Cross': cv2.getStructuringElement(cv2.MORPH_CROSS, (5, 5)),\n        'Ellipse': cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n    }\n    \n    results = {}\n    for name, kernel in kernels.items():\n        # Apply opening operation with different kernels\n        result = cv2.morphologyEx(morph_img, cv2.MORPH_OPEN, kernel)\n        results[name] = result\n    \n    return kernels, results\n\nkernels, kernel_results = show_kernel_effects()\n\n# Visualize kernel effects\nplt.figure(figsize=(20, 10))\n\nplt.subplot(2, 5, 1)\nplt.imshow(morph_img, cmap='gray')\nplt.title(\"Original\")\nplt.axis('off')\n\nfor i, (name, result) in enumerate(kernel_results.items(), 2):\n    plt.subplot(2, 5, i)\n    plt.imshow(result, cmap='gray')\n    plt.title(f\"Opening with {name}\")\n    plt.axis('off')\n\n# Show the kernels themselves\nfor i, (name, kernel) in enumerate(kernels.items(), 6):\n    plt.subplot(2, 5, i)\n    plt.imshow(kernel * 255, cmap='gray')\n    plt.title(f\"{name} Kernel\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üéØ Kernel Shape Effects:\")\nprint(\"- Rectangle: General-purpose, preserves rectangular features\")\nprint(\"- Cross: Connects horizontal/vertical elements\")\nprint(\"- Ellipse: Preserves circular/curved features\")\nprint(\"- Size matters: Larger kernels = stronger effects\")\n\n\n\n\n\nüéØ Kernel Shape Effects:\n- Rectangle: General-purpose, preserves rectangular features\n- Cross: Connects horizontal/vertical elements\n- Ellipse: Preserves circular/curved features\n- Size matters: Larger kernels = stronger effects"
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#technique-3-watershed-segmentation---flooding-algorithm",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#technique-3-watershed-segmentation---flooding-algorithm",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.5 Technique 3: Watershed Segmentation - Flooding Algorithm",
    "text": "0.5 Technique 3: Watershed Segmentation - Flooding Algorithm\nWatershed segmentation treats the image like a topographic map and ‚Äúfloods‚Äù it to separate regions. Perfect for separating touching objects!\n\n\nCode\nfrom scipy import ndimage\nfrom skimage.segmentation import watershed\n\ndef watershed_segmentation(image):\n    \"\"\"Apply watershed segmentation to separate touching objects\"\"\"\n    \n    # Convert to grayscale if needed\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = image\n    \n    # Apply threshold to get binary image\n    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    # Remove noise with morphological opening\n    kernel = np.ones((3, 3), np.uint8)\n    opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=2)\n    \n    # Find sure background area\n    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n    \n    # Find sure foreground area using distance transform\n    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n    \n    # Find unknown region\n    sure_fg = np.uint8(sure_fg)\n    unknown = cv2.subtract(sure_bg, sure_fg)\n    \n    # Marker labelling\n    _, markers = cv2.connectedComponents(sure_fg)\n    \n    # Add one to all labels so that sure background is not 0, but 1\n    markers = markers + 1\n    \n    # Mark the region of unknown with zero\n    markers[unknown == 255] = 0\n    \n    # Apply watershed\n    if len(image.shape) == 3:\n        markers = cv2.watershed(image, markers)\n        result = image.copy()\n        result[markers == -1] = [255, 0, 0]  # Mark boundaries in red\n    else:\n        # For grayscale, create a color version for visualization\n        result = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n        markers_watershed = cv2.watershed(result, markers)\n        result[markers_watershed == -1] = [255, 0, 0]\n    \n    return binary, sure_bg, sure_fg, unknown, markers, result\n\n# Apply watershed to our demo image\nbinary, sure_bg, sure_fg, unknown, markers, watershed_result = watershed_segmentation(img_rgb)\n\n# Display watershed process\nplt.figure(figsize=(20, 15))\n\nplt.subplot(3, 3, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nplt.subplot(3, 3, 2)\nplt.imshow(binary, cmap='gray')\nplt.title(\"Binary Threshold\")\nplt.axis('off')\n\nplt.subplot(3, 3, 3)\nplt.imshow(sure_bg, cmap='gray')\nplt.title(\"Sure Background\")\nplt.axis('off')\n\nplt.subplot(3, 3, 4)\nplt.imshow(sure_fg, cmap='gray')\nplt.title(\"Sure Foreground\")\nplt.axis('off')\n\nplt.subplot(3, 3, 5)\nplt.imshow(unknown, cmap='gray')\nplt.title(\"Unknown Region\")\nplt.axis('off')\n\nplt.subplot(3, 3, 6)\nplt.imshow(markers, cmap='tab20')\nplt.title(\"Markers\")\nplt.axis('off')\n\nplt.subplot(3, 3, 7)\nplt.imshow(watershed_result)\nplt.title(\"Watershed Result\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üåä Watershed Segmentation Steps:\")\nprint(\"1. Threshold to get binary image\")\nprint(\"2. Find sure background (dilated objects)\")\nprint(\"3. Find sure foreground (distance transform peaks)\")\nprint(\"4. Mark unknown regions between sure areas\")\nprint(\"5. Apply watershed algorithm to flood from markers\")\nprint(\"6. Red lines show the watershed boundaries!\")\n\n\n\n\n\nüåä Watershed Segmentation Steps:\n1. Threshold to get binary image\n2. Find sure background (dilated objects)\n3. Find sure foreground (distance transform peaks)\n4. Mark unknown regions between sure areas\n5. Apply watershed algorithm to flood from markers\n6. Red lines show the watershed boundaries!"
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#technique-4-contour-based-segmentation",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#technique-4-contour-based-segmentation",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.6 Technique 4: Contour-Based Segmentation",
    "text": "0.6 Technique 4: Contour-Based Segmentation\nContours are the boundaries of objects. We can use them to segment and analyze individual objects.\n\n\nCode\ndef contour_segmentation(image):\n    \"\"\"Find and analyze contours for segmentation\"\"\"\n    \n    # Convert to grayscale\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = image\n    \n    # Apply threshold\n    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    # Find contours\n    contours, hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Create result image\n    result = image.copy() if len(image.shape) == 3 else cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n    \n    # Analyze each contour\n    contour_info = []\n    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]\n    \n    for i, contour in enumerate(contours):\n        # Calculate contour properties\n        area = cv2.contourArea(contour)\n        perimeter = cv2.arcLength(contour, True)\n        \n        # Filter small contours (noise)\n        if area &gt; 500:  # Minimum area threshold\n            # Get bounding rectangle\n            x, y, w, h = cv2.boundingRect(contour)\n            \n            # Get center\n            M = cv2.moments(contour)\n            if M[\"m00\"] != 0:\n                cx = int(M[\"m10\"] / M[\"m00\"])\n                cy = int(M[\"m01\"] / M[\"m00\"])\n            else:\n                cx, cy = 0, 0\n            \n            # Draw contour and info\n            color = colors[i % len(colors)]\n            cv2.drawContours(result, [contour], -1, color, 3)\n            cv2.rectangle(result, (x, y), (x + w, y + h), color, 2)\n            cv2.circle(result, (cx, cy), 5, color, -1)\n            \n            # Add text\n            cv2.putText(result, f\"#{i+1}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n            \n            contour_info.append({\n                'id': i+1,\n                'area': area,\n                'perimeter': perimeter,\n                'center': (cx, cy),\n                'bounding_box': (x, y, w, h)\n            })\n    \n    return binary, result, contour_info\n\n# Apply contour segmentation\nbinary_contours, contour_result, contour_info = contour_segmentation(img_rgb)\n\n# Display results\nplt.figure(figsize=(15, 8))\n\nplt.subplot(1, 3, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(binary_contours, cmap='gray')\nplt.title(\"Binary Image\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(contour_result)\nplt.title(\"Contour Segmentation\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Print contour analysis\nprint(\"üîç Contour Analysis Results:\")\nprint(\"-\" * 50)\nfor info in contour_info:\n    print(f\"Object #{info['id']}:\")\n    print(f\"  Area: {info['area']:.0f} pixels\")\n    print(f\"  Perimeter: {info['perimeter']:.1f} pixels\")\n    print(f\"  Center: {info['center']}\")\n    print(f\"  Bounding box: {info['bounding_box']}\")\n    print()\n\n\n\n\n\nüîç Contour Analysis Results:\n--------------------------------------------------\nObject #1:\n  Area: 2736 pixels\n  Perimeter: 197.8 pixels\n  Center: (650, 500)\n  Bounding box: (620, 470, 61, 61)\n\nObject #2:\n  Area: 15000 pixels\n  Perimeter: 582.8 pixels\n  Center: (150, 449)\n  Bounding box: (50, 350, 201, 151)\n\nObject #3:\n  Area: 42662 pixels\n  Perimeter: 1073.3 pixels\n  Center: (425, 416)\n  Bounding box: (300, 300, 301, 251)\n\nObject #4:\n  Area: 1890 pixels\n  Perimeter: 164.9 pixels\n  Center: (700, 300)\n  Bounding box: (675, 275, 51, 51)\n\nObject #5:\n  Area: 1200 pixels\n  Perimeter: 131.9 pixels\n  Center: (100, 250)\n  Bounding box: (80, 230, 41, 41)\n\nObject #6:\n  Area: 20000 pixels\n  Perimeter: 600.0 pixels\n  Center: (500, 150)\n  Bounding box: (400, 100, 201, 101)\n\nObject #7:\n  Area: 19854 pixels\n  Perimeter: 529.9 pixels\n  Center: (200, 150)\n  Bounding box: (120, 70, 161, 161)"
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#real-world-application-document-cleaning",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#real-world-application-document-cleaning",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.7 Real-World Application: Document Cleaning",
    "text": "0.7 Real-World Application: Document Cleaning\nLet‚Äôs combine multiple techniques to clean up a noisy document:\n\n\nCode\ndef document_cleaning_pipeline(image):\n    \"\"\"Complete pipeline for cleaning scanned documents\"\"\"\n    \n    # Step 1: Convert to grayscale\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = image\n    \n    # Step 2: Apply adaptive thresholding\n    binary = cv2.adaptiveThreshold(\n        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n    )\n    \n    # Step 3: Remove noise with opening\n    kernel_noise = np.ones((2, 2), np.uint8)\n    denoised = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_noise)\n    \n    # Step 4: Fill gaps in text with closing\n    kernel_fill = np.ones((3, 3), np.uint8)\n    filled = cv2.morphologyEx(denoised, cv2.MORPH_CLOSE, kernel_fill)\n    \n    # Step 5: Enhance text with dilation\n    kernel_enhance = np.ones((2, 2), np.uint8)\n    enhanced = cv2.dilate(filled, kernel_enhance, iterations=1)\n    \n    return gray, binary, denoised, filled, enhanced\n\n# Apply document cleaning to our morphological demo\ngray_doc, binary_doc, denoised_doc, filled_doc, enhanced_doc = document_cleaning_pipeline(morph_img)\n\n# Display the cleaning pipeline\nplt.figure(figsize=(20, 12))\n\nplt.subplot(2, 3, 1)\nplt.imshow(gray_doc, cmap='gray')\nplt.title(\"1. Original Grayscale\")\nplt.axis('off')\n\nplt.subplot(2, 3, 2)\nplt.imshow(binary_doc, cmap='gray')\nplt.title(\"2. Adaptive Threshold\")\nplt.axis('off')\n\nplt.subplot(2, 3, 3)\nplt.imshow(denoised_doc, cmap='gray')\nplt.title(\"3. Noise Removal (Opening)\")\nplt.axis('off')\n\nplt.subplot(2, 3, 4)\nplt.imshow(filled_doc, cmap='gray')\nplt.title(\"4. Gap Filling (Closing)\")\nplt.axis('off')\n\nplt.subplot(2, 3, 5)\nplt.imshow(enhanced_doc, cmap='gray')\nplt.title(\"5. Text Enhancement (Dilation)\")\nplt.axis('off')\n\n# Show before/after comparison\nplt.subplot(2, 3, 6)\ncomparison = np.hstack([gray_doc, enhanced_doc])\nplt.imshow(comparison, cmap='gray')\nplt.title(\"Before ‚Üí After\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìÑ Document Cleaning Pipeline:\")\nprint(\"‚úÖ Adaptive thresholding handles varying lighting\")\nprint(\"‚úÖ Opening removes small noise pixels\")\nprint(\"‚úÖ Closing fills gaps in letters\")\nprint(\"‚úÖ Dilation strengthens text strokes\")\nprint(\"üéØ Result: Clean, readable text!\")\n\n\n\n\n\nüìÑ Document Cleaning Pipeline:\n‚úÖ Adaptive thresholding handles varying lighting\n‚úÖ Opening removes small noise pixels\n‚úÖ Closing fills gaps in letters\n‚úÖ Dilation strengthens text strokes\nüéØ Result: Clean, readable text!"
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#segmentation-comparison-tool",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#segmentation-comparison-tool",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.8 Segmentation Comparison Tool",
    "text": "0.8 Segmentation Comparison Tool\nLet‚Äôs create a tool to compare all segmentation techniques side by side:\n\n\nCode\ndef compare_segmentation_techniques(image):\n    \"\"\"Compare multiple segmentation techniques on the same image\"\"\"\n    \n    # Convert to grayscale\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = image\n    \n    techniques = {}\n    \n    # 1. Simple thresholding\n    _, techniques['Binary Threshold'] = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n    \n    # 2. Otsu's thresholding\n    _, techniques['Otsu Threshold'] = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    # 3. Adaptive thresholding\n    techniques['Adaptive Threshold'] = cv2.adaptiveThreshold(\n        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n    )\n    \n    # 4. Morphological opening\n    kernel = np.ones((5, 5), np.uint8)\n    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    techniques['Morphological Opening'] = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n    \n    # 5. Morphological closing\n    techniques['Morphological Closing'] = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n    \n    return techniques\n\n# Compare techniques on our demo image\ntechniques = compare_segmentation_techniques(img_rgb)\n\n# Display comparison\nplt.figure(figsize=(20, 15))\n\nplt.subplot(2, 3, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nfor i, (name, result) in enumerate(techniques.items(), 2):\n    plt.subplot(2, 3, i)\n    plt.imshow(result, cmap='gray')\n    plt.title(name)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üéØ Technique Comparison:\")\nprint(\"- Binary: Simple but effective for uniform lighting\")\nprint(\"- Otsu: Automatically finds optimal threshold\")\nprint(\"- Adaptive: Handles varying lighting conditions\")\nprint(\"- Opening: Removes noise while preserving object shape\")\nprint(\"- Closing: Fills gaps and connects broken parts\")\n\n\n\n\n\nüéØ Technique Comparison:\n- Binary: Simple but effective for uniform lighting\n- Otsu: Automatically finds optimal threshold\n- Adaptive: Handles varying lighting conditions\n- Opening: Removes noise while preserving object shape\n- Closing: Fills gaps and connects broken parts"
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#your-challenge-medical-image-segmentation",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#your-challenge-medical-image-segmentation",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.9 Your Challenge: Medical Image Segmentation",
    "text": "0.9 Your Challenge: Medical Image Segmentation\nNow it‚Äôs your turn! Here‚Äôs a framework for a medical image segmentation system:\n\n\nCode\nclass MedicalImageSegmenter:\n    def __init__(self):\n        self.techniques = {\n            'threshold': self.threshold_segment,\n            'morphological': self.morphological_segment,\n            'watershed': self.watershed_segment,\n            'contour': self.contour_segment\n        }\n    \n    def threshold_segment(self, image, method='otsu'):\n        \"\"\"Apply thresholding segmentation\"\"\"\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        else:\n            gray = image\n            \n        if method == 'otsu':\n            _, result = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        elif method == 'adaptive':\n            result = cv2.adaptiveThreshold(\n                gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n            )\n        else:\n            _, result = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n        \n        return result\n    \n    def morphological_segment(self, image, operation='opening', kernel_size=5):\n        \"\"\"Apply morphological operations\"\"\"\n        # First get binary image\n        binary = self.threshold_segment(image, 'otsu')\n        \n        # Apply morphological operation\n        kernel = np.ones((kernel_size, kernel_size), np.uint8)\n        \n        if operation == 'opening':\n            result = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n        elif operation == 'closing':\n            result = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n        elif operation == 'gradient':\n            result = cv2.morphologyEx(binary, cv2.MORPH_GRADIENT, kernel)\n        else:\n            result = binary\n            \n        return result\n    \n    def watershed_segment(self, image):\n        \"\"\"Apply watershed segmentation\"\"\"\n        # This is a simplified version\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        else:\n            gray = image\n            \n        # Apply threshold\n        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        \n        # Distance transform\n        dist_transform = cv2.distanceTransform(binary, cv2.DIST_L2, 5)\n        \n        # Find local maxima\n        _, markers = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n        \n        return markers.astype(np.uint8)\n    \n    def contour_segment(self, image, min_area=100):\n        \"\"\"Segment using contours\"\"\"\n        binary = self.threshold_segment(image, 'otsu')\n        \n        # Find contours\n        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        # Create result image\n        result = np.zeros_like(binary)\n        \n        # Draw significant contours\n        for contour in contours:\n            if cv2.contourArea(contour) &gt; min_area:\n                cv2.drawContours(result, [contour], -1, 255, -1)\n        \n        return result\n    \n    def analyze_segments(self, segmented_image):\n        \"\"\"Analyze segmented regions\"\"\"\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(segmented_image)\n        \n        analysis = {\n            'num_objects': num_labels - 1,  # Subtract background\n            'total_area': np.sum(stats[1:, cv2.CC_STAT_AREA]),  # Exclude background\n            'average_area': np.mean(stats[1:, cv2.CC_STAT_AREA]) if num_labels &gt; 1 else 0,\n            'largest_area': np.max(stats[1:, cv2.CC_STAT_AREA]) if num_labels &gt; 1 else 0\n        }\n        \n        return analysis\n\n# Example usage\nsegmenter = MedicalImageSegmenter()\n\n# Apply different techniques to our demo image\nresults = {}\nfor name, method in segmenter.techniques.items():\n    if name == 'morphological':\n        results[name] = method(img_rgb, 'opening')\n    else:\n        results[name] = method(img_rgb)\n\n# Analyze results\nprint(\"üè• Medical Image Segmentation Analysis:\")\nprint(\"=\" * 50)\n\nfor name, result in results.items():\n    analysis = segmenter.analyze_segments(result)\n    print(f\"\\n{name.upper()} TECHNIQUE:\")\n    print(f\"  Objects detected: {analysis['num_objects']}\")\n    print(f\"  Total area: {analysis['total_area']} pixels\")\n    print(f\"  Average area: {analysis['average_area']:.1f} pixels\")\n    print(f\"  Largest area: {analysis['largest_area']} pixels\")\n\n# Visualize results\nplt.figure(figsize=(20, 10))\n\nplt.subplot(2, 3, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nfor i, (name, result) in enumerate(results.items(), 2):\n    plt.subplot(2, 3, i)\n    plt.imshow(result, cmap='gray')\n    plt.title(f\"{name.title()} Segmentation\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüí° Challenge: Try this framework on medical images!\")\nprint(\"- X-rays: Segment bones from soft tissue\")\nprint(\"- MRI scans: Identify different brain regions\")\nprint(\"- Cell images: Count and measure individual cells\")\n\n\nüè• Medical Image Segmentation Analysis:\n==================================================\n\nTHRESHOLD TECHNIQUE:\n  Objects detected: 7\n  Total area: 104849 pixels\n  Average area: 14978.4 pixels\n  Largest area: 43177 pixels\n\nMORPHOLOGICAL TECHNIQUE:\n  Objects detected: 7\n  Total area: 104810 pixels\n  Average area: 14972.9 pixels\n  Largest area: 43177 pixels\n\nWATERSHED TECHNIQUE:\n  Objects detected: 3\n  Total area: 4742 pixels\n  Average area: 1580.7 pixels\n  Largest area: 2375 pixels\n\nCONTOUR TECHNIQUE:\n  Objects detected: 7\n  Total area: 104849 pixels\n  Average area: 14978.4 pixels\n  Largest area: 43177 pixels\n\nüí° Challenge: Try this framework on medical images!\n- X-rays: Segment bones from soft tissue\n- MRI scans: Identify different brain regions\n- Cell images: Count and measure individual cells"
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#key-takeaways",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#key-takeaways",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.10 Key Takeaways",
    "text": "0.10 Key Takeaways\n\nThresholding is your first line of defense - simple but powerful\nMorphological operations are like image surgery tools - they reshape and clean\nWatershed excels at separating touching objects\nContour analysis gives you detailed object measurements\nCombine techniques for robust real-world applications"
  },
  {
    "objectID": "posts/series/cv-foundations/05-image-segmentation.html#whats-coming-next",
    "href": "posts/series/cv-foundations/05-image-segmentation.html#whats-coming-next",
    "title": "Image Segmentation: Dividing and Conquering",
    "section": "0.11 What‚Äôs Coming Next?",
    "text": "0.11 What‚Äôs Coming Next?\nIn our next post, ‚ÄúFeature Magic: What Makes Images Unique‚Äù, we‚Äôll discover:\n\nSIFT and ORB feature detectors\nFeature matching between images\nObject recognition using keypoints\nPanorama stitching with feature alignment\n\nYou‚Äôve mastered the art of dividing images into meaningful parts‚Äînext, we‚Äôll learn to find the unique fingerprints that make each image special!\n\n\n\n\n\n\nHands-On Lab\n\n\n\nReady to segment your own images? Try the complete interactive notebook: Image Segmentation Lab\nExperiment with medical images, document cleaning, and object counting!\n\n\n\n\n\n\n\n\nSeries Navigation\n\n\n\n\nPrevious: Finding Patterns: Edges, Contours, and Shapes\nNext: Feature Magic: What Makes Images Unique\nSeries Home: Computer Vision Foundations\n\n\n\n\nYou‚Äôve just learned the fundamental techniques for breaking images into meaningful parts! Segmentation is used everywhere from medical imaging to autonomous vehicles. Next, we‚Äôll explore how to find unique features that make images recognizable."
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "",
    "text": "üéâ Congratulations! You‚Äôve just completed an incredible journey through the world of computer vision. Let‚Äôs take a moment to appreciate how far you‚Äôve come:\n\n\n\nBasic Python knowledge\nCuriosity about computer vision\nMaybe some confusion about ‚Äúhow computers see‚Äù\n\n\n\n\n\nImage fundamentals: Understanding pixels, arrays, and color spaces\nClassical computer vision: OpenCV operations, edge detection, contour analysis\nPattern recognition: Finding shapes, matching features, template matching\nDeep learning: CNNs, transfer learning, classification\nModern AI: Vision Transformers, foundation models, DINOv2\nReal applications: Built a complete computer vision web app\n\nYou‚Äôre no longer a beginner‚Äîyou‚Äôre a computer vision practitioner! üöÄ"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#the-journey-so-far-what-youve-accomplished",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#the-journey-so-far-what-youve-accomplished",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "",
    "text": "üéâ Congratulations! You‚Äôve just completed an incredible journey through the world of computer vision. Let‚Äôs take a moment to appreciate how far you‚Äôve come:\n\n\n\nBasic Python knowledge\nCuriosity about computer vision\nMaybe some confusion about ‚Äúhow computers see‚Äù\n\n\n\n\n\nImage fundamentals: Understanding pixels, arrays, and color spaces\nClassical computer vision: OpenCV operations, edge detection, contour analysis\nPattern recognition: Finding shapes, matching features, template matching\nDeep learning: CNNs, transfer learning, classification\nModern AI: Vision Transformers, foundation models, DINOv2\nReal applications: Built a complete computer vision web app\n\nYou‚Äôre no longer a beginner‚Äîyou‚Äôre a computer vision practitioner! üöÄ"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#the-computer-vision-landscape-where-you-can-go",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#the-computer-vision-landscape-where-you-can-go",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.2 The Computer Vision Landscape: Where You Can Go",
    "text": "0.2 The Computer Vision Landscape: Where You Can Go\nComputer vision is everywhere, and your skills open doors to exciting opportunities. Let‚Äôs explore the different paths ahead:\n\n0.2.1 üè¢ Industry Applications\n\n0.2.1.1 Autonomous Vehicles üöó\n\nWhat they do: Self-driving cars, drones, delivery robots\nCV tasks: Object detection, depth estimation, path planning\nCompanies: Tesla, Waymo, Cruise, Aurora\nSkills to develop: Real-time processing, sensor fusion, 3D vision\n\n\n\n0.2.1.2 Healthcare & Medical Imaging üè•\n\nWhat they do: Diagnostic imaging, surgical assistance, drug discovery\nCV tasks: Medical image analysis, anomaly detection, segmentation\nCompanies: Google Health, Zebra Medical, PathAI\nSkills to develop: Medical domain knowledge, regulatory compliance, precision\n\n\n\n0.2.1.3 Retail & E-commerce üõí\n\nWhat they do: Visual search, inventory management, customer analytics\nCV tasks: Product recognition, recommendation systems, AR try-on\nCompanies: Amazon, Google, Pinterest, Snapchat\nSkills to develop: Large-scale systems, recommendation algorithms, user experience\n\n\n\n0.2.1.4 Manufacturing & Quality Control üè≠\n\nWhat they do: Automated inspection, defect detection, robotics\nCV tasks: Anomaly detection, measurement, classification\nCompanies: Cognex, Keyence, NVIDIA, Siemens\nSkills to develop: Industrial systems, real-time processing, precision measurement\n\n\n\n0.2.1.5 Entertainment & Social Media üé¨\n\nWhat they do: Content creation, AR filters, video analysis\nCV tasks: Face recognition, style transfer, video understanding\nCompanies: Meta, TikTok, Snap, Adobe\nSkills to develop: Creative applications, user engagement, mobile optimization"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#advanced-topics-to-explore",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#advanced-topics-to-explore",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.3 Advanced Topics to Explore",
    "text": "0.3 Advanced Topics to Explore\nNow that you have the foundations, here are the exciting advanced areas to explore:\n\n0.3.1 üéØ Object Detection & Segmentation\n#| eval: true\n# What you'll learn\nareas = {\n    \"Object Detection\": {\n        \"description\": \"Finding and localizing objects in images\",\n        \"key_models\": [\"YOLO\", \"R-CNN\", \"SSD\", \"DETR\"],\n        \"applications\": [\"Autonomous driving\", \"Security\", \"Retail\"],\n        \"next_steps\": [\n            \"Study YOLO architecture\",\n            \"Implement custom object detector\",\n            \"Learn about anchor boxes and NMS\"\n        ]\n    },\n    \n    \"Semantic Segmentation\": {\n        \"description\": \"Pixel-level classification\",\n        \"key_models\": [\"U-Net\", \"DeepLab\", \"Mask R-CNN\"],\n        \"applications\": [\"Medical imaging\", \"Autonomous driving\", \"Satellite imagery\"],\n        \"next_steps\": [\n            \"Understand encoder-decoder architectures\",\n            \"Work with segmentation datasets\",\n            \"Learn about loss functions for segmentation\"\n        ]\n    }\n}\n\n\n0.3.2 üé® Generative AI for Vision\ngenerative_areas = {\n    \"Image Generation\": {\n        \"technologies\": [\"GANs\", \"Diffusion Models\", \"VAEs\"],\n        \"applications\": [\"Art creation\", \"Data augmentation\", \"Style transfer\"],\n        \"hot_models\": [\"DALL-E\", \"Midjourney\", \"Stable Diffusion\"],\n        \"learning_path\": [\n            \"Understand GAN basics\",\n            \"Explore diffusion models\",\n            \"Build your own image generator\"\n        ]\n    },\n    \n    \"Video Generation\": {\n        \"technologies\": [\"Video GANs\", \"Temporal models\"],\n        \"applications\": [\"Content creation\", \"Animation\", \"Deepfakes\"],\n        \"emerging_field\": \"Huge potential for innovation\"\n    }\n}\n\n\n0.3.3 ü§ñ 3D Computer Vision\nthree_d_vision = {\n    \"Depth Estimation\": {\n        \"description\": \"Understanding 3D structure from 2D images\",\n        \"techniques\": [\"Stereo vision\", \"Monocular depth\", \"LiDAR fusion\"],\n        \"applications\": [\"AR/VR\", \"Robotics\", \"Autonomous vehicles\"]\n    },\n    \n    \"3D Object Detection\": {\n        \"description\": \"Finding objects in 3D space\",\n        \"data_types\": [\"Point clouds\", \"Voxels\", \"Multi-view images\"],\n        \"applications\": [\"Autonomous driving\", \"Robotics\", \"Industrial automation\"]\n    },\n    \n    \"Neural Radiance Fields (NeRF)\": {\n        \"description\": \"Novel view synthesis from images\",\n        \"breakthrough\": \"Photorealistic 3D scene reconstruction\",\n        \"applications\": [\"VR content\", \"Digital twins\", \"Gaming\"]\n    }\n}\n\n\n0.3.4 üîó Multimodal AI\nmultimodal_ai = {\n    \"Vision-Language Models\": {\n        \"examples\": [\"CLIP\", \"BLIP\", \"GPT-4V\", \"LLaVA\"],\n        \"capabilities\": [\"Image captioning\", \"Visual question answering\", \"Text-to-image\"],\n        \"future\": \"The next frontier in AI\"\n    },\n    \n    \"Video Understanding\": {\n        \"tasks\": [\"Action recognition\", \"Video captioning\", \"Temporal reasoning\"],\n        \"models\": [\"Video Transformers\", \"3D CNNs\"],\n        \"applications\": [\"Content moderation\", \"Sports analysis\", \"Security\"]\n    }\n}"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#building-your-learning-path",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#building-your-learning-path",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.4 Building Your Learning Path",
    "text": "0.4 Building Your Learning Path\n\n0.4.1 üìö Essential Resources\n\n0.4.1.1 Books to Read\nrecommended_books = {\n    \"Beginner-Friendly\": [\n        \"Computer Vision: Algorithms and Applications - Szeliski\",\n        \"Learning OpenCV 4 - Kaehler & Bradski\",\n        \"Hands-On Computer Vision with TensorFlow 2 - Planche & Andres\"\n    ],\n    \n    \"Advanced\": [\n        \"Multiple View Geometry - Hartley & Zisserman\",\n        \"Computer Vision: A Modern Approach - Forsyth & Ponce\",\n        \"Deep Learning for Computer Vision - Raschka & Mirjalili\"\n    ],\n    \n    \"Specialized\": [\n        \"Programming Computer Vision with Python - Solem\",\n        \"OpenCV 4 Computer Vision Application Programming Cookbook\"\n    ]\n}\n\n\n0.4.1.2 Online Courses\ncourses = {\n    \"Free\": [\n        \"CS231n: Convolutional Neural Networks (Stanford)\",\n        \"Computer Vision Basics (Coursera - University at Buffalo)\",\n        \"Introduction to Computer Vision (Udacity)\"\n    ],\n    \n    \"Hands-On\": [\n        \"Fast.ai Practical Deep Learning for Coders\",\n        \"PyImageSearch University\",\n        \"Computer Vision Zone YouTube Channel\"\n    ]\n}\n\n\n0.4.1.3 Datasets to Practice With\ndatasets = {\n    \"Classification\": [\n        \"ImageNet - 1M+ images, 1000 classes\",\n        \"CIFAR-10/100 - Small images, good for experimentation\",\n        \"Fashion-MNIST - Clothing classification\"\n    ],\n    \n    \"Object Detection\": [\n        \"COCO - 330K images with object annotations\",\n        \"Pascal VOC - Classic object detection dataset\",\n        \"Open Images - Google's large-scale dataset\"\n    ],\n    \n    \"Segmentation\": [\n        \"Cityscapes - Urban scene understanding\",\n        \"ADE20K - Scene parsing dataset\",\n        \"Medical Segmentation Decathlon\"\n    ],\n    \n    \"Specialized\": [\n        \"CelebA - Face attributes\",\n        \"Places365 - Scene recognition\",\n        \"Kinetics - Video action recognition\"\n    ]\n}\n\n\n\n0.4.2 üõ†Ô∏è Tools and Frameworks to Master\ntools_roadmap = {\n    \"Computer Vision Libraries\": {\n        \"Essential\": [\"OpenCV\", \"PIL/Pillow\", \"scikit-image\"],\n        \"Advanced\": [\"Detectron2\", \"MMDetection\", \"Albumentations\"],\n        \"Specialized\": [\"Open3D\", \"PCL\", \"VTK\"]\n    },\n    \n    \"Deep Learning Frameworks\": {\n        \"Primary\": [\"PyTorch\", \"TensorFlow\"],\n        \"High-Level\": [\"Keras\", \"Lightning\", \"Hugging Face\"],\n        \"Deployment\": [\"ONNX\", \"TensorRT\", \"OpenVINO\"]\n    },\n    \n    \"Development Tools\": {\n        \"Notebooks\": [\"Jupyter\", \"Google Colab\", \"Kaggle Kernels\"],\n        \"Visualization\": [\"Matplotlib\", \"Plotly\", \"Weights & Biases\"],\n        \"Deployment\": [\"Docker\", \"Kubernetes\", \"AWS/GCP/Azure\"]\n    }\n}"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#building-your-portfolio",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#building-your-portfolio",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.5 Building Your Portfolio",
    "text": "0.5 Building Your Portfolio\n\n0.5.1 üéØ Project Ideas by Difficulty\n\n0.5.1.1 Beginner Projects (Weeks 1-4)\nbeginner_projects = [\n    {\n        \"name\": \"Smart Photo Organizer\",\n        \"description\": \"Automatically organize photos by content\",\n        \"skills\": [\"Classification\", \"Feature extraction\", \"File handling\"],\n        \"time\": \"1-2 weeks\"\n    },\n    {\n        \"name\": \"Document Scanner App\",\n        \"description\": \"Mobile app to scan and enhance documents\",\n        \"skills\": [\"Edge detection\", \"Perspective correction\", \"Mobile development\"],\n        \"time\": \"2-3 weeks\"\n    },\n    {\n        \"name\": \"Face Mask Detector\",\n        \"description\": \"Real-time detection of face masks\",\n        \"skills\": [\"Object detection\", \"Real-time processing\", \"OpenCV\"],\n        \"time\": \"1-2 weeks\"\n    }\n]\n\n\n0.5.1.2 Intermediate Projects (Months 2-4)\nintermediate_projects = [\n    {\n        \"name\": \"Custom Object Detector\",\n        \"description\": \"Train YOLO on your own dataset\",\n        \"skills\": [\"Data annotation\", \"Model training\", \"Evaluation metrics\"],\n        \"time\": \"3-4 weeks\"\n    },\n    {\n        \"name\": \"Style Transfer Web App\",\n        \"description\": \"Apply artistic styles to photos\",\n        \"skills\": [\"Neural style transfer\", \"Web development\", \"GPU optimization\"],\n        \"time\": \"2-3 weeks\"\n    },\n    {\n        \"name\": \"Medical Image Analyzer\",\n        \"description\": \"Detect anomalies in medical scans\",\n        \"skills\": [\"Medical imaging\", \"Segmentation\", \"Domain expertise\"],\n        \"time\": \"4-6 weeks\"\n    }\n]\n\n\n0.5.1.3 Advanced Projects (Months 4-8)\nadvanced_projects = [\n    {\n        \"name\": \"3D Scene Reconstruction\",\n        \"description\": \"Build 3D models from multiple photos\",\n        \"skills\": [\"Structure from Motion\", \"3D geometry\", \"Point clouds\"],\n        \"time\": \"6-8 weeks\"\n    },\n    {\n        \"name\": \"Real-time Video Analytics\",\n        \"description\": \"Analyze live video streams for insights\",\n        \"skills\": [\"Video processing\", \"Streaming\", \"Scalable architecture\"],\n        \"time\": \"8-10 weeks\"\n    },\n    {\n        \"name\": \"AR/VR Application\",\n        \"description\": \"Build augmented reality features\",\n        \"skills\": [\"3D tracking\", \"Rendering\", \"Mobile/VR development\"],\n        \"time\": \"10-12 weeks\"\n    }\n]\n\n\n\n0.5.2 üìù Portfolio Presentation Tips\nportfolio_tips = {\n    \"GitHub Repository\": {\n        \"structure\": [\n            \"Clear README with demo GIFs\",\n            \"Well-organized code with comments\",\n            \"Requirements.txt and setup instructions\",\n            \"Results and evaluation metrics\"\n        ],\n        \"best_practices\": [\n            \"Use meaningful commit messages\",\n            \"Include pre-trained models or download links\",\n            \"Add Jupyter notebooks for exploration\",\n            \"Document your learning process\"\n        ]\n    },\n    \n    \"Project Documentation\": {\n        \"include\": [\n            \"Problem statement and motivation\",\n            \"Approach and methodology\",\n            \"Results and visualizations\",\n            \"Challenges and learnings\",\n            \"Future improvements\"\n        ],\n        \"format\": \"Blog posts, videos, or interactive demos\"\n    }\n}"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#career-paths-and-job-opportunities",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#career-paths-and-job-opportunities",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.6 Career Paths and Job Opportunities",
    "text": "0.6 Career Paths and Job Opportunities\n\n0.6.1 üíº Job Roles in Computer Vision\ncareer_paths = {\n    \"Computer Vision Engineer\": {\n        \"responsibilities\": [\n            \"Develop and deploy CV algorithms\",\n            \"Optimize models for production\",\n            \"Integrate CV systems with applications\"\n        ],\n        \"salary_range\": \"$90K - $180K\",\n        \"companies\": [\"Tech giants\", \"Startups\", \"Automotive\", \"Healthcare\"]\n    },\n    \n    \"Machine Learning Engineer\": {\n        \"responsibilities\": [\n            \"Build and maintain ML pipelines\",\n            \"Deploy models at scale\",\n            \"Monitor model performance\"\n        ],\n        \"salary_range\": \"$100K - $200K\",\n        \"focus\": \"More infrastructure and deployment focused\"\n    },\n    \n    \"Research Scientist\": {\n        \"responsibilities\": [\n            \"Develop new algorithms\",\n            \"Publish research papers\",\n            \"Advance state-of-the-art\"\n        ],\n        \"salary_range\": \"$120K - $250K+\",\n        \"requirements\": \"PhD often preferred, strong research background\"\n    },\n    \n    \"Product Manager (AI/CV)\": {\n        \"responsibilities\": [\n            \"Define product vision for CV features\",\n            \"Coordinate between technical and business teams\",\n            \"Understand market needs and technical constraints\"\n        ],\n        \"salary_range\": \"$120K - $220K\",\n        \"background\": \"Technical background with business acumen\"\n    }\n}\n\n\n0.6.2 üéØ Job Search Strategy\njob_search_strategy = {\n    \"Portfolio Preparation\": [\n        \"3-5 strong projects showcasing different skills\",\n        \"Clean, documented code on GitHub\",\n        \"Blog posts explaining your projects\",\n        \"Contributions to open source projects\"\n    ],\n    \n    \"Skill Development\": [\n        \"Master the fundamentals (you've done this!)\",\n        \"Specialize in 1-2 areas (e.g., medical imaging, autonomous vehicles)\",\n        \"Learn deployment and production skills\",\n        \"Understand business applications\"\n    ],\n    \n    \"Networking\": [\n        \"Join CV communities (Reddit, Discord, LinkedIn groups)\",\n        \"Attend conferences (CVPR, ICCV, ECCV)\",\n        \"Participate in competitions (Kaggle, DrivenData)\",\n        \"Connect with professionals on LinkedIn\"\n    ],\n    \n    \"Application Process\": [\n        \"Tailor resume to highlight relevant projects\",\n        \"Prepare for technical interviews\",\n        \"Practice explaining your projects clearly\",\n        \"Understand the company's CV applications\"\n    ]\n}"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#staying-current-in-a-fast-moving-field",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#staying-current-in-a-fast-moving-field",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.7 Staying Current in a Fast-Moving Field",
    "text": "0.7 Staying Current in a Fast-Moving Field\n\n0.7.1 üì∞ Information Sources\nstaying_current = {\n    \"Research Papers\": [\n        \"arXiv.org - Latest research preprints\",\n        \"Papers with Code - Papers with implementation\",\n        \"Google Scholar alerts for specific topics\"\n    ],\n    \n    \"Conferences\": [\n        \"CVPR - Computer Vision and Pattern Recognition\",\n        \"ICCV - International Conference on Computer Vision\",\n        \"ECCV - European Conference on Computer Vision\",\n        \"NeurIPS - Neural Information Processing Systems\"\n    ],\n    \n    \"Blogs and Newsletters\": [\n        \"Towards Data Science on Medium\",\n        \"The Batch by DeepLearning.AI\",\n        \"Papers With Code newsletter\",\n        \"PyImageSearch blog\"\n    ],\n    \n    \"Communities\": [\n        \"r/computervision on Reddit\",\n        \"Computer Vision Discord servers\",\n        \"LinkedIn CV groups\",\n        \"Twitter CV researchers\"\n    ]\n}\n\n\n0.7.2 üèÜ Competitions and Challenges\ncompetitions = {\n    \"Kaggle\": {\n        \"benefits\": [\"Real datasets\", \"Community learning\", \"Portfolio projects\"],\n        \"popular_competitions\": [\n            \"Image classification challenges\",\n            \"Object detection competitions\",\n            \"Medical imaging contests\"\n        ]\n    },\n    \n    \"DrivenData\": {\n        \"focus\": \"Social good applications\",\n        \"examples\": [\"Wildlife conservation\", \"Disaster response\", \"Healthcare\"]\n    },\n    \n    \"Company Challenges\": [\n        \"Google AI challenges\",\n        \"Facebook AI Research competitions\",\n        \"NVIDIA challenges\"\n    ]\n}"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#open-source-contributions",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#open-source-contributions",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.8 Open Source Contributions",
    "text": "0.8 Open Source Contributions\n\n0.8.1 ü§ù How to Contribute\nopen_source_guide = {\n    \"Getting Started\": [\n        \"Find projects you use and understand\",\n        \"Look for 'good first issue' labels\",\n        \"Start with documentation improvements\",\n        \"Fix small bugs or add minor features\"\n    ],\n    \n    \"Popular CV Projects\": [\n        \"OpenCV - The foundation library\",\n        \"PyTorch Vision - Deep learning for vision\",\n        \"Detectron2 - Facebook's object detection\",\n        \"MMDetection - OpenMMLab's detection toolbox\",\n        \"Albumentations - Image augmentation library\"\n    ],\n    \n    \"Benefits\": [\n        \"Learn from experienced developers\",\n        \"Build reputation in the community\",\n        \"Improve coding skills\",\n        \"Network with industry professionals\",\n        \"Give back to the community\"\n    ]\n}"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#the-future-of-computer-vision",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#the-future-of-computer-vision",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.9 The Future of Computer Vision",
    "text": "0.9 The Future of Computer Vision\n\n0.9.1 üîÆ Emerging Trends\nfuture_trends = {\n    \"Foundation Models\": {\n        \"description\": \"Large models trained on massive datasets\",\n        \"examples\": [\"DINOv2\", \"CLIP\", \"SAM (Segment Anything)\"],\n        \"impact\": \"Will change how we approach CV problems\"\n    },\n    \n    \"Multimodal AI\": {\n        \"description\": \"Models that understand multiple data types\",\n        \"examples\": [\"GPT-4V\", \"LLaVA\", \"BLIP-2\"],\n        \"future\": \"The next big breakthrough in AI\"\n    },\n    \n    \"Edge AI\": {\n        \"description\": \"Running CV models on mobile/edge devices\",\n        \"technologies\": [\"Model quantization\", \"Pruning\", \"Specialized chips\"],\n        \"applications\": [\"Mobile apps\", \"IoT devices\", \"Autonomous vehicles\"]\n    },\n    \n    \"3D and Spatial AI\": {\n        \"description\": \"Understanding 3D structure and spatial relationships\",\n        \"technologies\": [\"NeRF\", \"3D Gaussians\", \"Spatial transformers\"],\n        \"applications\": [\"AR/VR\", \"Robotics\", \"Digital twins\"]\n    }\n}"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#your-action-plan",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#your-action-plan",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.10 Your Action Plan",
    "text": "0.10 Your Action Plan\n\n0.10.1 üìÖ Next 30 Days\nnext_30_days = [\n    \"Week 1: Choose your specialization area\",\n    \"Week 2: Start an intermediate project\",\n    \"Week 3: Set up your portfolio website\",\n    \"Week 4: Join CV communities and start networking\"\n]\n\n\n0.10.2 üìÖ Next 90 Days\nnext_90_days = [\n    \"Month 1: Complete 1-2 intermediate projects\",\n    \"Month 2: Contribute to an open source project\",\n    \"Month 3: Start applying for CV positions or internships\"\n]\n\n\n0.10.3 üìÖ Next Year\nnext_year = [\n    \"Build 3-5 strong portfolio projects\",\n    \"Develop expertise in chosen specialization\",\n    \"Attend at least one major CV conference\",\n    \"Land your first CV role or advance in current position\",\n    \"Start mentoring other beginners\"\n]"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#final-words-youre-ready-to-change-the-world",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#final-words-youre-ready-to-change-the-world",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.11 Final Words: You‚Äôre Ready to Change the World",
    "text": "0.11 Final Words: You‚Äôre Ready to Change the World\n\n0.11.1 üåü What You‚Äôve Achieved\nYou started this journey as a beginner, curious about how computers could see and understand images. Now you:\n\nUnderstand the fundamentals of computer vision\nCan build real applications that solve practical problems\nKnow the latest technologies like Vision Transformers and DINOv2\nHave hands-on experience with the tools professionals use\nCan continue learning independently and stay current\n\n\n\n0.11.2 üöÄ Your Impact Potential\nComputer vision is not just about technology‚Äîit‚Äôs about solving real-world problems:\n\nHealthcare: Help doctors detect diseases earlier\nEnvironment: Monitor climate change and wildlife conservation\nSafety: Make transportation and workplaces safer\nAccessibility: Create tools that help people with disabilities\nEducation: Build interactive learning experiences\n\n\n\n0.11.3 üí™ You‚Äôre Part of the Community\nRemember, you‚Äôre now part of a global community of computer vision practitioners. Don‚Äôt hesitate to:\n\nAsk questions in forums and communities\nShare your projects and get feedback\nHelp other beginners who are just starting\nContribute to open source projects\nStay curious and keep experimenting\n\n\n\n0.11.4 üéØ The Journey Continues\nThis series may be ending, but your computer vision journey is just beginning. The field is evolving rapidly, with new breakthroughs happening regularly. Stay curious, keep building, and remember:\nEvery expert was once a beginner. You‚Äôve taken the hardest step‚Äîyou‚Äôve started."
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#resources-for-your-journey",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#resources-for-your-journey",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.12 Resources for Your Journey",
    "text": "0.12 Resources for Your Journey\n\n0.12.1 üìö Quick Reference Links\nquick_links = {\n    \"Documentation\": [\n        \"OpenCV Documentation: https://docs.opencv.org/\",\n        \"PyTorch Vision: https://pytorch.org/vision/\",\n        \"Hugging Face Transformers: https://huggingface.co/docs/\"\n    ],\n    \n    \"Communities\": [\n        \"r/ComputerVision: https://reddit.com/r/computervision\",\n        \"PyImageSearch: https://pyimagesearch.com/\",\n        \"Papers With Code: https://paperswithcode.com/\"\n    ],\n    \n    \"Datasets\": [\n        \"Kaggle Datasets: https://kaggle.com/datasets\",\n        \"Google Dataset Search: https://datasetsearch.research.google.com/\",\n        \"AWS Open Data: https://aws.amazon.com/opendata/\"\n    ]\n}\n\n\n0.12.2 üéì Certification Programs\ncertifications = [\n    \"Google Cloud Professional ML Engineer\",\n    \"AWS Certified Machine Learning - Specialty\",\n    \"Microsoft Azure AI Engineer Associate\",\n    \"NVIDIA Deep Learning Institute Certificates\"\n]"
  },
  {
    "objectID": "posts/series/cv-foundations/10-where-to-go-next.html#thank-you-for-this-journey",
    "href": "posts/series/cv-foundations/10-where-to-go-next.html#thank-you-for-this-journey",
    "title": "Where to Go Next: Your Computer Vision Journey",
    "section": "0.13 Thank You for This Journey! üôè",
    "text": "0.13 Thank You for This Journey! üôè\nIt‚Äôs been an incredible privilege to guide you through the world of computer vision. Watching you grow from understanding pixels to building complete applications has been truly rewarding.\nRemember: You now have the knowledge and skills to build the future. Computer vision is transforming every industry, and you‚Äôre equipped to be part of that transformation.\nGo build something amazing. The world is waiting to see what you‚Äôll create! üåü\n\n\n\n\n\n\nStay Connected\n\n\n\n\nShare your projects on social media with #CVFoundations\nJoin our community discussions\nKeep us updated on your computer vision journey!\n\n\n\n\n\n\n\n\n\nSeries Navigation\n\n\n\n\nPrevious: Your First CV Project: Putting It All Together\nSeries Home: Computer Vision Foundations\nStart Over: Why Computer Vision?\n\n\n\n\nThis concludes the Computer Vision Foundations series. Thank you for being part of this journey. Now go forth and build the future with computer vision! üöÄ"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html",
    "href": "posts/series/cv-foundations/04-finding-patterns.html",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "",
    "text": "Imagine you‚Äôre a detective looking at a crime scene photo. Your brain instantly picks out: - The outline of a footprint in the mud - The edge of a broken window - The shape of a mysterious object on the table\nHow do you do this so effortlessly? You‚Äôre detecting edges and patterns‚Äîthe fundamental building blocks of vision.\nToday, we‚Äôre going to teach computers this same superpower. By the end of this post, your computer will be able to find shapes, count objects, and even detect specific patterns in any image!"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#the-detective-story-teaching-computers-to-see-patterns",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#the-detective-story-teaching-computers-to-see-patterns",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "",
    "text": "Imagine you‚Äôre a detective looking at a crime scene photo. Your brain instantly picks out: - The outline of a footprint in the mud - The edge of a broken window - The shape of a mysterious object on the table\nHow do you do this so effortlessly? You‚Äôre detecting edges and patterns‚Äîthe fundamental building blocks of vision.\nToday, we‚Äôre going to teach computers this same superpower. By the end of this post, your computer will be able to find shapes, count objects, and even detect specific patterns in any image!"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#the-edge-detection-revolution",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#the-edge-detection-revolution",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "0.2 The Edge Detection Revolution",
    "text": "0.2 The Edge Detection Revolution\nEdges are where the magic happens. They‚Äôre the boundaries between different regions in an image‚Äîwhere one object ends and another begins. Think of them as the ‚Äúoutlines‚Äù in a coloring book.\n\n0.2.1 Why Edges Matter\nBefore we dive into code, let‚Äôs understand why edge detection changed everything:\n\nObject Recognition: Edges define object boundaries\nFeature Extraction: Corners and curves are key features\nNoise Reduction: Edges help separate signal from noise\nCompression: JPEG uses edge information to compress images"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#your-first-edge-detector-the-canny-algorithm",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#your-first-edge-detector-the-canny-algorithm",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "0.3 Your First Edge Detector: The Canny Algorithm",
    "text": "0.3 Your First Edge Detector: The Canny Algorithm\nThe Canny edge detector is the gold standard‚Äîit‚Äôs been the go-to algorithm for over 30 years! Here‚Äôs how it works:\n\n\nCode\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load and prepare image\nimg = cv2.imread('images/image.jpg')\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\ngray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n\n# Apply Canny edge detection\nedges = cv2.Canny(gray, 50, 150)\n\n# Display the results\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(gray, cmap='gray')\nplt.title(\"Grayscale\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(edges, cmap='gray')\nplt.title(\"Canny Edges\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Found edges in {np.sum(edges &gt; 0)} pixels out of {edges.size} total pixels\")\n\n\n\n\n\nFound edges in 16716 pixels out of 752400 total pixels\n\n\nüéØ Try it yourself! Open in Colab\n\n0.3.1 Understanding the Canny Parameters\nThe Canny algorithm has two important parameters‚Äîthink of them as sensitivity controls:\n\n\nCode\ndef explore_canny_thresholds(image, low_thresholds, high_thresholds):\n    \"\"\"Explore different Canny threshold combinations\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    \n    plt.figure(figsize=(15, 10))\n    \n    plot_idx = 1\n    for low in low_thresholds:\n        for high in high_thresholds:\n            edges = cv2.Canny(gray, low, high)\n            \n            plt.subplot(len(low_thresholds), len(high_thresholds), plot_idx)\n            plt.imshow(edges, cmap='gray')\n            plt.title(f\"Low: {low}, High: {high}\")\n            plt.axis('off')\n            \n            plot_idx += 1\n    \n    plt.tight_layout()\n    plt.show()\n\n# Test different threshold combinations\nlow_thresholds = [50, 100, 150]\nhigh_thresholds = [100, 150, 200]\nexplore_canny_thresholds(img_rgb, low_thresholds, high_thresholds)\n\n\n\n\n\n\n\n0.3.2 The Magic Behind Canny\nHere‚Äôs what happens inside the Canny algorithm (don‚Äôt worry, OpenCV does this for you!):\n\nBlur the image to reduce noise\nCalculate gradients to find intensity changes\nNon-maximum suppression to thin the edges\nDouble thresholding to classify edge pixels\nEdge tracking to connect broken edge segments"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#from-edges-to-shapes-contour-detection",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#from-edges-to-shapes-contour-detection",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "0.4 From Edges to Shapes: Contour Detection",
    "text": "0.4 From Edges to Shapes: Contour Detection\nEdges are great, but contours are even better! A contour is a curve that connects all the edge points around an object‚Äôs boundary. Think of it as tracing around objects with your finger.\n\n\nCode\n# Find edges first\ngray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\nedges = cv2.Canny(gray, 50, 150)\n\n# Find contours\ncontours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\nprint(f\"Found {len(contours)} contours!\")\n\n# Draw contours on the original image\nimg_with_contours = img_rgb.copy()\ncv2.drawContours(img_with_contours, contours, -1, (0, 255, 0), 2)\n\n# Display results\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(edges, cmap='gray')\nplt.title(\"Edges\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(img_with_contours)\nplt.title(f\"Contours ({len(contours)} found)\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nFound 42 contours!\n\n\n\n\n\n\n0.4.1 Analyzing Contour Properties\nOnce we have contours, we can learn a lot about each shape:\n\n\nCode\ndef analyze_contours(contours, image):\n    \"\"\"Analyze properties of each contour\"\"\"\n    img_analysis = image.copy()\n    \n    for i, contour in enumerate(contours):\n        # Calculate contour properties\n        area = cv2.contourArea(contour)\n        perimeter = cv2.arcLength(contour, True)\n        \n        # Skip tiny contours (probably noise)\n        if area &lt; 100:\n            continue\n        \n        # Get bounding rectangle\n        x, y, w, h = cv2.boundingRect(contour)\n        \n        # Calculate aspect ratio\n        aspect_ratio = float(w) / h\n        \n        # Calculate extent (contour area / bounding rectangle area)\n        rect_area = w * h\n        extent = float(area) / rect_area\n        \n        # Draw bounding rectangle\n        cv2.rectangle(img_analysis, (x, y), (x + w, y + h), (255, 0, 0), 2)\n        \n        # Add text with properties\n        text = f\"#{i}: A={int(area)}, AR={aspect_ratio:.2f}\"\n        cv2.putText(img_analysis, text, (x, y-10), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n        \n        print(f\"Contour {i}:\")\n        print(f\"  Area: {area:.0f} pixels\")\n        print(f\"  Perimeter: {perimeter:.0f} pixels\")\n        print(f\"  Aspect Ratio: {aspect_ratio:.2f}\")\n        print(f\"  Extent: {extent:.2f}\")\n        print()\n    \n    return img_analysis\n\n# Analyze our contours\nanalyzed_img = analyze_contours(contours, img_rgb)\n\nplt.figure(figsize=(12, 8))\nplt.imshow(analyzed_img)\nplt.title(\"Contour Analysis\")\nplt.axis('off')\nplt.show()\n\n\nContour 6:\n  Area: 136 pixels\n  Perimeter: 76 pixels\n  Aspect Ratio: 1.29\n  Extent: 0.54\n\nContour 9:\n  Area: 111 pixels\n  Perimeter: 53 pixels\n  Aspect Ratio: 0.81\n  Extent: 0.53\n\nContour 10:\n  Area: 165 pixels\n  Perimeter: 50 pixels\n  Aspect Ratio: 0.81\n  Extent: 0.79\n\nContour 21:\n  Area: 248 pixels\n  Perimeter: 73 pixels\n  Aspect Ratio: 0.79\n  Extent: 0.54\n\nContour 22:\n  Area: 176 pixels\n  Perimeter: 1243 pixels\n  Aspect Ratio: 1.12\n  Extent: 0.00\n\nContour 23:\n  Area: 24131 pixels\n  Perimeter: 695 pixels\n  Aspect Ratio: 1.05\n  Extent: 0.62\n\nContour 24:\n  Area: 564 pixels\n  Perimeter: 126 pixels\n  Aspect Ratio: 0.93\n  Extent: 0.72\n\nContour 25:\n  Area: 416 pixels\n  Perimeter: 116 pixels\n  Aspect Ratio: 0.83\n  Extent: 0.60\n\nContour 26:\n  Area: 246 pixels\n  Perimeter: 73 pixels\n  Aspect Ratio: 0.34\n  Extent: 0.85\n\nContour 27:\n  Area: 24144 pixels\n  Perimeter: 705 pixels\n  Aspect Ratio: 0.95\n  Extent: 0.65\n\nContour 28:\n  Area: 24115 pixels\n  Perimeter: 657 pixels\n  Aspect Ratio: 0.94\n  Extent: 0.64\n\nContour 29:\n  Area: 576 pixels\n  Perimeter: 149 pixels\n  Aspect Ratio: 0.93\n  Extent: 0.69\n\nContour 30:\n  Area: 570 pixels\n  Perimeter: 127 pixels\n  Aspect Ratio: 0.90\n  Extent: 0.70\n\nContour 31:\n  Area: 418 pixels\n  Perimeter: 116 pixels\n  Aspect Ratio: 0.83\n  Extent: 0.60\n\nContour 32:\n  Area: 619 pixels\n  Perimeter: 163 pixels\n  Aspect Ratio: 0.68\n  Extent: 0.63\n\nContour 34:\n  Area: 715 pixels\n  Perimeter: 2951 pixels\n  Aspect Ratio: 0.89\n  Extent: 0.01\n\nContour 35:\n  Area: 582 pixels\n  Perimeter: 126 pixels\n  Aspect Ratio: 0.93\n  Extent: 0.74\n\nContour 36:\n  Area: 584 pixels\n  Perimeter: 127 pixels\n  Aspect Ratio: 0.93\n  Extent: 0.70\n\nContour 37:\n  Area: 424 pixels\n  Perimeter: 112 pixels\n  Aspect Ratio: 0.60\n  Extent: 0.58\n\nContour 38:\n  Area: 984 pixels\n  Perimeter: 123 pixels\n  Aspect Ratio: 0.84\n  Extent: 0.81\n\nContour 39:\n  Area: 24210 pixels\n  Perimeter: 656 pixels\n  Aspect Ratio: 0.94\n  Extent: 0.65\n\nContour 40:\n  Area: 24182 pixels\n  Perimeter: 699 pixels\n  Aspect Ratio: 0.91\n  Extent: 0.62\n\nContour 41:\n  Area: 50164 pixels\n  Perimeter: 1049 pixels\n  Aspect Ratio: 2.24\n  Extent: 0.65"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#shape-detection-teaching-computers-geometry",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#shape-detection-teaching-computers-geometry",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "0.5 Shape Detection: Teaching Computers Geometry",
    "text": "0.5 Shape Detection: Teaching Computers Geometry\nNow for the really cool part‚Äîdetecting specific shapes! Let‚Äôs build a shape classifier:\n\n\nCode\ndef classify_shape(contour):\n    \"\"\"Classify a contour as a specific shape\"\"\"\n    # Approximate the contour to reduce number of points\n    epsilon = 0.02 * cv2.arcLength(contour, True)\n    approx = cv2.approxPolyDP(contour, epsilon, True)\n    \n    # Get number of vertices\n    vertices = len(approx)\n    \n    # Calculate aspect ratio\n    x, y, w, h = cv2.boundingRect(contour)\n    aspect_ratio = float(w) / h\n    \n    # Classify based on number of vertices and aspect ratio\n    if vertices == 3:\n        return \"Triangle\"\n    elif vertices == 4:\n        if 0.95 &lt;= aspect_ratio &lt;= 1.05:\n            return \"Square\"\n        else:\n            return \"Rectangle\"\n    elif vertices == 5:\n        return \"Pentagon\"\n    elif vertices &gt; 5:\n        # Check if it's circular\n        area = cv2.contourArea(contour)\n        perimeter = cv2.arcLength(contour, True)\n        circularity = 4 * np.pi * area / (perimeter * perimeter)\n        \n        if circularity &gt; 0.7:\n            return \"Circle\"\n        else:\n            return \"Polygon\"\n    else:\n        return \"Unknown\"\n\ndef detect_shapes(image):\n    \"\"\"Detect and classify shapes in an image\"\"\"\n    # Preprocessing\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    edges = cv2.Canny(blurred, 50, 150)\n    \n    # Find contours\n    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Classify shapes\n    result_img = image.copy()\n    shape_counts = {}\n    \n    for contour in contours:\n        # Skip small contours\n        if cv2.contourArea(contour) &lt; 500:\n            continue\n        \n        # Classify the shape\n        shape = classify_shape(contour)\n        \n        # Count shapes\n        shape_counts[shape] = shape_counts.get(shape, 0) + 1\n        \n        # Draw contour and label\n        cv2.drawContours(result_img, [contour], -1, (0, 255, 0), 2)\n        \n        # Get centroid for text placement\n        M = cv2.moments(contour)\n        if M[\"m00\"] != 0:\n            cx = int(M[\"m10\"] / M[\"m00\"])\n            cy = int(M[\"m01\"] / M[\"m00\"])\n            cv2.putText(result_img, shape, (cx-30, cy), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n    \n    return result_img, shape_counts\n\n# Test shape detection\nshapes_img, shape_counts = detect_shapes(img_rgb)\n\nplt.figure(figsize=(12, 8))\nplt.imshow(shapes_img)\nplt.title(\"Shape Detection Results\")\nplt.axis('off')\nplt.show()\n\nprint(\"Detected shapes:\")\nfor shape, count in shape_counts.items():\n    print(f\"  {shape}: {count}\")\n\n\n\n\n\nDetected shapes:\n  Circle: 4\n  Polygon: 7"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#real-world-application-coin-counter",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#real-world-application-coin-counter",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "0.6 Real-World Application: Coin Counter",
    "text": "0.6 Real-World Application: Coin Counter\nLet‚Äôs build something practical‚Äîa coin counting system:\n\n\nCode\ndef count_coins(image):\n    \"\"\"Count circular objects (coins) in an image\"\"\"\n    # Convert to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    \n    # Apply median blur to reduce noise\n    blurred = cv2.medianBlur(gray, 5)\n    \n    # Use HoughCircles to detect circular objects\n    circles = cv2.HoughCircles(\n        blurred,\n        cv2.HOUGH_GRADIENT,\n        dp=1,\n        minDist=30,\n        param1=50,\n        param2=30,\n        minRadius=10,\n        maxRadius=100\n    )\n    \n    result_img = image.copy()\n    coin_count = 0\n    \n    if circles is not None:\n        circles = np.round(circles[0, :]).astype(\"int\")\n        coin_count = len(circles)\n        \n        # Draw detected circles\n        for (x, y, r) in circles:\n            cv2.circle(result_img, (x, y), r, (0, 255, 0), 2)\n            cv2.circle(result_img, (x, y), 2, (0, 0, 255), 3)\n            cv2.putText(result_img, f\"Coin\", (x-20, y-r-10), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n    \n    return result_img, coin_count\n\n# Test coin counting (works best with images of coins on a plain background)\ncoins_img, coin_count = count_coins(img_rgb)\n\nplt.figure(figsize=(12, 8))\nplt.imshow(coins_img)\nplt.title(f\"Coin Detection - Found {coin_count} coins\")\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#advanced-pattern-detection-template-matching",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#advanced-pattern-detection-template-matching",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "0.7 Advanced Pattern Detection: Template Matching",
    "text": "0.7 Advanced Pattern Detection: Template Matching\nSometimes you want to find a specific pattern or object. Template matching is perfect for this:\n\n\nCode\ndef find_template(image, template, threshold=0.8):\n    \"\"\"Find template in image using template matching\"\"\"\n    # Convert both to grayscale\n    gray_img = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    gray_template = cv2.cvtColor(template, cv2.COLOR_RGB2GRAY)\n    \n    # Perform template matching\n    result = cv2.matchTemplate(gray_img, gray_template, cv2.TM_CCOEFF_NORMED)\n    \n    # Find locations where matching exceeds threshold\n    locations = np.where(result &gt;= threshold)\n    \n    # Draw rectangles around matches\n    result_img = image.copy()\n    h, w = gray_template.shape\n    \n    for pt in zip(*locations[::-1]):  # Switch x and y coordinates\n        cv2.rectangle(result_img, pt, (pt[0] + w, pt[1] + h), (0, 255, 0), 2)\n    \n    return result_img, len(locations[0])\n\n# Create a simple template (you can crop from your image)\n# For demo, let's use a small patch from the original image\ntemplate = img_rgb[50:150, 50:150]  # 100x100 patch\n\nmatched_img, match_count = find_template(img_rgb, template, threshold=0.6)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(template)\nplt.title(\"Template to Find\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(matched_img)\nplt.title(f\"Matches Found: {match_count}\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#building-a-complete-object-detection-pipeline",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#building-a-complete-object-detection-pipeline",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "0.8 Building a Complete Object Detection Pipeline",
    "text": "0.8 Building a Complete Object Detection Pipeline\nLet‚Äôs combine everything into a comprehensive object detection system:\n\n\nCode\nclass SimpleObjectDetector:\n    def __init__(self):\n        self.min_contour_area = 100\n        self.canny_low = 50\n        self.canny_high = 150\n    \n    def preprocess(self, image):\n        \"\"\"Preprocess image for better detection\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n        return blurred\n    \n    def find_objects(self, image):\n        \"\"\"Find all objects in the image\"\"\"\n        # Preprocess\n        processed = self.preprocess(image)\n        \n        # Edge detection\n        edges = cv2.Canny(processed, self.canny_low, self.canny_high)\n        \n        # Find contours\n        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        # Filter contours by area\n        valid_contours = [c for c in contours if cv2.contourArea(c) &gt; self.min_contour_area]\n        \n        return valid_contours, edges\n    \n    def analyze_objects(self, contours, image):\n        \"\"\"Analyze detected objects\"\"\"\n        results = []\n        result_img = image.copy()\n        \n        for i, contour in enumerate(contours):\n            # Basic properties\n            area = cv2.contourArea(contour)\n            perimeter = cv2.arcLength(contour, True)\n            \n            # Bounding rectangle\n            x, y, w, h = cv2.boundingRect(contour)\n            aspect_ratio = float(w) / h\n            \n            # Shape classification\n            shape = classify_shape(contour)\n            \n            # Store results\n            obj_info = {\n                'id': i,\n                'area': area,\n                'perimeter': perimeter,\n                'aspect_ratio': aspect_ratio,\n                'shape': shape,\n                'bbox': (x, y, w, h)\n            }\n            results.append(obj_info)\n            \n            # Draw on image\n            cv2.drawContours(result_img, [contour], -1, (0, 255, 0), 2)\n            cv2.rectangle(result_img, (x, y), (x + w, y + h), (255, 0, 0), 1)\n            cv2.putText(result_img, f\"{shape} #{i}\", (x, y-10), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n        \n        return results, result_img\n    \n    def detect(self, image):\n        \"\"\"Complete detection pipeline\"\"\"\n        contours, edges = self.find_objects(image)\n        results, result_img = self.analyze_objects(contours, image)\n        \n        return {\n            'objects': results,\n            'result_image': result_img,\n            'edges': edges,\n            'object_count': len(results)\n        }\n\n# Use the detector\ndetector = SimpleObjectDetector()\ndetection_results = detector.detect(img_rgb)\n\n# Display results\nplt.figure(figsize=(15, 10))\n\nplt.subplot(2, 2, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.axis('off')\n\nplt.subplot(2, 2, 2)\nplt.imshow(detection_results['edges'], cmap='gray')\nplt.title(\"Edge Detection\")\nplt.axis('off')\n\nplt.subplot(2, 2, 3)\nplt.imshow(detection_results['result_image'])\nplt.title(f\"Detected Objects: {detection_results['object_count']}\")\nplt.axis('off')\n\nplt.subplot(2, 2, 4)\n# Create a summary plot\nobject_shapes = [obj['shape'] for obj in detection_results['objects']]\nshape_counts = {}\nfor shape in object_shapes:\n    shape_counts[shape] = shape_counts.get(shape, 0) + 1\n\nif shape_counts:\n    shapes = list(shape_counts.keys())\n    counts = list(shape_counts.values())\n    plt.bar(shapes, counts)\n    plt.title(\"Object Type Distribution\")\n    plt.xticks(rotation=45)\nelse:\n    plt.text(0.5, 0.5, \"No objects detected\", ha='center', va='center', transform=plt.gca().transAxes)\n    plt.title(\"No Objects Found\")\n\nplt.tight_layout()\nplt.show()\n\n# Print detailed results\nprint(\"Detection Results:\")\nprint(f\"Total objects found: {detection_results['object_count']}\")\nprint(\"\\nObject details:\")\nfor obj in detection_results['objects']:\n    print(f\"  Object {obj['id']}: {obj['shape']}\")\n    print(f\"    Area: {obj['area']:.0f} pixels\")\n    print(f\"    Aspect Ratio: {obj['aspect_ratio']:.2f}\")\n    print(f\"    Bounding Box: {obj['bbox']}\")\n\n\n\n\n\nDetection Results:\nTotal objects found: 30\n\nObject details:\n  Object 0: Polygon\n    Area: 260 pixels\n    Aspect Ratio: 1.81\n    Bounding Box: (662, 543, 29, 16)\n  Object 1: Polygon\n    Area: 119 pixels\n    Aspect Ratio: 0.81\n    Bounding Box: (696, 542, 13, 16)\n  Object 2: Polygon\n    Area: 142 pixels\n    Aspect Ratio: 0.77\n    Bounding Box: (733, 541, 17, 22)\n  Object 3: Circle\n    Area: 169 pixels\n    Aspect Ratio: 0.72\n    Bounding Box: (648, 541, 13, 18)\n  Object 4: Polygon\n    Area: 152 pixels\n    Aspect Ratio: 1.12\n    Bounding Box: (746, 522, 19, 17)\n  Object 5: Polygon\n    Area: 137 pixels\n    Aspect Ratio: 0.94\n    Bounding Box: (691, 517, 17, 18)\n  Object 6: Polygon\n    Area: 320 pixels\n    Aspect Ratio: 1.45\n    Bounding Box: (658, 517, 32, 22)\n  Object 7: Polygon\n    Area: 251 pixels\n    Aspect Ratio: 0.79\n    Bounding Box: (713, 476, 19, 24)\n  Object 8: Polygon\n    Area: 206 pixels\n    Aspect Ratio: 1.31\n    Bounding Box: (818, 423, 21, 16)\n  Object 9: Circle\n    Area: 24126 pixels\n    Aspect Ratio: 1.09\n    Bounding Box: (423, 420, 202, 185)\n  Object 10: Polygon\n    Area: 124 pixels\n    Aspect Ratio: 0.67\n    Bounding Box: (786, 417, 12, 18)\n  Object 11: Polygon\n    Area: 142 pixels\n    Aspect Ratio: 1.75\n    Bounding Box: (833, 395, 28, 16)\n  Object 12: Circle\n    Area: 181 pixels\n    Aspect Ratio: 0.78\n    Bounding Box: (818, 393, 14, 18)\n  Object 13: Polygon\n    Area: 248 pixels\n    Aspect Ratio: 0.75\n    Bounding Box: (834, 349, 18, 24)\n  Object 14: Polygon\n    Area: 560 pixels\n    Aspect Ratio: 0.93\n    Bounding Box: (702, 317, 27, 29)\n  Object 15: Polygon\n    Area: 414 pixels\n    Aspect Ratio: 0.86\n    Bounding Box: (675, 317, 24, 28)\n  Object 16: Polygon\n    Area: 562 pixels\n    Aspect Ratio: 1.00\n    Bounding Box: (643, 317, 28, 28)\n  Object 17: Polygon\n    Area: 558 pixels\n    Aspect Ratio: 0.96\n    Bounding Box: (611, 317, 27, 28)\n  Object 18: Rectangle\n    Area: 243 pixels\n    Aspect Ratio: 0.36\n    Bounding Box: (596, 317, 10, 28)\n  Object 19: Polygon\n    Area: 412 pixels\n    Aspect Ratio: 0.86\n    Bounding Box: (568, 317, 24, 28)\n  Object 20: Circle\n    Area: 24141 pixels\n    Aspect Ratio: 0.95\n    Bounding Box: (331, 317, 188, 198)\n  Object 21: Polygon\n    Area: 616 pixels\n    Aspect Ratio: 0.72\n    Bounding Box: (539, 309, 26, 36)\n  Object 22: Polygon\n    Area: 574 pixels\n    Aspect Ratio: 0.96\n    Bounding Box: (665, 259, 27, 28)\n  Object 23: Polygon\n    Area: 575 pixels\n    Aspect Ratio: 0.93\n    Bounding Box: (611, 258, 27, 29)\n  Object 24: Polygon\n    Area: 422 pixels\n    Aspect Ratio: 0.62\n    Bounding Box: (642, 253, 21, 34)\n  Object 25: Circle\n    Area: 971 pixels\n    Aspect Ratio: 0.89\n    Bounding Box: (576, 251, 32, 36)\n  Object 26: Polygon\n    Area: 177 pixels\n    Aspect Ratio: 0.75\n    Bounding Box: (396, 181, 18, 24)\n  Object 27: Circle\n    Area: 24217 pixels\n    Aspect Ratio: 0.94\n    Bounding Box: (741, 109, 188, 199)\n  Object 28: Polygon\n    Area: 198 pixels\n    Aspect Ratio: 0.94\n    Bounding Box: (331, 109, 188, 199)\n  Object 29: Polygon\n    Area: 50177 pixels\n    Aspect Ratio: 2.25\n    Bounding Box: (423, 19, 414, 184)"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#your-challenge-build-a-practical-application",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#your-challenge-build-a-practical-application",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "0.9 Your Challenge: Build a Practical Application",
    "text": "0.9 Your Challenge: Build a Practical Application\nNow it‚Äôs your turn! Try building one of these applications:\n\n0.9.1 üéØ Challenge 1: Document Page Counter\n\n\nCode\ndef count_pages(image):\n    \"\"\"Count rectangular pages/documents in an image\"\"\"\n    # Your code here!\n    # Hint: Look for large rectangular contours\n    pass\n\n\n\n\n0.9.2 üéØ Challenge 2: Parking Space Detector\n\n\nCode\ndef detect_parking_spaces(image):\n    \"\"\"Detect empty parking spaces (rectangular regions)\"\"\"\n    # Your code here!\n    # Hint: Look for rectangular shapes of a certain size\n    pass\n\n\n\n\n0.9.3 üéØ Challenge 3: Playing Card Detector\n\n\nCode\ndef detect_playing_cards(image):\n    \"\"\"Detect playing cards in an image\"\"\"\n    # Your code here!\n    # Hint: Cards are rectangular with a specific aspect ratio\n    pass"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#whats-coming-next",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#whats-coming-next",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "0.10 What‚Äôs Coming Next?",
    "text": "0.10 What‚Äôs Coming Next?\nIn our next adventure, ‚ÄúFeature Magic: What Makes Images Unique‚Äù, we‚Äôll explore:\n\nKeypoint detection (finding interesting points)\nFeature descriptors (describing what makes each point unique)\nFeature matching (finding the same object in different images)\nPanorama stitching (combining multiple photos)\n\nYou‚Äôve just learned to find shapes and patterns‚Äînext, we‚Äôll learn to recognize and match them!"
  },
  {
    "objectID": "posts/series/cv-foundations/04-finding-patterns.html#key-takeaways",
    "href": "posts/series/cv-foundations/04-finding-patterns.html#key-takeaways",
    "title": "Finding Patterns: Edges, Contours, and Shapes",
    "section": "0.11 Key Takeaways",
    "text": "0.11 Key Takeaways\n\nEdges are fundamental to computer vision\nCanny edge detection is the gold standard algorithm\nContours help us find and analyze shapes\nShape classification uses geometric properties\nTemplate matching finds specific patterns\nCombining techniques creates powerful applications\n\n\n\n\n\n\n\nHands-On Lab\n\n\n\nReady to detect patterns in your own images? Try the complete interactive notebook: Pattern Detection Lab\nUpload photos and watch your computer find shapes, count objects, and detect patterns!\n\n\n\n\n\n\n\n\nSeries Navigation\n\n\n\n\nPrevious: OpenCV Essentials: Your First Computer Vision Toolkit\nNext: Image Segmentation: Dividing and Conquering\nSeries Home: Computer Vision Foundations\n\n\n\n\nYou‚Äôve just taught computers to see patterns and shapes! This is a huge milestone‚Äîyou‚Äôre now working with the same techniques used in industrial quality control, medical imaging, and autonomous vehicles."
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "",
    "text": "Imagine you‚Äôre a carpenter, and someone hands you the ultimate toolbox‚Äîevery tool you could ever need, perfectly organized and ready to use. That‚Äôs OpenCV for computer vision!\nOpenCV (Open Source Computer Vision Library) is like having a Swiss Army knife that can: - Resize photos faster than Photoshop - Find faces in crowds - Track moving objects - Apply Instagram-worthy filters - And about 2,500+ other amazing things!\nThe best part? It‚Äôs completely free and used by everyone from Google to your neighbor‚Äôs startup."
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#meet-your-new-best-friend-opencv",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#meet-your-new-best-friend-opencv",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "",
    "text": "Imagine you‚Äôre a carpenter, and someone hands you the ultimate toolbox‚Äîevery tool you could ever need, perfectly organized and ready to use. That‚Äôs OpenCV for computer vision!\nOpenCV (Open Source Computer Vision Library) is like having a Swiss Army knife that can: - Resize photos faster than Photoshop - Find faces in crowds - Track moving objects - Apply Instagram-worthy filters - And about 2,500+ other amazing things!\nThe best part? It‚Äôs completely free and used by everyone from Google to your neighbor‚Äôs startup."
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#the-origin-story",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#the-origin-story",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.2 The Origin Story",
    "text": "0.2 The Origin Story\nBack in 1999, Intel created OpenCV to accelerate computer vision research. Fast forward 25 years, and it‚Äôs become the #1 computer vision library in the world. Every self-driving car, security camera, and photo app uses OpenCV under the hood.\nToday, we‚Äôre going to master the essential 20% that gives you 80% of the power!"
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#your-first-opencv-adventure",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#your-first-opencv-adventure",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.3 Your First OpenCV Adventure",
    "text": "0.3 Your First OpenCV Adventure\nYou can use any photo. Just make sure it‚Äôs in the same folder as this notebook or provide the full path. Let‚Äôs start with the basics‚Äîloading and displaying an image:\n\n\nCode\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load an image (OpenCV reads in BGR format by default)\nimg = cv2.imread('images/image.jpg')\n\n# Convert BGR to RGB (for matplotlib display)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nprint(f\"Image shape: {img_rgb.shape}\")\nprint(f\"Image size: {img_rgb.size} total pixels\")\n\n# Display the image\nplt.figure(figsize=(10, 6))\nplt.imshow(img_rgb)\nplt.title(\"Your Photo Loaded with OpenCV!\")\nplt.axis('off')\nplt.show()\n\n\nImage shape: (627, 1200, 3)\nImage size: 2257200 total pixels\n\n\n\n\n\nüéØ Try it yourself! Open in Colab"
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#the-essential-operations-your-daily-toolkit",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#the-essential-operations-your-daily-toolkit",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.4 The Essential Operations: Your Daily Toolkit",
    "text": "0.4 The Essential Operations: Your Daily Toolkit\n\n0.4.1 1. Resizing: Making Images Fit\nThink of resizing like adjusting a picture frame‚Äîsometimes you need it bigger, sometimes smaller:\n\n\nCode\n# Original image\nheight, width = img_rgb.shape[:2]\nprint(f\"Original size: {width} x {height}\")\n\n# Method 1: Specify exact dimensions\nresized_exact = cv2.resize(img_rgb, (400, 300))\n\n# Method 2: Scale by percentage\nscale_percent = 50  # 50% of original size\nnew_width = int(width * scale_percent / 100)\nnew_height = int(height * scale_percent / 100)\nresized_scaled = cv2.resize(img_rgb, (new_width, new_height))\n\n# Method 3: Keep aspect ratio (the smart way!)\ndef resize_with_aspect_ratio(image, width=None, height=None):\n    h, w = image.shape[:2]\n    \n    if width is None and height is None:\n        return image\n    \n    if width is None:\n        # Calculate width based on height\n        ratio = height / h\n        width = int(w * ratio)\n    else:\n        # Calculate height based on width\n        ratio = width / w\n        height = int(h * ratio)\n    \n    return cv2.resize(image, (width, height))\n\nresized_smart = resize_with_aspect_ratio(img_rgb, width=400)\n\n# Show the results\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(resized_exact)\nplt.title(f\"Exact: 400x300\\n(might be distorted)\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(resized_scaled)\nplt.title(f\"Scaled: 50%\\n{new_width}x{new_height}\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(resized_smart)\nplt.title(f\"Smart Resize\\nAspect ratio preserved\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nOriginal size: 1200 x 627\n\n\n\n\n\n\n\n0.4.2 2. Cropping: Focus on What Matters\nCropping is like using scissors on a digital photo‚Äîyou keep the interesting part and throw away the rest:\n\n\nCode\n# Cropping is just array slicing!\nheight, width = img_rgb.shape[:2]\n\n# Center crop (most common)\ncrop_size = 300\nstart_x = (width - crop_size) // 2\nstart_y = (height - crop_size) // 2\ncenter_crop = img_rgb[start_y:start_y+crop_size, start_x:start_x+crop_size]\n\n# Top-left crop\ntop_left_crop = img_rgb[0:300, 0:300]\n\n# Bottom-right crop\nbottom_right_crop = img_rgb[height-300:height, width-300:width]\n\n# Custom crop function\ndef smart_crop(image, x, y, width, height):\n    \"\"\"Crop with bounds checking\"\"\"\n    h, w = image.shape[:2]\n    \n    # Make sure we don't go out of bounds\n    x = max(0, min(x, w - width))\n    y = max(0, min(y, h - height))\n    width = min(width, w - x)\n    height = min(height, h - y)\n    \n    return image[y:y+height, x:x+width]\n\ncustom_crop = smart_crop(img_rgb, 100, 50, 400, 300)\n\n# Display results\nplt.figure(figsize=(15, 10))\n\nplt.subplot(2, 2, 1)\nplt.imshow(center_crop)\nplt.title(\"Center Crop\")\nplt.axis('off')\n\nplt.subplot(2, 2, 2)\nplt.imshow(top_left_crop)\nplt.title(\"Top-Left Crop\")\nplt.axis('off')\n\nplt.subplot(2, 2, 3)\nplt.imshow(bottom_right_crop)\nplt.title(\"Bottom-Right Crop\")\nplt.axis('off')\n\nplt.subplot(2, 2, 4)\nplt.imshow(custom_crop)\nplt.title(\"Custom Crop\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n0.4.3 3. Rotation: Spinning Photos Like a DJ\nSometimes your photo is sideways, or you want a creative angle. OpenCV makes rotation easy:\n\n\nCode\ndef rotate_image(image, angle):\n    \"\"\"Rotate image by angle (in degrees)\"\"\"\n    height, width = image.shape[:2]\n    \n    # Get rotation matrix\n    center = (width // 2, height // 2)\n    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    \n    # Apply rotation\n    rotated = cv2.warpAffine(image, rotation_matrix, (width, height))\n    return rotated\n\ndef rotate_and_crop(image, angle):\n    \"\"\"Rotate and crop to remove black borders\"\"\"\n    height, width = image.shape[:2]\n    center = (width // 2, height // 2)\n    \n    # Calculate new dimensions to avoid black borders\n    angle_rad = np.radians(abs(angle))\n    new_width = int(width * np.cos(angle_rad) + height * np.sin(angle_rad))\n    new_height = int(height * np.cos(angle_rad) + width * np.sin(angle_rad))\n    \n    # Get rotation matrix\n    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    \n    # Adjust translation\n    rotation_matrix[0, 2] += (new_width - width) / 2\n    rotation_matrix[1, 2] += (new_height - height) / 2\n    \n    # Apply rotation\n    rotated = cv2.warpAffine(image, rotation_matrix, (new_width, new_height))\n    return rotated\n\n# Try different rotations\nangles = [15, 45, 90, 180]\nplt.figure(figsize=(15, 10))\n\nfor i, angle in enumerate(angles):\n    rotated = rotate_image(img_rgb, angle)\n    \n    plt.subplot(2, 2, i+1)\n    plt.imshow(rotated)\n    plt.title(f\"Rotated {angle}¬∞\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n0.4.4 4. Flipping: Mirror, Mirror on the Wall\nFlipping is like looking in a mirror‚Äîsuper simple but very useful:\n\n\nCode\n# Horizontal flip (left-right mirror)\nflipped_horizontal = cv2.flip(img_rgb, 1)\n\n# Vertical flip (upside down)\nflipped_vertical = cv2.flip(img_rgb, 0)\n\n# Both directions (180¬∞ rotation equivalent)\nflipped_both = cv2.flip(img_rgb, -1)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 4, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original\")\nplt.axis('off')\n\nplt.subplot(1, 4, 2)\nplt.imshow(flipped_horizontal)\nplt.title(\"Horizontal Flip\")\nplt.axis('off')\n\nplt.subplot(1, 4, 3)\nplt.imshow(flipped_vertical)\nplt.title(\"Vertical Flip\")\nplt.axis('off')\n\nplt.subplot(1, 4, 4)\nplt.imshow(flipped_both)\nplt.title(\"Both Directions\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#color-space-adventures",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#color-space-adventures",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.5 Color Space Adventures",
    "text": "0.5 Color Space Adventures\nRemember how we learned about RGB? OpenCV knows many other color spaces too! Each one is useful for different tasks:\n\n\nCode\n# Convert to different color spaces\ngray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\nhsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\nlab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)\n\nplt.figure(figsize=(15, 10))\n\n# Original RGB\nplt.subplot(2, 3, 1)\nplt.imshow(img_rgb)\nplt.title(\"RGB (Red, Green, Blue)\")\nplt.axis('off')\n\n# Grayscale\nplt.subplot(2, 3, 2)\nplt.imshow(gray, cmap='gray')\nplt.title(\"Grayscale\")\nplt.axis('off')\n\n# HSV channels\nplt.subplot(2, 3, 3)\nplt.imshow(hsv[:, :, 0], cmap='hsv')  # Hue\nplt.title(\"HSV - Hue Channel\")\nplt.axis('off')\n\nplt.subplot(2, 3, 4)\nplt.imshow(hsv[:, :, 1], cmap='gray')  # Saturation\nplt.title(\"HSV - Saturation Channel\")\nplt.axis('off')\n\nplt.subplot(2, 3, 5)\nplt.imshow(hsv[:, :, 2], cmap='gray')  # Value\nplt.title(\"HSV - Value Channel\")\nplt.axis('off')\n\n# LAB\nplt.subplot(2, 3, 6)\nplt.imshow(lab[:, :, 0], cmap='gray')\nplt.title(\"LAB - Lightness Channel\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Color space info:\")\nprint(f\"RGB shape: {img_rgb.shape}\")\nprint(f\"Grayscale shape: {gray.shape}\")\nprint(f\"HSV shape: {hsv.shape}\")\n\n\n\n\n\nColor space info:\nRGB shape: (627, 1200, 3)\nGrayscale shape: (627, 1200)\nHSV shape: (627, 1200, 3)"
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#your-first-image-filters",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#your-first-image-filters",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.6 Your First Image Filters",
    "text": "0.6 Your First Image Filters\nNow for the fun part‚Äîapplying filters to make your photos look amazing!\n\n0.6.1 Blur Effects: Softening the World\n\n\nCode\n# Convert to grayscale for cleaner examples\ngray_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n\n# Different types of blur\ngaussian_blur = cv2.GaussianBlur(gray_img, (15, 15), 0)\nmedian_blur = cv2.medianBlur(gray_img, 15)\nbilateral_blur = cv2.bilateralFilter(gray_img, 15, 80, 80)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 4, 1)\nplt.imshow(gray_img, cmap='gray')\nplt.title(\"Original\")\nplt.axis('off')\n\nplt.subplot(1, 4, 2)\nplt.imshow(gaussian_blur, cmap='gray')\nplt.title(\"Gaussian Blur\\n(smooth)\")\nplt.axis('off')\n\nplt.subplot(1, 4, 3)\nplt.imshow(median_blur, cmap='gray')\nplt.title(\"Median Blur\\n(noise reduction)\")\nplt.axis('off')\n\nplt.subplot(1, 4, 4)\nplt.imshow(bilateral_blur, cmap='gray')\nplt.title(\"Bilateral Filter\\n(edge-preserving)\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n0.6.2 Sharpening: Making Everything Pop\n\n\nCode\n# Create a sharpening kernel\nsharpening_kernel = np.array([\n    [0, -1, 0],\n    [-1, 5, -1],\n    [0, -1, 0]\n])\n\n# Apply the kernel\nsharpened = cv2.filter2D(gray_img, -1, sharpening_kernel)\n\n# Alternative: Unsharp masking (more sophisticated)\nblurred = cv2.GaussianBlur(gray_img, (0, 0), 2.0)\nunsharp_mask = cv2.addWeighted(gray_img, 1.5, blurred, -0.5, 0)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(gray_img, cmap='gray')\nplt.title(\"Original\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(sharpened, cmap='gray')\nplt.title(\"Kernel Sharpening\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(unsharp_mask, cmap='gray')\nplt.title(\"Unsharp Masking\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#building-your-first-photo-editor",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#building-your-first-photo-editor",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.7 Building Your First Photo Editor",
    "text": "0.7 Building Your First Photo Editor\nLet‚Äôs combine everything we‚Äôve learned into a simple photo editor:\n\n\nCode\ndef photo_editor(image, operation='original', **kwargs):\n    \"\"\"A simple photo editor with multiple operations\"\"\"\n    \n    if operation == 'resize':\n        width = kwargs.get('width', 400)\n        return resize_with_aspect_ratio(image, width=width)\n    \n    elif operation == 'crop':\n        x, y, w, h = kwargs.get('crop_box', (100, 100, 300, 300))\n        return smart_crop(image, x, y, w, h)\n    \n    elif operation == 'rotate':\n        angle = kwargs.get('angle', 45)\n        return rotate_image(image, angle)\n    \n    elif operation == 'flip':\n        direction = kwargs.get('direction', 'horizontal')\n        if direction == 'horizontal':\n            return cv2.flip(image, 1)\n        elif direction == 'vertical':\n            return cv2.flip(image, 0)\n        else:\n            return cv2.flip(image, -1)\n    \n    elif operation == 'blur':\n        kernel_size = kwargs.get('kernel_size', 15)\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        blurred = cv2.GaussianBlur(gray, (kernel_size, kernel_size), 0)\n        return cv2.cvtColor(blurred, cv2.COLOR_GRAY2RGB)\n    \n    elif operation == 'brightness':\n        value = kwargs.get('value', 50)\n        return np.clip(image.astype(np.int16) + value, 0, 255).astype(np.uint8)\n    \n    else:\n        return image\n\n# Demo the photo editor\noperations = [\n    ('original', {}),\n    ('resize', {'width': 300}),\n    ('rotate', {'angle': 15}),\n    ('brightness', {'value': 50}),\n    ('blur', {'kernel_size': 21}),\n    ('flip', {'direction': 'horizontal'})\n]\n\nplt.figure(figsize=(18, 12))\n\nfor i, (op, params) in enumerate(operations):\n    result = photo_editor(img_rgb, op, **params)\n    \n    plt.subplot(2, 3, i+1)\n    if len(result.shape) == 2:  # Grayscale\n        plt.imshow(result, cmap='gray')\n    else:\n        plt.imshow(result)\n    plt.title(f\"{op.title()}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#real-world-applications",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#real-world-applications",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.8 Real-World Applications",
    "text": "0.8 Real-World Applications\nNow you might be wondering: ‚ÄúThis is cool, but when would I actually use this?‚Äù Here are some real examples:\n\n0.8.1 1. Photo Processing Pipeline\n\n\nCode\ndef process_photo_batch(image_path):\n    \"\"\"Process a photo for social media\"\"\"\n    # Load image\n    img = cv2.imread(image_path)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    # Resize for Instagram (square format)\n    img_resized = resize_with_aspect_ratio(img_rgb, width=1080)\n    \n    # Center crop to square\n    h, w = img_resized.shape[:2]\n    size = min(h, w)\n    start_x = (w - size) // 2\n    start_y = (h - size) // 2\n    img_square = img_resized[start_y:start_y+size, start_x:start_x+size]\n    \n    # Apply slight blur for dreamy effect\n    img_final = cv2.GaussianBlur(img_square, (3, 3), 0)\n    \n    return img_final\n\n\n\n\n0.8.2 2. Document Scanner\n\n\nCode\ndef scan_document(image):\n    \"\"\"Simple document scanner\"\"\"\n    # Convert to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    \n    # Apply adaptive thresholding (makes text crisp)\n    binary = cv2.adaptiveThreshold(\n        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n        cv2.THRESH_BINARY, 11, 2\n    )\n    \n    return binary\n\n# Try it on your image (if it has text)\nscanned = scan_document(img_rgb)\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original\")\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(scanned, cmap='gray')\nplt.title(\"Scanned Document\")\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#the-power-of-combining-operations",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#the-power-of-combining-operations",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.9 The Power of Combining Operations",
    "text": "0.9 The Power of Combining Operations\nThe real magic happens when you combine multiple operations:\n\n\nCode\ndef instagram_filter(image):\n    \"\"\"Create an Instagram-style vintage filter\"\"\"\n    # Step 1: Slight blur for dreamy effect\n    blurred = cv2.GaussianBlur(image, (3, 3), 0)\n    \n    # Step 2: Increase saturation (convert to HSV)\n    hsv = cv2.cvtColor(blurred, cv2.COLOR_RGB2HSV)\n    hsv[:, :, 1] = np.clip(hsv[:, :, 1] * 1.3, 0, 255)  # Boost saturation\n    enhanced = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n    \n    # Step 3: Warm color temperature (add yellow tint)\n    warm = enhanced.copy().astype(np.float32)\n    warm[:, :, 0] = np.clip(warm[:, :, 0] * 1.1, 0, 255)  # More red\n    warm[:, :, 1] = np.clip(warm[:, :, 1] * 1.05, 0, 255)  # More green\n    \n    return warm.astype(np.uint8)\n\n# Apply the filter\nfiltered = instagram_filter(img_rgb)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(img_rgb)\nplt.title(\"Original\")\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(filtered)\nplt.title(\"Instagram Filter\")\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#your-homework-build-something-cool",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#your-homework-build-something-cool",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.10 Your Homework: Build Something Cool!",
    "text": "0.10 Your Homework: Build Something Cool!\nBefore we move to the next post, try these challenges:\n\n0.10.1 üéØ Challenge 1: Photo Collage Maker\n\n\nCode\ndef create_collage(images, grid_size=(2, 2)):\n    \"\"\"Create a photo collage\"\"\"\n    rows, cols = grid_size\n    \n    # Resize all images to same size\n    target_size = (300, 300)\n    resized_images = []\n    \n    for img in images[:rows*cols]:  # Take only what we need\n        resized = cv2.resize(img, target_size)\n        resized_images.append(resized)\n    \n    # Create the collage\n    row_images = []\n    for r in range(rows):\n        row_imgs = resized_images[r*cols:(r+1)*cols]\n        if len(row_imgs) == cols:\n            row_combined = np.hstack(row_imgs)\n            row_images.append(row_combined)\n    \n    if len(row_images) == rows:\n        collage = np.vstack(row_images)\n        return collage\n    \n    return None\n\n# Try it with multiple copies of your image (or different images)\nimages = [img_rgb, cv2.flip(img_rgb, 1), rotate_image(img_rgb, 90), filtered]\ncollage = create_collage(images)\n\nif collage is not None:\n    plt.figure(figsize=(10, 10))\n    plt.imshow(collage)\n    plt.title(\"Your Photo Collage\")\n    plt.axis('off')\n    plt.show()\n\n\n\n\n\n\n\n0.10.2 üéØ Challenge 2: Color Palette Extractor\n\n\nCode\ndef extract_dominant_colors(image, k=5):\n    \"\"\"Extract dominant colors from an image\"\"\"\n    # Reshape image to be a list of pixels\n    pixels = image.reshape(-1, 3)\n    pixels = np.float32(pixels)\n    \n    # Use k-means clustering to find dominant colors\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n    _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n    \n    # Convert back to uint8\n    centers = np.uint8(centers)\n    \n    return centers\n\n# Extract colors\ncolors = extract_dominant_colors(img_rgb)\n\n# Display the palette\nplt.figure(figsize=(12, 3))\nfor i, color in enumerate(colors):\n    plt.subplot(1, len(colors), i+1)\n    color_patch = np.full((100, 100, 3), color, dtype=np.uint8)\n    plt.imshow(color_patch)\n    plt.title(f\"RGB: {color}\")\n    plt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#whats-next",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#whats-next",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.11 What‚Äôs Next?",
    "text": "0.11 What‚Äôs Next?\nIn our next post, ‚ÄúFinding Patterns: Edges, Contours, and Shapes‚Äù, we‚Äôll discover how computers find objects in images:\n\nEdge detection (finding boundaries)\nContour detection (finding shapes)\nShape analysis (counting objects)\nYour first object detector!\n\nYou now have a solid foundation in image manipulation. Next, we‚Äôll teach computers to understand what they‚Äôre seeing!"
  },
  {
    "objectID": "posts/series/cv-foundations/03-opencv-essentials.html#key-takeaways",
    "href": "posts/series/cv-foundations/03-opencv-essentials.html#key-takeaways",
    "title": "OpenCV Essentials: Your First Computer Vision Toolkit",
    "section": "0.12 Key Takeaways",
    "text": "0.12 Key Takeaways\n\nOpenCV is your Swiss Army knife for computer vision\nBasic operations (resize, crop, rotate, flip) are the building blocks\nColor spaces give you different ways to analyze images\nFilters can enhance or stylize your images\nCombining operations creates powerful effects\n\n\n\n\n\n\n\nHands-On Lab\n\n\n\nReady to experiment with all these techniques? Try the complete interactive notebook: OpenCV Essentials Lab\nUpload your own photos and build your own filters!\n\n\n\n\n\n\n\n\nSeries Navigation\n\n\n\n\nPrevious: Images as Data: How Computers See the World\nNext: Finding Patterns: Edges, Contours, and Shapes\nSeries Home: Computer Vision Foundations\n\n\n\n\nYou‚Äôve just mastered the essential tools of computer vision! In the next post, we‚Äôll use these tools to teach computers to find and recognize objects in images."
  },
  {
    "objectID": "posts/series/computer-vision-foundations.html",
    "href": "posts/series/computer-vision-foundations.html",
    "title": "Computer Vision Foundations Series",
    "section": "",
    "text": "Imagine you‚Äôre teaching a robot to see. You show it a picture of your cat, but to the robot, this is just a giant grid of numbers! How do we help it understand what it‚Äôs looking at? That‚Äôs the beautiful challenge of computer vision.\nThis series follows the Pareto Principle: we‚Äôll focus on the 20% of concepts and tools that give you 80% of the results. By the end, you‚Äôll understand the journey from classical OpenCV techniques to modern foundation models like DINOv2."
  },
  {
    "objectID": "posts/series/computer-vision-foundations.html#the-story-behind-this-series",
    "href": "posts/series/computer-vision-foundations.html#the-story-behind-this-series",
    "title": "Computer Vision Foundations Series",
    "section": "",
    "text": "Imagine you‚Äôre teaching a robot to see. You show it a picture of your cat, but to the robot, this is just a giant grid of numbers! How do we help it understand what it‚Äôs looking at? That‚Äôs the beautiful challenge of computer vision.\nThis series follows the Pareto Principle: we‚Äôll focus on the 20% of concepts and tools that give you 80% of the results. By the end, you‚Äôll understand the journey from classical OpenCV techniques to modern foundation models like DINOv2."
  },
  {
    "objectID": "posts/series/computer-vision-foundations.html#learning-philosophy",
    "href": "posts/series/computer-vision-foundations.html#learning-philosophy",
    "title": "Computer Vision Foundations Series",
    "section": "0.2 Learning Philosophy",
    "text": "0.2 Learning Philosophy\nFollowing Jeremy Howard‚Äôs fastai approach: - Start with working code: See results first, understand theory later - Build intuition: Every concept explained with simple analogies - Learn by doing: Each post includes hands-on Colab notebooks - Progressive complexity: From pixels to neural networks, step by step"
  },
  {
    "objectID": "posts/series/computer-vision-foundations.html#the-journey-ahead",
    "href": "posts/series/computer-vision-foundations.html#the-journey-ahead",
    "title": "Computer Vision Foundations Series",
    "section": "0.3 The Journey Ahead",
    "text": "0.3 The Journey Ahead\n\n0.3.1 üéØ Phase 1: Understanding Images\n\nWhy Computer Vision? - The big picture and motivation\nImages as Data - How computers see the world\n\n\n\n0.3.2 üîß Phase 2: Classical Computer Vision\n\nOpenCV Essentials - Your first computer vision toolkit\nFinding Patterns - Edges, contours, and shapes\nImage Segmentation - Dividing images into meaningful parts\nFeature Magic - What makes images unique\n\n\n\n0.3.3 üß† Phase 3: Deep Learning Revolution\n\nWhy Deep Learning? - When classical methods hit the wall\nModern Vision Models - CNNs, Vision Transformers, and DINOv2\n\n\n\n0.3.4 üöÄ Phase 4: Building Your Future\n\nYour First CV Project - Putting it all together\nWhere to Go Next - Your learning roadmap"
  },
  {
    "objectID": "posts/series/computer-vision-foundations.html#what-youll-build",
    "href": "posts/series/computer-vision-foundations.html#what-youll-build",
    "title": "Computer Vision Foundations Series",
    "section": "0.4 What You‚Äôll Build",
    "text": "0.4 What You‚Äôll Build\nBy the end of this series, you‚Äôll have: - Built 5+ hands-on projects (from edge detection to object classification) - Used both classical and modern techniques (OpenCV + PyTorch) - Extracted features with DINOv2 (foundation model magic) - Created your own image classifier (transfer learning)"
  },
  {
    "objectID": "posts/series/computer-vision-foundations.html#prerequisites",
    "href": "posts/series/computer-vision-foundations.html#prerequisites",
    "title": "Computer Vision Foundations Series",
    "section": "0.5 Prerequisites",
    "text": "0.5 Prerequisites\n\nPython basics: Variables, functions, loops (we‚Äôll explain the rest!)\nCuriosity: That‚Äôs honestly the most important part\nGoogle account: For running our Colab notebooks"
  },
  {
    "objectID": "posts/series/computer-vision-foundations.html#tools-well-master",
    "href": "posts/series/computer-vision-foundations.html#tools-well-master",
    "title": "Computer Vision Foundations Series",
    "section": "0.6 Tools We‚Äôll Master",
    "text": "0.6 Tools We‚Äôll Master\n\nOpenCV: The Swiss Army knife of classical computer vision\nNumPy & Matplotlib: For working with image data\nPyTorch: Modern deep learning framework\nHuggingFace: Pre-trained models made easy\nGoogle Colab: Free GPU power for everyone"
  },
  {
    "objectID": "posts/series/computer-vision-foundations.html#how-to-use-this-series",
    "href": "posts/series/computer-vision-foundations.html#how-to-use-this-series",
    "title": "Computer Vision Foundations Series",
    "section": "0.7 How to Use This Series",
    "text": "0.7 How to Use This Series\n\nRead each post like a story (they‚Äôre designed to be fun!)\nRun the Colab notebooks (hands-on learning is key)\nExperiment with your own images (use photos of your pets, friends, anything!)\nAsk questions in the comments (no question is too basic)"
  },
  {
    "objectID": "posts/series/computer-vision-foundations.html#a-personal-note",
    "href": "posts/series/computer-vision-foundations.html#a-personal-note",
    "title": "Computer Vision Foundations Series",
    "section": "0.8 A Personal Note",
    "text": "0.8 A Personal Note\nThis series is designed for someone just starting their computer vision journey. I believe everyone can learn to teach computers to see ‚Äì it just takes the right approach. We‚Äôll go slow, explain everything, and have fun along the way.\nRemember: every expert was once a beginner. Let‚Äôs begin this journey together!\n\n\n\n\n\n\nQuick Start\n\n\n\nReady to jump in? Start with Why Computer Vision? and follow along with the Colab notebook. Each post builds on the previous one, so take your time and enjoy the process!"
  },
  {
    "objectID": "posts/series/computer-vision-foundations.html#getting-help",
    "href": "posts/series/computer-vision-foundations.html#getting-help",
    "title": "Computer Vision Foundations Series",
    "section": "0.9 Getting Help",
    "text": "0.9 Getting Help\n\nüí¨ Comments: Use the discussion section below each post\nüêô GitHub: Find all code in our repository\nüìß Email: Reach out directly for specific questions\n\n\n\n\n\n\n\nNote\n\n\n\nThis series is continuously updated based on reader feedback and new developments in computer vision. Your questions and suggestions help make it better for everyone!"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html",
    "href": "posts/data-science-steps-to-follow-part06/index.html",
    "title": "Data Science Steps to Follow - 06",
    "section": "",
    "text": "Data Science Steps Series\n\n\n\nThis is Part 6 of a 6-part series on data science fundamentals:\n\nPart 1: EDA Fundamentals\nPart 2: Feature Preprocessing and Generation\nPart 3: Handling Anonymized Data\nPart 4: Text Feature Extraction\nPart 5: Image Feature Extraction\nPart 6: Advanced Techniques (You are here)"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#validation-strategies",
    "href": "posts/data-science-steps-to-follow-part06/index.html#validation-strategies",
    "title": "Data Science Steps to Follow - 06",
    "section": "2.1 Validation Strategies",
    "text": "2.1 Validation Strategies\n\n2.1.1 Holdout Validation\nThe simplest form of validation where we split our data into training and validation sets:\nflowchart TB\n    A[data] --&gt; B[Validation data]\n    A[data] --&gt; C[Training data]\nThis approach is good when: - You have sufficient data - The data distribution is relatively uniform - Time-based dependencies are not critical\n\n\n2.1.2 K-Fold Cross Validation\nA more robust approach that uses multiple training-validation splits:\nflowchart TB\n    A[data] --&gt; B[Fold 1]\n    B[Fold 1] --&gt; C[Training data]\n    B[Fold 1] --&gt; D[Validation data]\n\n    A[data] --&gt; E[Fold 2]\n    E[Fold 2] --&gt; F[Training data]\n    E[Fold 2] --&gt; G[Validation data]\n\n    A[data] --&gt; H[Fold 3]\n    H[Fold 3] --&gt; I[Training data]\n    H[Fold 3] --&gt; J[Validation data]\n\n    C(Training data) --&gt; K[Average]\n    D(Training data) --&gt; K[Average]\n    F(Training data) --&gt; K[Average]\n    G(Training data) --&gt; K[Average]\n    I(Training data) --&gt; K[Average]\n    J(Training data) --&gt; K[Average]\n\n    K(Average) --&gt; L[Final model performance]\n\n\n2.1.3 Leave-One-Out Validation\nBest for very small datasets where we need to maximize training data usage."
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#common-validation-problems",
    "href": "posts/data-science-steps-to-follow-part06/index.html#common-validation-problems",
    "title": "Data Science Steps to Follow - 06",
    "section": "2.2 Common Validation Problems",
    "text": "2.2 Common Validation Problems\n\n2.2.1 1. Data Leakage\n\nTarget leakage\nTrain-test contamination\nFeature leakage\n\n\n\n2.2.2 2. Distribution Mismatch\n\nTraining data not representative\nTemporal dependencies\nConcept drift\n\n\n\n2.2.3 3. Sample Size Issues\n\nToo little validation data\nImbalanced splits\nHigh variance in metrics"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#best-practices",
    "href": "posts/data-science-steps-to-follow-part06/index.html#best-practices",
    "title": "Data Science Steps to Follow - 06",
    "section": "2.3 Best Practices",
    "text": "2.3 Best Practices\n\nAlways use stratification for:\n\nSmall datasets\nImbalanced classes\nMulti-class problems\n\nConsider temporal aspects:\n\nTime-based splits for time series\nRolling window validation\nForward-chaining\n\nMonitor multiple metrics:\n\nAccuracy/RMSE\nROC-AUC\nPrecision-Recall\nBusiness metrics"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#implementation-examples",
    "href": "posts/data-science-steps-to-follow-part06/index.html#implementation-examples",
    "title": "Data Science Steps to Follow - 06",
    "section": "2.4 Implementation Examples",
    "text": "2.4 Implementation Examples\nHere‚Äôs how to implement different validation strategies using scikit-learn:\nfrom sklearn.model_selection import (\n    train_test_split,\n    KFold,\n    StratifiedKFold,\n    LeaveOneOut\n)\n\n# Simple holdout\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# K-Fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfor train_idx, val_idx in kf.split(X):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n# Stratified K-Fold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor train_idx, val_idx in skf.split(X, y):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#next-steps",
    "href": "posts/data-science-steps-to-follow-part06/index.html#next-steps",
    "title": "Data Science Steps to Follow - 06",
    "section": "2.5 Next Steps",
    "text": "2.5 Next Steps\nNow that you understand validation strategies, you can: 1. Choose appropriate validation methods for your data 2. Implement robust validation pipelines 3. Avoid common pitfalls in model evaluation"
  },
  {
    "objectID": "posts/data-science-steps-to-follow-part06/index.html#references",
    "href": "posts/data-science-steps-to-follow-part06/index.html#references",
    "title": "Data Science Steps to Follow - 06",
    "section": "2.6 References",
    "text": "2.6 References\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning\nRaschka, S. (2018). Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning\nKohavi, R. (1995). A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection"
  },
  {
    "objectID": "notebooks/cv-foundations-06-why-deep-learning.html",
    "href": "notebooks/cv-foundations-06-why-deep-learning.html",
    "title": "Computer Vision Foundations #6: 06 Why Deep Learning",
    "section": "",
    "text": "This notebook accompanies the blog post series.\nüöß **Coming Soon-p notebooks* This notebook is being prepared with interactive examples.\nFor now, you can read the blog post and we‚Äôll have the full interactive notebook ready soon!\n\n\nCode\n# Setup code will go here\nprint('Notebook coming soon! üöÄ')"
  },
  {
    "objectID": "notebooks/cv-foundations-02-images-as-data.html",
    "href": "notebooks/cv-foundations-02-images-as-data.html",
    "title": "Computer Vision Foundations #2: 02 Images As Data",
    "section": "",
    "text": "This notebook accompanies the blog post series.\nüöß **Coming Soon-p notebooks* This notebook is being prepared with interactive examples.\nFor now, you can read the blog post and we‚Äôll have the full interactive notebook ready soon!\n\n\nCode\n# Setup code will go here\nprint('Notebook coming soon! üöÄ')"
  },
  {
    "objectID": "notebooks/cv-foundations-03-opencv-essentials.html",
    "href": "notebooks/cv-foundations-03-opencv-essentials.html",
    "title": "Computer Vision Foundations #3: 03 Opencv Essentials",
    "section": "",
    "text": "This notebook accompanies the blog post series.\nüöß **Coming Soon-p notebooks* This notebook is being prepared with interactive examples.\nFor now, you can read the blog post and we‚Äôll have the full interactive notebook ready soon!\n\n\nCode\n# Setup code will go here\nprint('Notebook coming soon! üöÄ')"
  },
  {
    "objectID": "notebooks/cv-foundations-05-feature-magic.html",
    "href": "notebooks/cv-foundations-05-feature-magic.html",
    "title": "Computer Vision Foundations #5: 05 Feature Magic",
    "section": "",
    "text": "This notebook accompanies the blog post series.\nüöß **Coming Soon-p notebooks* This notebook is being prepared with interactive examples.\nFor now, you can read the blog post and we‚Äôll have the full interactive notebook ready soon!\n\n\nCode\n# Setup code will go here\nprint('Notebook coming soon! üöÄ')"
  },
  {
    "objectID": "categories.html",
    "href": "categories.html",
    "title": "Categories",
    "section": "",
    "text": "Welcome to the category index! Here you can find all posts organized by their main topics.\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nFeature Preprocessing and Generation\n\n\n\ndata-science\n\n\nfeature-engineering\n\n\ntutorial\n\n\n\n\nHasan Goni\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation Strategy and Model Evaluation\n\n\n\ndata-science\n\n\nmachine-learning\n\n\nvalidation\n\n\n\n\nHasan Goni\n\n\nNov 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\ndata-science\n\n\n\n\nHasan Goni\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nA Deep Dive into Neural Network Fundamentals\n\n\n\ndeep-learning\n\n\nmathematics\n\n\nfastai\n\n\n\n\nHasan Goni\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items\n\n\n\n\n\nUse the links below to browse specific categories:\n\n\n\n\nData Science\nMachine Learning\nDeep Learning\n\n\n\n\n\nPython\nPyTorch\nNumPy\n\n\n\n\n\nEDA\nVisualization\nTools"
  },
  {
    "objectID": "categories.html#data-science",
    "href": "categories.html#data-science",
    "title": "Categories",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nFeature Preprocessing and Generation\n\n\n\ndata-science\n\n\nfeature-engineering\n\n\ntutorial\n\n\n\n\nHasan Goni\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation Strategy and Model Evaluation\n\n\n\ndata-science\n\n\nmachine-learning\n\n\nvalidation\n\n\n\n\nHasan Goni\n\n\nNov 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\ndata-science\n\n\n\n\nHasan Goni\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "categories.html#deep-learning",
    "href": "categories.html#deep-learning",
    "title": "Categories",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nA Deep Dive into Neural Network Fundamentals\n\n\n\ndeep-learning\n\n\nmathematics\n\n\nfastai\n\n\n\n\nHasan Goni\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "categories.html#tools-and-technologies",
    "href": "categories.html#tools-and-technologies",
    "title": "Categories",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "categories.html#all-categories",
    "href": "categories.html#all-categories",
    "title": "Categories",
    "section": "",
    "text": "Use the links below to browse specific categories:\n\n\n\n\nData Science\nMachine Learning\nDeep Learning\n\n\n\n\n\nPython\nPyTorch\nNumPy\n\n\n\n\n\nEDA\nVisualization\nTools"
  }
]